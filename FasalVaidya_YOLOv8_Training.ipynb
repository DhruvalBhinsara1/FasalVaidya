{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9808aba3",
   "metadata": {},
   "source": [
    "# üå± FasalVaidya: YOLOv8 Multi-Crop Nutrient Deficiency Classification\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook trains a **YOLOv8 Classification** model for detecting nutrient deficiencies across **9 crops** with **43 classes**.\n",
    "\n",
    "### üéØ Why YOLOv8 for Classification?\n",
    "\n",
    "- ‚ö° **Ultra-fast inference** (<10ms on GPU, <50ms on mobile)\n",
    "- üéØ **State-of-the-art accuracy** for image classification\n",
    "- üì± **Easy export** to ONNX, TensorFlow Lite, CoreML, TensorRT\n",
    "- üîß **Simple API** with Ultralytics framework\n",
    "- üìä **Built-in augmentation** and training optimization\n",
    "\n",
    "### üåæ Supported Crops (9 total, 43 classes)\n",
    "\n",
    "| Category | Crops | Classes |\n",
    "|----------|-------|---------|\n",
    "| **Cereals** | Rice, Wheat, Maize | 11 |\n",
    "| **Commercial** | Banana, Coffee | 7 |\n",
    "| **Vegetables** | Ashgourd, EggPlant, Snakegourd, Bittergourd | 25 |\n",
    "\n",
    "### üìä YOLOv8 Classification Models\n",
    "\n",
    "| Model | Size | Accuracy | Speed (CPU) | Speed (GPU) |\n",
    "|-------|------|----------|-------------|-------------|\n",
    "| YOLOv8n-cls | 5.3MB | Good | 12ms | 0.6ms |\n",
    "| YOLOv8s-cls | 11.4MB | Better | 23ms | 0.9ms |\n",
    "| YOLOv8m-cls | 36.6MB | Best | 85ms | 2.0ms |\n",
    "\n",
    "We'll use **YOLOv8s-cls** for the best balance of accuracy and speed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768edc8",
   "metadata": {},
   "source": [
    "## üîß Section 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3833ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üì¶ Install Ultralytics YOLOv8 and dependencies\n",
    "# ==========================================\n",
    "\n",
    "!pip install ultralytics>=8.2.0 --quiet\n",
    "!pip install opencv-python-headless pillow matplotlib seaborn tqdm --quiet\n",
    "\n",
    "# Verify installation\n",
    "import ultralytics\n",
    "print(f\"‚úÖ Ultralytics version: {ultralytics.__version__}\")\n",
    "\n",
    "from ultralytics import YOLO\n",
    "print(\"‚úÖ YOLOv8 imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d18d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üñ•Ô∏è Check GPU availability\n",
    "# ==========================================\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# GPU Check\n",
    "print(\"=\" * 50)\n",
    "print(\"üñ•Ô∏è Hardware Check\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úÖ GPU: {gpu_name}\")\n",
    "    print(f\"‚úÖ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"‚úÖ CUDA Version: {torch.version.cuda}\")\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected - training will be slow!\")\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "print(f\"‚úÖ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"‚úÖ Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c94dfb",
   "metadata": {},
   "source": [
    "## üìÇ Section 2: Mount Google Drive & Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2edfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üìÅ Mount Google Drive\n",
    "# ==========================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted!\")\n",
    "\n",
    "# Verify mount\n",
    "!ls /content/drive/MyDrive/ | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ec849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üåæ Define Crop Dataset Paths (Same as EfficientNet notebook)\n",
    "# ==========================================\n",
    "\n",
    "# Base paths in Google Drive\n",
    "GDRIVE_BASE = \"/content/drive/MyDrive\"\n",
    "\n",
    "# Dataset configurations - paths relative to GDRIVE_BASE\n",
    "CROP_DATASETS = {\n",
    "    'rice': 'PlantNutrientDeficiency_MobileCapture/Rice',\n",
    "    'wheat': 'PlantNutrientDeficiency_MobileCapture/Wheat',\n",
    "    'maize': 'maize_nutrient_deficiency',\n",
    "    'banana': 'Banana_Nutrient_Deficiency_Dataset',\n",
    "    'coffee': 'Coffee_Nutrient_Deficiency_dataset',\n",
    "    'ashgourd': 'VegetableLeaves/Ashgourd',\n",
    "    'eggplant': 'VegetableLeaves/EggPlant',\n",
    "    'snakegourd': 'VegetableLeaves/Snakegourd',\n",
    "    'bittergourd': 'VegetableLeaves/Bittergourd',\n",
    "}\n",
    "\n",
    "# Class renaming map - standardize class names across all datasets\n",
    "CLASS_RENAME_MAP = {\n",
    "    'rice': {\n",
    "        'Nitrogen(N)': 'rice_nitrogen',\n",
    "        'Phosphorus(P)': 'rice_phosphorus',\n",
    "        'Potassium(K)': 'rice_potassium',\n",
    "    },\n",
    "    'wheat': {\n",
    "        'control': 'wheat_control',\n",
    "        'deficiency': 'wheat_deficiency',\n",
    "    },\n",
    "    'maize': {\n",
    "        'ALL Present': 'maize_all_present',\n",
    "        'ALLAB': 'maize_allab',\n",
    "        'KAB': 'maize_kab',\n",
    "        'NAB': 'maize_nab',\n",
    "        'PAB': 'maize_pab',\n",
    "        'ZNAB': 'maize_znab',\n",
    "    },\n",
    "    'banana': {\n",
    "        'healthy': 'banana_healthy',\n",
    "        'magnesium': 'banana_magnesium',\n",
    "        'potassium': 'banana_potassium',\n",
    "    },\n",
    "    'coffee': {\n",
    "        'healthy': 'coffee_healthy',\n",
    "        'nitrogen-N': 'coffee_nitrogen_n',\n",
    "        'phosphorus-P': 'coffee_phosphorus_p',\n",
    "        'potasium-K': 'coffee_potassium_k',\n",
    "    },\n",
    "    'ashgourd': {\n",
    "        'Deficiency of Boron': 'ashgourd_boron_deficiency',\n",
    "        'Deficiency of Iron': 'ashgourd_iron_deficiency',\n",
    "        'Deficiency of Manganese': 'ashgourd_manganese_deficiency',\n",
    "        'Deficiency of Molybdenum': 'ashgourd_molybdenum_deficiency',\n",
    "        'Deficiency of Nitrogen': 'ashgourd_nitrogen_deficiency',\n",
    "        'Deficiency of Potassium': 'ashgourd_potassium_deficiency',\n",
    "        'Healthy': 'ashgourd_healthy',\n",
    "    },\n",
    "    'eggplant': {\n",
    "        'Deficiency of Magnesium': 'eggplant_magnesium_deficiency',\n",
    "        'Deficiency of Nitrogen': 'eggplant_nitrogen_deficiency',\n",
    "        'Deficiency of Potassium': 'eggplant_potassium_deficiency',\n",
    "        'Healthy': 'eggplant_healthy',\n",
    "    },\n",
    "    'snakegourd': {\n",
    "        'Deficiency of Copper': 'snakegourd_copper_deficiency',\n",
    "        'Deficiency of Molybdenum': 'snakegourd_molybdenum_deficiency',\n",
    "        'Deficiency of Nitrogen': 'snakegourd_nitrogen_deficiency',\n",
    "        'Deficiency of Potassium': 'snakegourd_potassium_deficiency',\n",
    "        'Healthy': 'snakegourd_healthy',\n",
    "    },\n",
    "    'bittergourd': {\n",
    "        'Deficiency of Boron': 'bittergourd_boron_deficiency',\n",
    "        'Deficiency of Calcium': 'bittergourd_calcium_deficiency',\n",
    "        'Deficiency of Copper': 'bittergourd_copper_deficiency',\n",
    "        'Deficiency of Iron': 'bittergourd_iron_deficiency',\n",
    "        'Deficiency of Manganese': 'bittergourd_manganese_deficiency',\n",
    "        'Deficiency of Nitrogen': 'bittergourd_nitrogen_deficiency',\n",
    "        'Deficiency of Potassium': 'bittergourd_potassium_deficiency',\n",
    "        'Deficiency of Sulfur': 'bittergourd_sulfur_deficiency',\n",
    "        'Healthy': 'bittergourd_healthy',\n",
    "    }\n",
    "}\n",
    "\n",
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'img_size': 224,           # YOLOv8 classification default\n",
    "    'batch_size': 32,          # Adjust based on GPU memory\n",
    "    'epochs': 50,              # Training epochs\n",
    "    'min_samples': 150,        # Minimum images per class\n",
    "    'max_samples': 400,        # Maximum images per class\n",
    "    'train_split': 0.8,        # 80% train, 20% val\n",
    "    'model_variant': 'yolov8s-cls',  # Small model for balance of speed/accuracy\n",
    "    'patience': 10,            # Early stopping patience\n",
    "}\n",
    "\n",
    "# Output paths\n",
    "YOLO_DATASET_DIR = Path('/content/yolo_dataset')  # YOLO format dataset\n",
    "OUTPUT_DIR = Path('/content/yolo_output')          # Training outputs\n",
    "FINAL_MODEL_DIR = Path(f'{GDRIVE_BASE}/FasalVaidya_YOLOv8_Model')\n",
    "\n",
    "print(\"‚úÖ Configuration loaded!\")\n",
    "print(f\"üìä Total crops: {len(CROP_DATASETS)}\")\n",
    "total_classes = sum(len(v) for v in CLASS_RENAME_MAP.values())\n",
    "print(f\"üìä Total classes: {total_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee51fba1",
   "metadata": {},
   "source": [
    "## üîç Section 3: Dataset Discovery & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1902d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üîç Discover and analyze all datasets\n",
    "# ==========================================\n",
    "\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "def discover_datasets():\n",
    "    \"\"\"Discover all available datasets and their class distributions.\"\"\"\n",
    "    \n",
    "    dataset_info = {}\n",
    "    all_classes = []\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîç DATASET DISCOVERY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for crop, rel_path in CROP_DATASETS.items():\n",
    "        full_path = Path(GDRIVE_BASE) / rel_path\n",
    "        \n",
    "        if not full_path.exists():\n",
    "            print(f\"‚ö†Ô∏è  {crop.upper()}: Path not found - {full_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Find all subdirectories (classes)\n",
    "        class_dirs = [d for d in full_path.iterdir() if d.is_dir()]\n",
    "        \n",
    "        if not class_dirs:\n",
    "            print(f\"‚ö†Ô∏è  {crop.upper()}: No class subdirectories found\")\n",
    "            continue\n",
    "        \n",
    "        crop_info = {\n",
    "            'path': full_path,\n",
    "            'classes': {},\n",
    "            'total_images': 0\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüå± {crop.upper()} ({full_path})\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rename_map = CLASS_RENAME_MAP.get(crop, {})\n",
    "        \n",
    "        for class_dir in sorted(class_dirs):\n",
    "            original_name = class_dir.name\n",
    "            \n",
    "            # Get standardized name or create one\n",
    "            if original_name in rename_map:\n",
    "                std_name = rename_map[original_name]\n",
    "            else:\n",
    "                # Auto-generate standardized name\n",
    "                std_name = f\"{crop}_{original_name.lower().replace(' ', '_').replace('-', '_')}\"\n",
    "            \n",
    "            # Count images\n",
    "            img_count = len(list(class_dir.glob('*.jpg'))) + \\\n",
    "                        len(list(class_dir.glob('*.jpeg'))) + \\\n",
    "                        len(list(class_dir.glob('*.png'))) + \\\n",
    "                        len(list(class_dir.glob('*.JPG'))) + \\\n",
    "                        len(list(class_dir.glob('*.JPEG'))) + \\\n",
    "                        len(list(class_dir.glob('*.PNG')))\n",
    "            \n",
    "            crop_info['classes'][original_name] = {\n",
    "                'std_name': std_name,\n",
    "                'count': img_count,\n",
    "                'path': class_dir\n",
    "            }\n",
    "            crop_info['total_images'] += img_count\n",
    "            all_classes.append(std_name)\n",
    "            \n",
    "            status = \"‚úÖ\" if img_count >= CONFIG['min_samples'] else \"‚ö†Ô∏è\"\n",
    "            print(f\"  {status} {original_name:35s} ‚Üí {std_name:40s} | {img_count:4d} images\")\n",
    "        \n",
    "        dataset_info[crop] = crop_info\n",
    "        print(f\"  üìä Subtotal: {crop_info['total_images']} images across {len(crop_info['classes'])} classes\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    total = sum(d['total_images'] for d in dataset_info.values())\n",
    "    print(f\"‚úÖ Found {len(dataset_info)} crops\")\n",
    "    print(f\"‚úÖ Total classes: {len(all_classes)}\")\n",
    "    print(f\"‚úÖ Total images: {total:,}\")\n",
    "    \n",
    "    return dataset_info, sorted(set(all_classes))\n",
    "\n",
    "dataset_info, all_class_names = discover_datasets()\n",
    "print(f\"\\nüìã All classes ({len(all_class_names)}):\")\n",
    "for i, name in enumerate(all_class_names, 1):\n",
    "    print(f\"  {i:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538829d",
   "metadata": {},
   "source": [
    "## üìä Section 4: Visualize Dataset Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb25426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üìä Visualize class distribution\n",
    "# ==========================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_distribution(dataset_info):\n",
    "    \"\"\"Create visualizations of dataset distribution.\"\"\"\n",
    "    \n",
    "    # Collect all class data\n",
    "    class_data = []\n",
    "    for crop, info in dataset_info.items():\n",
    "        for orig_name, class_info in info['classes'].items():\n",
    "            class_data.append({\n",
    "                'crop': crop,\n",
    "                'class': class_info['std_name'],\n",
    "                'count': class_info['count']\n",
    "            })\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(class_data)\n",
    "    \n",
    "    # Figure 1: Class distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Bar chart of all classes\n",
    "    ax1 = axes[0, 0]\n",
    "    colors = plt.cm.Set3(range(len(df)))\n",
    "    bars = ax1.barh(df['class'], df['count'], color=[plt.cm.tab20(i % 20) for i in range(len(df))])\n",
    "    ax1.axvline(x=CONFIG['min_samples'], color='r', linestyle='--', label=f\"Min: {CONFIG['min_samples']}\")\n",
    "    ax1.axvline(x=CONFIG['max_samples'], color='g', linestyle='--', label=f\"Max: {CONFIG['max_samples']}\")\n",
    "    ax1.set_xlabel('Number of Images')\n",
    "    ax1.set_ylabel('Class')\n",
    "    ax1.set_title('üìä Image Count per Class')\n",
    "    ax1.legend()\n",
    "    ax1.tick_params(axis='y', labelsize=7)\n",
    "    \n",
    "    # 2. Per-crop totals\n",
    "    ax2 = axes[0, 1]\n",
    "    crop_totals = df.groupby('crop')['count'].sum().sort_values(ascending=True)\n",
    "    ax2.barh(crop_totals.index, crop_totals.values, color=plt.cm.Pastel1(range(len(crop_totals))))\n",
    "    ax2.set_xlabel('Number of Images')\n",
    "    ax2.set_title('üåæ Images per Crop')\n",
    "    for i, v in enumerate(crop_totals.values):\n",
    "        ax2.text(v + 10, i, str(v), va='center')\n",
    "    \n",
    "    # 3. Classes per crop\n",
    "    ax3 = axes[1, 0]\n",
    "    crop_classes = df.groupby('crop').size().sort_values(ascending=True)\n",
    "    ax3.barh(crop_classes.index, crop_classes.values, color=plt.cm.Pastel2(range(len(crop_classes))))\n",
    "    ax3.set_xlabel('Number of Classes')\n",
    "    ax3.set_title('üìã Classes per Crop')\n",
    "    for i, v in enumerate(crop_classes.values):\n",
    "        ax3.text(v + 0.1, i, str(v), va='center')\n",
    "    \n",
    "    # 4. Distribution histogram\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.hist(df['count'], bins=20, edgecolor='black', alpha=0.7)\n",
    "    ax4.axvline(x=CONFIG['min_samples'], color='r', linestyle='--', label=f\"Min threshold: {CONFIG['min_samples']}\")\n",
    "    ax4.axvline(x=CONFIG['max_samples'], color='g', linestyle='--', label=f\"Max threshold: {CONFIG['max_samples']}\")\n",
    "    ax4.axvline(x=df['count'].mean(), color='blue', linestyle='-', label=f\"Mean: {df['count'].mean():.0f}\")\n",
    "    ax4.set_xlabel('Number of Images')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('üìà Distribution of Class Sizes')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/dataset_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary stats\n",
    "    print(\"\\nüìä Dataset Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Total classes: {len(df)}\")\n",
    "    print(f\"  ‚Ä¢ Total images: {df['count'].sum():,}\")\n",
    "    print(f\"  ‚Ä¢ Mean images/class: {df['count'].mean():.1f}\")\n",
    "    print(f\"  ‚Ä¢ Min images/class: {df['count'].min()}\")\n",
    "    print(f\"  ‚Ä¢ Max images/class: {df['count'].max()}\")\n",
    "    \n",
    "    under_min = len(df[df['count'] < CONFIG['min_samples']])\n",
    "    over_max = len(df[df['count'] > CONFIG['max_samples']])\n",
    "    print(f\"\\n‚ö†Ô∏è Classes below minimum ({CONFIG['min_samples']}): {under_min}\")\n",
    "    print(f\"üìà Classes above maximum ({CONFIG['max_samples']}): {over_max}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "class_df = visualize_distribution(dataset_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e40a8c",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Section 5: Create Balanced YOLO Dataset\n",
    "\n",
    "YOLOv8 classification expects the following directory structure:\n",
    "```\n",
    "dataset/\n",
    "‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ class1/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img1.jpg\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ img2.jpg\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ class2/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ val/\n",
    "    ‚îú‚îÄ‚îÄ class1/\n",
    "    ‚îî‚îÄ‚îÄ class2/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07e33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ‚öñÔ∏è Create balanced YOLO-format dataset\n",
    "# ==========================================\n",
    "\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def create_yolo_dataset(dataset_info, all_classes, output_dir, \n",
    "                        min_samples=150, max_samples=400, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Create a balanced YOLO classification dataset.\n",
    "    \n",
    "    - Undersample classes with > max_samples\n",
    "    - Augment classes with < min_samples\n",
    "    - Split into train/val\n",
    "    \"\"\"\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    train_dir = output_dir / 'train'\n",
    "    val_dir = output_dir / 'val'\n",
    "    \n",
    "    # Clean previous dataset\n",
    "    if output_dir.exists():\n",
    "        shutil.rmtree(output_dir)\n",
    "    \n",
    "    # Create directories\n",
    "    for class_name in all_classes:\n",
    "        (train_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "        (val_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚öñÔ∏è CREATING BALANCED YOLO DATASET\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üìÅ Output: {output_dir}\")\n",
    "    print(f\"üìä Min samples/class: {min_samples}\")\n",
    "    print(f\"üìä Max samples/class: {max_samples}\")\n",
    "    print(f\"üìä Train/Val split: {train_split:.0%}/{1-train_split:.0%}\")\n",
    "    print()\n",
    "    \n",
    "    stats = {'train': defaultdict(int), 'val': defaultdict(int), 'augmented': 0}\n",
    "    \n",
    "    for crop, info in tqdm(dataset_info.items(), desc=\"Processing crops\"):\n",
    "        for orig_name, class_info in info['classes'].items():\n",
    "            std_name = class_info['std_name']\n",
    "            src_path = class_info['path']\n",
    "            \n",
    "            # Get all image files\n",
    "            images = list(src_path.glob('*.jpg')) + \\\n",
    "                     list(src_path.glob('*.jpeg')) + \\\n",
    "                     list(src_path.glob('*.png')) + \\\n",
    "                     list(src_path.glob('*.JPG')) + \\\n",
    "                     list(src_path.glob('*.JPEG')) + \\\n",
    "                     list(src_path.glob('*.PNG'))\n",
    "            \n",
    "            if not images:\n",
    "                continue\n",
    "            \n",
    "            # Shuffle images\n",
    "            random.shuffle(images)\n",
    "            \n",
    "            # Balance: undersample or prepare for augmentation\n",
    "            count = len(images)\n",
    "            \n",
    "            if count > max_samples:\n",
    "                # Undersample\n",
    "                images = images[:max_samples]\n",
    "            \n",
    "            # Split into train/val\n",
    "            split_idx = int(len(images) * train_split)\n",
    "            train_images = images[:split_idx]\n",
    "            val_images = images[split_idx:]\n",
    "            \n",
    "            # Copy validation images (no augmentation)\n",
    "            for img_path in val_images:\n",
    "                dst = val_dir / std_name / f\"{std_name}_{img_path.name}\"\n",
    "                shutil.copy2(img_path, dst)\n",
    "                stats['val'][std_name] += 1\n",
    "            \n",
    "            # Process training images\n",
    "            train_target = int(min_samples * train_split) if count < min_samples else len(train_images)\n",
    "            \n",
    "            for i, img_path in enumerate(train_images):\n",
    "                dst = train_dir / std_name / f\"{std_name}_{img_path.name}\"\n",
    "                shutil.copy2(img_path, dst)\n",
    "                stats['train'][std_name] += 1\n",
    "            \n",
    "            # Augment if needed\n",
    "            if count < min_samples:\n",
    "                needed = train_target - len(train_images)\n",
    "                if needed > 0:\n",
    "                    augment_images(train_images, train_dir / std_name, std_name, needed, stats)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä DATASET SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    total_train = sum(stats['train'].values())\n",
    "    total_val = sum(stats['val'].values())\n",
    "    \n",
    "    print(f\"‚úÖ Train images: {total_train:,}\")\n",
    "    print(f\"‚úÖ Val images: {total_val:,}\")\n",
    "    print(f\"‚úÖ Total images: {total_train + total_val:,}\")\n",
    "    print(f\"üìà Augmented images: {stats['augmented']:,}\")\n",
    "    \n",
    "    return output_dir, stats\n",
    "\n",
    "\n",
    "def augment_images(source_images, output_dir, class_name, count, stats):\n",
    "    \"\"\"Create augmented copies of images.\"\"\"\n",
    "    from PIL import ImageEnhance, ImageFilter\n",
    "    \n",
    "    augmentations = [\n",
    "        lambda img: img.transpose(Image.FLIP_LEFT_RIGHT),\n",
    "        lambda img: img.rotate(15, fillcolor=(255, 255, 255)),\n",
    "        lambda img: img.rotate(-15, fillcolor=(255, 255, 255)),\n",
    "        lambda img: ImageEnhance.Brightness(img).enhance(1.2),\n",
    "        lambda img: ImageEnhance.Brightness(img).enhance(0.8),\n",
    "        lambda img: ImageEnhance.Contrast(img).enhance(1.2),\n",
    "        lambda img: img.filter(ImageFilter.GaussianBlur(radius=1)),\n",
    "    ]\n",
    "    \n",
    "    created = 0\n",
    "    idx = 0\n",
    "    \n",
    "    while created < count:\n",
    "        src_img_path = source_images[idx % len(source_images)]\n",
    "        aug_fn = random.choice(augmentations)\n",
    "        \n",
    "        try:\n",
    "            with Image.open(src_img_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                augmented = aug_fn(img)\n",
    "                \n",
    "                dst_name = f\"{class_name}_aug_{created}_{src_img_path.stem}.jpg\"\n",
    "                augmented.save(output_dir / dst_name, 'JPEG', quality=95)\n",
    "                \n",
    "                created += 1\n",
    "                stats['train'][class_name] += 1\n",
    "                stats['augmented'] += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        idx += 1\n",
    "        if idx > count * 2:  # Safety break\n",
    "            break\n",
    "\n",
    "# Create the dataset\n",
    "yolo_dataset_path, dataset_stats = create_yolo_dataset(\n",
    "    dataset_info, \n",
    "    all_class_names,\n",
    "    YOLO_DATASET_DIR,\n",
    "    min_samples=CONFIG['min_samples'],\n",
    "    max_samples=CONFIG['max_samples'],\n",
    "    train_split=CONFIG['train_split']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üìã Verify dataset structure\n",
    "# ==========================================\n",
    "\n",
    "def verify_yolo_dataset(dataset_path):\n",
    "    \"\"\"Verify the YOLO dataset structure and print statistics.\"\"\"\n",
    "    \n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìã YOLO DATASET VERIFICATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        split_path = dataset_path / split\n",
    "        \n",
    "        if not split_path.exists():\n",
    "            print(f\"‚ö†Ô∏è {split} directory not found!\")\n",
    "            continue\n",
    "        \n",
    "        classes = sorted([d.name for d in split_path.iterdir() if d.is_dir()])\n",
    "        \n",
    "        print(f\"\\nüìÅ {split.upper()} ({len(classes)} classes)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        total = 0\n",
    "        class_counts = {}\n",
    "        for class_name in classes:\n",
    "            class_path = split_path / class_name\n",
    "            count = len(list(class_path.glob('*.jpg'))) + len(list(class_path.glob('*.jpeg'))) + len(list(class_path.glob('*.png')))\n",
    "            class_counts[class_name] = count\n",
    "            total += count\n",
    "        \n",
    "        # Print sorted by count\n",
    "        for class_name, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"  {class_name:45s}: {count:4d} images\")\n",
    "        \n",
    "        if len(classes) > 10:\n",
    "            print(f\"  ... and {len(classes) - 10} more classes\")\n",
    "        \n",
    "        print(f\"\\n  üìä Total: {total:,} images\")\n",
    "    \n",
    "    # Save class names file\n",
    "    classes_file = dataset_path / 'classes.txt'\n",
    "    train_classes = sorted([d.name for d in (dataset_path / 'train').iterdir() if d.is_dir()])\n",
    "    with open(classes_file, 'w') as f:\n",
    "        for cls in train_classes:\n",
    "            f.write(f\"{cls}\\n\")\n",
    "    print(f\"\\n‚úÖ Saved class names to {classes_file}\")\n",
    "    \n",
    "    return train_classes\n",
    "\n",
    "yolo_classes = verify_yolo_dataset(YOLO_DATASET_DIR)\n",
    "print(f\"\\nüìä Total classes for training: {len(yolo_classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b430fa7e",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Section 6: Train YOLOv8 Classification Model\n",
    "\n",
    "### Model Options:\n",
    "- `yolov8n-cls`: Nano - fastest, smallest (5.3MB)\n",
    "- `yolov8s-cls`: Small - best balance (11.4MB) ‚¨ÖÔ∏è **Recommended**\n",
    "- `yolov8m-cls`: Medium - more accurate (36.6MB)\n",
    "- `yolov8l-cls`: Large - higher accuracy (83.3MB)\n",
    "- `yolov8x-cls`: XLarge - highest accuracy (136.0MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9047cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üèãÔ∏è Initialize and train YOLOv8 Classification Model\n",
    "# ==========================================\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model selection\n",
    "MODEL_VARIANT = CONFIG['model_variant']  # 'yolov8s-cls' recommended\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üèãÔ∏è YOLOv8 CLASSIFICATION TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üì¶ Model: {MODEL_VARIANT}\")\n",
    "print(f\"üìÅ Dataset: {YOLO_DATASET_DIR}\")\n",
    "print(f\"üìä Image size: {CONFIG['img_size']}x{CONFIG['img_size']}\")\n",
    "print(f\"üìä Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"üìä Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"üìä Early stopping patience: {CONFIG['patience']}\")\n",
    "print()\n",
    "\n",
    "# Load pretrained YOLOv8 classification model\n",
    "model = YOLO(f'{MODEL_VARIANT}.pt')\n",
    "\n",
    "print(f\"‚úÖ Loaded {MODEL_VARIANT} pretrained model\")\n",
    "print(f\"üìä Model info:\")\n",
    "print(model.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üöÄ Start Training\n",
    "# ==========================================\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "# YOLOv8 automatically handles:\n",
    "# - Data augmentation (flips, rotations, color jitter)\n",
    "# - Learning rate scheduling\n",
    "# - Best model checkpointing\n",
    "# - Early stopping\n",
    "\n",
    "results = model.train(\n",
    "    data=str(YOLO_DATASET_DIR),     # Path to dataset\n",
    "    epochs=CONFIG['epochs'],         # Number of epochs\n",
    "    imgsz=CONFIG['img_size'],        # Image size\n",
    "    batch=CONFIG['batch_size'],      # Batch size\n",
    "    patience=CONFIG['patience'],     # Early stopping patience\n",
    "    device=0 if DEVICE == 'cuda' else 'cpu',  # GPU or CPU\n",
    "    project=str(OUTPUT_DIR),         # Output directory\n",
    "    name='fasalvaidya_yolov8',       # Run name\n",
    "    exist_ok=True,                   # Overwrite existing\n",
    "    pretrained=True,                 # Use pretrained weights\n",
    "    optimizer='AdamW',               # Optimizer\n",
    "    lr0=0.001,                       # Initial learning rate\n",
    "    lrf=0.01,                        # Final LR (lr0 * lrf)\n",
    "    momentum=0.937,                  # Momentum\n",
    "    weight_decay=0.0005,             # Weight decay\n",
    "    warmup_epochs=3,                 # Warmup epochs\n",
    "    warmup_momentum=0.8,             # Warmup momentum\n",
    "    warmup_bias_lr=0.1,              # Warmup bias LR\n",
    "    close_mosaic=0,                  # Disable mosaic (for classification)\n",
    "    amp=True,                        # Mixed precision\n",
    "    fraction=1.0,                    # Dataset fraction\n",
    "    seed=42,                         # Random seed\n",
    "    verbose=True,                    # Verbose output\n",
    "    plots=True,                      # Generate plots\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training completed in {elapsed/60:.1f} minutes ({elapsed/3600:.2f} hours)\")\n",
    "print(f\"üìÅ Results saved to: {OUTPUT_DIR / 'fasalvaidya_yolov8'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e05839",
   "metadata": {},
   "source": [
    "## üìà Section 7: Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ab18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üìà Display training curves and results\n",
    "# ==========================================\n",
    "\n",
    "from IPython.display import Image as IPImage, display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Training results directory\n",
    "results_dir = OUTPUT_DIR / 'fasalvaidya_yolov8'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìà TRAINING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display training curves\n",
    "curves_to_show = ['results.png', 'confusion_matrix.png', 'confusion_matrix_normalized.png']\n",
    "\n",
    "for curve_name in curves_to_show:\n",
    "    curve_path = results_dir / curve_name\n",
    "    if curve_path.exists():\n",
    "        print(f\"\\nüìä {curve_name}:\")\n",
    "        display(IPImage(filename=str(curve_path), width=800))\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {curve_name} not found\")\n",
    "\n",
    "# Print metrics from CSV if available\n",
    "results_csv = results_dir / 'results.csv'\n",
    "if results_csv.exists():\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(results_csv)\n",
    "    df.columns = df.columns.str.strip()  # Clean column names\n",
    "    \n",
    "    print(\"\\nüìä Training Metrics (last 5 epochs):\")\n",
    "    print(df.tail().to_string())\n",
    "    \n",
    "    # Best epoch\n",
    "    if 'metrics/accuracy_top1' in df.columns:\n",
    "        best_idx = df['metrics/accuracy_top1'].idxmax()\n",
    "        print(f\"\\nüèÜ Best Top-1 Accuracy: {df.loc[best_idx, 'metrics/accuracy_top1']:.4f} (epoch {best_idx + 1})\")\n",
    "    if 'metrics/accuracy_top5' in df.columns:\n",
    "        print(f\"üèÜ Best Top-5 Accuracy: {df['metrics/accuracy_top5'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3163ba4",
   "metadata": {},
   "source": [
    "## üß™ Section 8: Evaluate Model on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804dd7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üß™ Run validation with the best model\n",
    "# ==========================================\n",
    "\n",
    "# Load best model\n",
    "best_model_path = results_dir / 'weights' / 'best.pt'\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(f\"‚úÖ Loading best model from: {best_model_path}\")\n",
    "    best_model = YOLO(str(best_model_path))\n",
    "    \n",
    "    # Run validation\n",
    "    print(\"\\nüß™ Running validation...\")\n",
    "    val_results = best_model.val(\n",
    "        data=str(YOLO_DATASET_DIR),\n",
    "        split='val',\n",
    "        imgsz=CONFIG['img_size'],\n",
    "        batch=CONFIG['batch_size'],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä VALIDATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"‚úÖ Top-1 Accuracy: {val_results.top1:.4f} ({val_results.top1*100:.2f}%)\")\n",
    "    print(f\"‚úÖ Top-5 Accuracy: {val_results.top5:.4f} ({val_results.top5*100:.2f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Best model not found at {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üìä Per-class accuracy analysis\n",
    "# ==========================================\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def analyze_per_class_performance(model, dataset_path, class_names):\n",
    "    \"\"\"Analyze per-class performance with detailed metrics.\"\"\"\n",
    "    \n",
    "    val_dir = Path(dataset_path) / 'val'\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    confidences = []\n",
    "    \n",
    "    print(\"üîç Running predictions on validation set...\")\n",
    "    \n",
    "    for class_idx, class_name in enumerate(tqdm(class_names)):\n",
    "        class_dir = val_dir / class_name\n",
    "        if not class_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        images = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.jpeg')) + list(class_dir.glob('*.png'))\n",
    "        \n",
    "        for img_path in images:\n",
    "            try:\n",
    "                results = model.predict(str(img_path), verbose=False)\n",
    "                pred_class = results[0].probs.top1\n",
    "                confidence = results[0].probs.top1conf.item()\n",
    "                \n",
    "                y_true.append(class_idx)\n",
    "                y_pred.append(pred_class)\n",
    "                confidences.append(confidence)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä CLASSIFICATION REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    # Print per-class metrics\n",
    "    print(f\"\\n{'Class':<45} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Support':>10}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    class_metrics = []\n",
    "    for class_name in class_names:\n",
    "        if class_name in report:\n",
    "            m = report[class_name]\n",
    "            class_metrics.append({\n",
    "                'name': class_name,\n",
    "                'precision': m['precision'],\n",
    "                'recall': m['recall'],\n",
    "                'f1': m['f1-score'],\n",
    "                'support': m['support']\n",
    "            })\n",
    "            print(f\"{class_name:<45} {m['precision']:>10.3f} {m['recall']:>10.3f} {m['f1-score']:>10.3f} {m['support']:>10.0f}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'Macro Avg':<45} {report['macro avg']['precision']:>10.3f} {report['macro avg']['recall']:>10.3f} {report['macro avg']['f1-score']:>10.3f}\")\n",
    "    print(f\"{'Weighted Avg':<45} {report['weighted avg']['precision']:>10.3f} {report['weighted avg']['recall']:>10.3f} {report['weighted avg']['f1-score']:>10.3f}\")\n",
    "    \n",
    "    # Find worst performing classes\n",
    "    print(\"\\n‚ö†Ô∏è Bottom 5 Classes by F1-Score:\")\n",
    "    sorted_classes = sorted(class_metrics, key=lambda x: x['f1'])\n",
    "    for i, m in enumerate(sorted_classes[:5], 1):\n",
    "        print(f\"  {i}. {m['name']}: F1={m['f1']:.3f}, Precision={m['precision']:.3f}, Recall={m['recall']:.3f}\")\n",
    "    \n",
    "    # Average confidence\n",
    "    print(f\"\\nüìä Average Prediction Confidence: {np.mean(confidences):.3f}\")\n",
    "    \n",
    "    return report, y_true, y_pred\n",
    "\n",
    "# Run analysis\n",
    "if best_model_path.exists():\n",
    "    report, y_true, y_pred = analyze_per_class_performance(best_model, YOLO_DATASET_DIR, yolo_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d41fed",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Section 9: Test Predictions on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a420284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üñºÔ∏è Visualize predictions on random samples\n",
    "# ==========================================\n",
    "\n",
    "def visualize_predictions(model, dataset_path, class_names, num_samples=12):\n",
    "    \"\"\"Show predictions on random validation samples.\"\"\"\n",
    "    \n",
    "    val_dir = Path(dataset_path) / 'val'\n",
    "    \n",
    "    # Collect random images from different classes\n",
    "    all_images = []\n",
    "    for class_name in class_names:\n",
    "        class_dir = val_dir / class_name\n",
    "        if class_dir.exists():\n",
    "            images = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.jpeg'))\n",
    "            for img in images[:3]:  # Max 3 per class\n",
    "                all_images.append((img, class_name))\n",
    "    \n",
    "    # Random sample\n",
    "    random.shuffle(all_images)\n",
    "    samples = all_images[:num_samples]\n",
    "    \n",
    "    # Create figure\n",
    "    cols = 4\n",
    "    rows = (num_samples + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n",
    "    axes = axes.flatten() if num_samples > 1 else [axes]\n",
    "    \n",
    "    for idx, (img_path, true_class) in enumerate(samples):\n",
    "        # Predict\n",
    "        results = model.predict(str(img_path), verbose=False)\n",
    "        pred_class_idx = results[0].probs.top1\n",
    "        pred_class = class_names[pred_class_idx]\n",
    "        confidence = results[0].probs.top1conf.item()\n",
    "        \n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Plot\n",
    "        ax = axes[idx]\n",
    "        ax.imshow(img)\n",
    "        \n",
    "        correct = pred_class == true_class\n",
    "        color = 'green' if correct else 'red'\n",
    "        symbol = '‚úÖ' if correct else '‚ùå'\n",
    "        \n",
    "        ax.set_title(f\"{symbol} Pred: {pred_class}\\n(True: {true_class})\\nConf: {confidence:.2%}\", \n",
    "                     fontsize=9, color=color)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(samples), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(OUTPUT_DIR / 'fasalvaidya_yolov8' / 'sample_predictions.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Show predictions\n",
    "if best_model_path.exists():\n",
    "    print(\"üñºÔ∏è Sample Predictions:\")\n",
    "    visualize_predictions(best_model, YOLO_DATASET_DIR, yolo_classes, num_samples=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e93a7a5",
   "metadata": {},
   "source": [
    "## üì¶ Section 10: Export Model to Multiple Formats\n",
    "\n",
    "YOLOv8 supports export to:\n",
    "- **ONNX**: Cross-platform, web deployment\n",
    "- **TensorFlow Lite**: Mobile (Android/iOS)\n",
    "- **CoreML**: iOS native\n",
    "- **TensorRT**: NVIDIA GPU optimization\n",
    "- **OpenVINO**: Intel CPU optimization\n",
    "- **NCNN**: Mobile/embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af68f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üì¶ Export to ONNX format\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üì¶ EXPORTING MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if best_model_path.exists():\n",
    "    # Export to ONNX\n",
    "    print(\"\\nüîÑ Exporting to ONNX...\")\n",
    "    onnx_path = best_model.export(\n",
    "        format='onnx',\n",
    "        imgsz=CONFIG['img_size'],\n",
    "        half=False,  # FP32 for compatibility\n",
    "        simplify=True,\n",
    "        opset=12,\n",
    "    )\n",
    "    print(f\"‚úÖ ONNX model saved: {onnx_path}\")\n",
    "    \n",
    "    # Get file size\n",
    "    onnx_size = Path(onnx_path).stat().st_size / (1024 * 1024)\n",
    "    print(f\"üìä ONNX model size: {onnx_size:.2f} MB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Best model not found, skipping export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üì± Export to TensorFlow Lite (for mobile)\n",
    "# ==========================================\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(\"\\nüîÑ Exporting to TensorFlow Lite...\")\n",
    "    \n",
    "    try:\n",
    "        tflite_path = best_model.export(\n",
    "            format='tflite',\n",
    "            imgsz=CONFIG['img_size'],\n",
    "            half=False,  # FP32\n",
    "        )\n",
    "        print(f\"‚úÖ TFLite model saved: {tflite_path}\")\n",
    "        \n",
    "        # Get file size\n",
    "        tflite_size = Path(tflite_path).stat().st_size / (1024 * 1024)\n",
    "        print(f\"üìä TFLite model size: {tflite_size:.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è TFLite export failed: {e}\")\n",
    "        print(\"   This is common on some systems. Try the following:\")\n",
    "        print(\"   !pip install tensorflow>=2.10.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e7cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üçé Export to CoreML (for iOS)\n",
    "# ==========================================\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(\"\\nüîÑ Exporting to CoreML (iOS)...\")\n",
    "    \n",
    "    try:\n",
    "        coreml_path = best_model.export(\n",
    "            format='coreml',\n",
    "            imgsz=CONFIG['img_size'],\n",
    "            half=False,\n",
    "        )\n",
    "        print(f\"‚úÖ CoreML model saved: {coreml_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è CoreML export failed: {e}\")\n",
    "        print(\"   Install coremltools: pip install coremltools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a8052",
   "metadata": {},
   "source": [
    "## üíæ Section 11: Save Final Model and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3046a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üíæ Save model and metadata to Google Drive\n",
    "# ==========================================\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create final model directory\n",
    "FINAL_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üíæ SAVING FINAL MODEL\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìÅ Destination: {FINAL_MODEL_DIR}\")\n",
    "\n",
    "# Copy best model weights\n",
    "if best_model_path.exists():\n",
    "    shutil.copy2(best_model_path, FINAL_MODEL_DIR / 'best.pt')\n",
    "    print(f\"‚úÖ Copied best.pt\")\n",
    "\n",
    "# Copy last model weights\n",
    "last_model_path = results_dir / 'weights' / 'last.pt'\n",
    "if last_model_path.exists():\n",
    "    shutil.copy2(last_model_path, FINAL_MODEL_DIR / 'last.pt')\n",
    "    print(f\"‚úÖ Copied last.pt\")\n",
    "\n",
    "# Copy ONNX model\n",
    "onnx_model = results_dir / 'weights' / 'best.onnx'\n",
    "if onnx_model.exists():\n",
    "    shutil.copy2(onnx_model, FINAL_MODEL_DIR / 'fasalvaidya_yolov8.onnx')\n",
    "    print(f\"‚úÖ Copied ONNX model\")\n",
    "\n",
    "# Copy TFLite model\n",
    "for tflite_file in (results_dir / 'weights').glob('*.tflite'):\n",
    "    shutil.copy2(tflite_file, FINAL_MODEL_DIR / 'fasalvaidya_yolov8.tflite')\n",
    "    print(f\"‚úÖ Copied TFLite model\")\n",
    "    break\n",
    "\n",
    "# Save class labels\n",
    "with open(FINAL_MODEL_DIR / 'labels.txt', 'w') as f:\n",
    "    for class_name in yolo_classes:\n",
    "        f.write(f\"{class_name}\\n\")\n",
    "print(f\"‚úÖ Saved labels.txt ({len(yolo_classes)} classes)\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_name': 'FasalVaidya YOLOv8 Classification',\n",
    "    'model_variant': CONFIG['model_variant'],\n",
    "    'version': '1.0.0',\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'framework': 'Ultralytics YOLOv8',\n",
    "    'ultralytics_version': ultralytics.__version__,\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'input_shape': [1, 3, CONFIG['img_size'], CONFIG['img_size']],\n",
    "    'image_size': CONFIG['img_size'],\n",
    "    'num_classes': len(yolo_classes),\n",
    "    'classes': yolo_classes,\n",
    "    'crops': list(CROP_DATASETS.keys()),\n",
    "    'training_config': CONFIG,\n",
    "    'class_mapping': CLASS_RENAME_MAP,\n",
    "    'metrics': {\n",
    "        'top1_accuracy': float(val_results.top1) if 'val_results' in dir() else None,\n",
    "        'top5_accuracy': float(val_results.top5) if 'val_results' in dir() else None,\n",
    "    },\n",
    "    'export_formats': ['pt', 'onnx', 'tflite', 'coreml'],\n",
    "}\n",
    "\n",
    "with open(FINAL_MODEL_DIR / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"‚úÖ Saved metadata.json\")\n",
    "\n",
    "# Copy training results\n",
    "if (results_dir / 'results.csv').exists():\n",
    "    shutil.copy2(results_dir / 'results.csv', FINAL_MODEL_DIR / 'training_results.csv')\n",
    "    print(f\"‚úÖ Copied training_results.csv\")\n",
    "\n",
    "# Copy confusion matrix\n",
    "for img_file in results_dir.glob('*.png'):\n",
    "    shutil.copy2(img_file, FINAL_MODEL_DIR / img_file.name)\n",
    "print(f\"‚úÖ Copied result images\")\n",
    "\n",
    "print(\"\\nüì¶ Final model package contents:\")\n",
    "for f in sorted(FINAL_MODEL_DIR.iterdir()):\n",
    "    size = f.stat().st_size / (1024 * 1024) if f.is_file() else 0\n",
    "    print(f\"  üìÑ {f.name}: {size:.2f} MB\" if size > 0.01 else f\"  üìÑ {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee8742",
   "metadata": {},
   "source": [
    "## üîç Section 12: Validate Exported Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b6bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üîç Validate ONNX Model\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîç VALIDATING EXPORTED MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test ONNX model\n",
    "onnx_path = FINAL_MODEL_DIR / 'fasalvaidya_yolov8.onnx'\n",
    "if onnx_path.exists():\n",
    "    print(\"\\nüì¶ Testing ONNX model...\")\n",
    "    \n",
    "    try:\n",
    "        import onnxruntime as ort\n",
    "        \n",
    "        # Create session\n",
    "        session = ort.InferenceSession(str(onnx_path))\n",
    "        \n",
    "        # Get input info\n",
    "        input_info = session.get_inputs()[0]\n",
    "        print(f\"  ‚úÖ Input name: {input_info.name}\")\n",
    "        print(f\"  ‚úÖ Input shape: {input_info.shape}\")\n",
    "        print(f\"  ‚úÖ Input type: {input_info.type}\")\n",
    "        \n",
    "        # Get output info\n",
    "        output_info = session.get_outputs()[0]\n",
    "        print(f\"  ‚úÖ Output name: {output_info.name}\")\n",
    "        print(f\"  ‚úÖ Output shape: {output_info.shape}\")\n",
    "        \n",
    "        # Test inference with dummy data\n",
    "        import numpy as np\n",
    "        dummy_input = np.random.randn(1, 3, CONFIG['img_size'], CONFIG['img_size']).astype(np.float32)\n",
    "        outputs = session.run(None, {input_info.name: dummy_input})\n",
    "        print(f\"  ‚úÖ Test inference successful!\")\n",
    "        print(f\"  ‚úÖ Output shape: {outputs[0].shape}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"  ‚ö†Ô∏è onnxruntime not installed. Run: pip install onnxruntime\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå ONNX validation failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è ONNX model not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2189652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üîç Validate TFLite Model\n",
    "# ==========================================\n",
    "\n",
    "tflite_path = FINAL_MODEL_DIR / 'fasalvaidya_yolov8.tflite'\n",
    "if tflite_path.exists():\n",
    "    print(\"\\nüì± Testing TFLite model...\")\n",
    "    \n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # Load TFLite model\n",
    "        interpreter = tf.lite.Interpreter(model_path=str(tflite_path))\n",
    "        interpreter.allocate_tensors()\n",
    "        \n",
    "        # Get input/output details\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "        \n",
    "        print(f\"  ‚úÖ Input shape: {input_details[0]['shape']}\")\n",
    "        print(f\"  ‚úÖ Input dtype: {input_details[0]['dtype']}\")\n",
    "        print(f\"  ‚úÖ Output shape: {output_details[0]['shape']}\")\n",
    "        \n",
    "        # Test inference\n",
    "        input_shape = input_details[0]['shape']\n",
    "        test_input = np.random.randn(*input_shape).astype(np.float32)\n",
    "        interpreter.set_tensor(input_details[0]['index'], test_input)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        \n",
    "        print(f\"  ‚úÖ Test inference successful!\")\n",
    "        print(f\"  ‚úÖ Output shape: {output.shape}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"  ‚ö†Ô∏è TensorFlow not installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå TFLite validation failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è TFLite model not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cc07d3",
   "metadata": {},
   "source": [
    "## ‚ö° Section 13: Benchmark Inference Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ‚ö° Benchmark inference speed\n",
    "# ==========================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def benchmark_model(model_path, model_type, num_runs=100):\n",
    "    \"\"\"Benchmark inference speed for different model formats.\"\"\"\n",
    "    \n",
    "    # Create dummy image\n",
    "    dummy_img = np.random.randint(0, 255, (CONFIG['img_size'], CONFIG['img_size'], 3), dtype=np.uint8)\n",
    "    \n",
    "    # Warmup runs\n",
    "    warmup = 10\n",
    "    \n",
    "    if model_type == 'pytorch':\n",
    "        model = YOLO(str(model_path))\n",
    "        # Warmup\n",
    "        for _ in range(warmup):\n",
    "            model.predict(dummy_img, verbose=False)\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(num_runs):\n",
    "            start = time.perf_counter()\n",
    "            model.predict(dummy_img, verbose=False)\n",
    "            times.append((time.perf_counter() - start) * 1000)  # ms\n",
    "        \n",
    "        return np.mean(times), np.std(times)\n",
    "    \n",
    "    elif model_type == 'onnx':\n",
    "        import onnxruntime as ort\n",
    "        session = ort.InferenceSession(str(model_path))\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        \n",
    "        # Prepare input\n",
    "        img = dummy_img.transpose(2, 0, 1).astype(np.float32) / 255.0\n",
    "        img = np.expand_dims(img, 0)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(warmup):\n",
    "            session.run(None, {input_name: img})\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(num_runs):\n",
    "            start = time.perf_counter()\n",
    "            session.run(None, {input_name: img})\n",
    "            times.append((time.perf_counter() - start) * 1000)\n",
    "        \n",
    "        return np.mean(times), np.std(times)\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚ö° INFERENCE SPEED BENCHMARK\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìä Running {100} iterations per model\\n\")\n",
    "\n",
    "# Benchmark PyTorch model\n",
    "pt_path = FINAL_MODEL_DIR / 'best.pt'\n",
    "if pt_path.exists():\n",
    "    mean_time, std_time = benchmark_model(pt_path, 'pytorch', num_runs=100)\n",
    "    print(f\"üî• PyTorch (.pt):  {mean_time:.2f} ¬± {std_time:.2f} ms  ({1000/mean_time:.1f} FPS)\")\n",
    "\n",
    "# Benchmark ONNX model\n",
    "onnx_path = FINAL_MODEL_DIR / 'fasalvaidya_yolov8.onnx'\n",
    "if onnx_path.exists():\n",
    "    try:\n",
    "        mean_time, std_time = benchmark_model(onnx_path, 'onnx', num_runs=100)\n",
    "        print(f\"üì¶ ONNX:           {mean_time:.2f} ¬± {std_time:.2f} ms  ({1000/mean_time:.1f} FPS)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è ONNX benchmark failed: {e}\")\n",
    "\n",
    "print(\"\\nüí° Note: TFLite benchmarking is best done on actual mobile devices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c3d71",
   "metadata": {},
   "source": [
    "## üì• Section 14: Download Model Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5184dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üì• Create downloadable ZIP archive\n",
    "# ==========================================\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# Create ZIP file\n",
    "zip_filename = f\"FasalVaidya_YOLOv8_Model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip\"\n",
    "zip_path = Path('/content') / zip_filename\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üì• CREATING DOWNLOAD PACKAGE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file in FINAL_MODEL_DIR.rglob('*'):\n",
    "        if file.is_file():\n",
    "            arcname = file.relative_to(FINAL_MODEL_DIR)\n",
    "            zipf.write(file, arcname)\n",
    "            print(f\"  üìÑ Added: {arcname}\")\n",
    "\n",
    "zip_size = zip_path.stat().st_size / (1024 * 1024)\n",
    "print(f\"\\n‚úÖ Created: {zip_path}\")\n",
    "print(f\"üìä Size: {zip_size:.2f} MB\")\n",
    "\n",
    "# Download link (for Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"\\nüì• Click below to download:\")\n",
    "    files.download(str(zip_path))\n",
    "except ImportError:\n",
    "    print(\"\\nüí° Download from: \" + str(zip_path))\n",
    "\n",
    "print(f\"\\nüìÅ Model also saved to Google Drive at:\")\n",
    "print(f\"   {FINAL_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2e2f9",
   "metadata": {},
   "source": [
    "## üìã Section 15: Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eef01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üìã Print final summary\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üéâ TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "üì¶ Model Information:\n",
    "   ‚Ä¢ Architecture: {CONFIG['model_variant']}\n",
    "   ‚Ä¢ Framework: Ultralytics YOLOv8 {ultralytics.__version__}\n",
    "   ‚Ä¢ Number of Classes: {len(yolo_classes)}\n",
    "   ‚Ä¢ Input Size: {CONFIG['img_size']}x{CONFIG['img_size']}\n",
    "\n",
    "üåæ Crops Covered:\n",
    "   ‚Ä¢ Rice, Wheat, Maize (Cereals)\n",
    "   ‚Ä¢ Banana, Coffee (Commercial)\n",
    "   ‚Ä¢ Ashgourd, Eggplant, Snakegourd, Bittergourd (Vegetables)\n",
    "\n",
    "üìä Training Configuration:\n",
    "   ‚Ä¢ Epochs: {CONFIG['epochs']}\n",
    "   ‚Ä¢ Batch Size: {CONFIG['batch_size']}\n",
    "   ‚Ä¢ Optimizer: AdamW\n",
    "   ‚Ä¢ Learning Rate: 0.001 ‚Üí 0.00001\n",
    "\n",
    "üìÅ Output Files:\n",
    "   ‚Ä¢ best.pt - PyTorch weights (best validation accuracy)\n",
    "   ‚Ä¢ last.pt - PyTorch weights (final epoch)\n",
    "   ‚Ä¢ fasalvaidya_yolov8.onnx - ONNX format\n",
    "   ‚Ä¢ fasalvaidya_yolov8.tflite - TensorFlow Lite\n",
    "   ‚Ä¢ labels.txt - Class labels\n",
    "   ‚Ä¢ metadata.json - Model metadata\n",
    "\n",
    "üöÄ Next Steps:\n",
    "   1. Integrate into FasalVaidya backend (ml/inference_yolov8.py)\n",
    "   2. Test on mobile devices\n",
    "   3. Compare accuracy with EfficientNet-B0 model\n",
    "   4. Deploy to production\n",
    "\n",
    "üìç Model saved to Google Drive:\n",
    "   {FINAL_MODEL_DIR}\n",
    "\"\"\")\n",
    "\n",
    "# Print accuracy if available\n",
    "if 'val_results' in dir():\n",
    "    print(f\"üìä Final Metrics:\")\n",
    "    print(f\"   ‚Ä¢ Top-1 Accuracy: {val_results.top1*100:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Top-5 Accuracy: {val_results.top5*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b2d50",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Appendix A: Using the Trained Model\n",
    "\n",
    "### Python Inference Example:\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the trained model\n",
    "model = YOLO('path/to/best.pt')\n",
    "\n",
    "# Predict on an image\n",
    "results = model.predict('leaf_image.jpg')\n",
    "\n",
    "# Get top prediction\n",
    "top_class = results[0].probs.top1\n",
    "top_conf = results[0].probs.top1conf\n",
    "class_name = results[0].names[top_class]\n",
    "\n",
    "print(f\"Prediction: {class_name} ({top_conf:.2%})\")\n",
    "```\n",
    "\n",
    "### Mobile Integration (React Native with ONNX):\n",
    "\n",
    "```javascript\n",
    "import * as ort from 'onnxruntime-react-native';\n",
    "\n",
    "// Load ONNX model\n",
    "const session = await ort.InferenceSession.create(modelPath);\n",
    "\n",
    "// Run inference\n",
    "const feeds = { 'images': inputTensor };\n",
    "const results = await session.run(feeds);\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea581214",
   "metadata": {},
   "source": [
    "## üîß Appendix B: Hyperparameter Tuning (Optional)\n",
    "\n",
    "Run this cell to perform hyperparameter optimization using Ultralytics' built-in tuner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ad83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üîß Hyperparameter Tuning (Optional - takes longer)\n",
    "# ==========================================\n",
    "\n",
    "# Uncomment to run hyperparameter tuning\n",
    "# This will search for optimal learning rate, momentum, etc.\n",
    "\n",
    "\"\"\"\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load base model\n",
    "tune_model = YOLO('yolov8s-cls.pt')\n",
    "\n",
    "# Run tuning\n",
    "tune_results = tune_model.tune(\n",
    "    data=str(YOLO_DATASET_DIR),\n",
    "    epochs=30,\n",
    "    iterations=50,  # Number of search iterations\n",
    "    optimizer='AdamW',\n",
    "    plots=True,\n",
    "    save=True,\n",
    "    val=True\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(tune_results.best)\n",
    "\"\"\"\n",
    "\n",
    "print(\"üí° Hyperparameter tuning is disabled by default.\")\n",
    "print(\"   Uncomment the code above to run tuning (takes 2-4 hours).\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
