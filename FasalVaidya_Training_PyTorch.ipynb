{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "018c21e5",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Check GPU\n",
    "\n",
    "First, check if PyTorch can detect your GPU (AMD or NVIDIA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a10b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.1+cpu\n",
      "CUDA Available (NVIDIA): False\n",
      "‚ö†Ô∏è No GPU detected - using CPU\n",
      "\n",
      "üí° To enable AMD GPU support, install: pip install torch-directml\n",
      "\n",
      "üéØ Training device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check PyTorch and GPU availability\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available (NVIDIA): {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check for DirectML (AMD GPU)\n",
    "try:\n",
    "    import torch_directml\n",
    "    dml_device = torch_directml.device()\n",
    "    print(f\"‚úÖ DirectML Available (AMD GPU): Yes\")\n",
    "    print(f\"   Device: {dml_device}\")\n",
    "    device = dml_device\n",
    "except ImportError:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"‚úÖ Using NVIDIA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(f\"‚ö†Ô∏è No GPU detected - using CPU\")\n",
    "        print(f\"\\nüí° To enable AMD GPU support, install: pip install torch-directml\")\n",
    "\n",
    "print(f\"\\nüéØ Training device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc8f669",
   "metadata": {},
   "source": [
    "### üéÆ AMD GPU Setup (If Not Detected)\n",
    "\n",
    "If DirectML is not available, install it:\n",
    "\n",
    "```bash\n",
    "pip install torch-directml\n",
    "```\n",
    "\n",
    "Then restart the kernel and run the cell above again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61a6f6",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration\n",
    "\n",
    "**IMPORTANT:** Update `DATASET_ROOT` to point to your local dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5027862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset found at: E:\\FasalVaidya\\Leaf Nutrient Data Sets\n",
      "üìÇ Contents: ['Ashgourd Nutrients', 'Banana leaves Nutrient', 'Bittergourd Nutrients', 'Coffee Nutrients', 'Cucumber Nutrients']...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ========================================\n",
    "# üìÅ UPDATE THIS PATH TO YOUR LOCAL DATASET\n",
    "# ========================================\n",
    "DATASET_ROOT = Path('E:/FasalVaidya/Leaf Nutrient Data Sets')\n",
    "MODEL_OUTPUT = Path('E:/FasalVaidya/backend/ml/models_pytorch')\n",
    "\n",
    "# Create output directory\n",
    "MODEL_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify dataset exists\n",
    "if DATASET_ROOT.exists():\n",
    "    print(f\"‚úÖ Dataset found at: {DATASET_ROOT}\")\n",
    "    print(f\"üìÇ Contents: {[f.name for f in DATASET_ROOT.iterdir() if f.is_dir()][:5]}...\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset NOT found at: {DATASET_ROOT}\")\n",
    "    print(\"Please update DATASET_ROOT to your local path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e8445",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Crop Configurations\n",
    "\n",
    "Same crop configurations as TensorFlow version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca13f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Available crops: ['rice', 'tomato', 'wheat', 'maize']\n"
     ]
    }
   ],
   "source": [
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Image settings\n",
    "IMG_SIZE = 224\n",
    "MAX_SAMPLES_PER_CLASS = 2000\n",
    "\n",
    "# Crop configurations (same as TensorFlow version)\n",
    "CROP_CONFIGS = {\n",
    "    'rice': {\n",
    "        'name': 'Rice',\n",
    "        'dataset_path': DATASET_ROOT / 'Rice Nutrients',\n",
    "        'class_mapping': {\n",
    "            'Nitrogen(N)': {'N': 1, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'Phosphorus(P)': {'N': 0, 'P': 1, 'K': 0, 'Mg': 0},\n",
    "            'Potassium(K)': {'N': 0, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': False,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "    'tomato': {\n",
    "        'name': 'Tomato',\n",
    "        'dataset_path': DATASET_ROOT / 'Tomato Nutrients',\n",
    "        'use_train_folder': True,\n",
    "        'class_mapping': {\n",
    "            'Tomato - Healthy': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'Tomato - Nitrogen Deficiency': {'N': 1, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'Tomato - Potassium Deficiency': {'N': 0, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "            'Tomato - Nitrogen and Potassium Deficiency': {'N': 1, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "            'Tomato - Jassid and Mite': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'Tomato - Leaf Miner': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'Tomato - Mite': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': True,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "    'wheat': {\n",
    "        'name': 'Wheat',\n",
    "        'dataset_path': DATASET_ROOT / 'Wheat Nitrogen',\n",
    "        'use_train_folder': True,\n",
    "        'class_mapping': {\n",
    "            'control': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'deficiency': {'N': 1, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': True,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "    'maize': {\n",
    "        'name': 'Maize',\n",
    "        'dataset_path': DATASET_ROOT / 'Maize Nutrients',\n",
    "        'use_train_folder': True,\n",
    "        'class_mapping': {\n",
    "            'ALL Present': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'NAB': {'N': 1, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'PAB': {'N': 0, 'P': 1, 'K': 0, 'Mg': 0},\n",
    "            'KAB': {'N': 0, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "            'ALLAB': {'N': 1, 'P': 1, 'K': 1, 'Mg': 0},\n",
    "            'ZNAB': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': True,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"üìã Available crops: {list(CROP_CONFIGS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d6bbe",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ PyTorch Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a037dd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch Dataset and transforms ready\n"
     ]
    }
   ],
   "source": [
    "class NutrientDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for nutrient deficiency detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, torch.FloatTensor(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Return a dummy image\n",
    "            return torch.zeros(3, IMG_SIZE, IMG_SIZE), torch.FloatTensor(label)\n",
    "\n",
    "\n",
    "def get_image_paths_and_labels(crop_id):\n",
    "    \"\"\"Get list of image paths and their labels.\"\"\"\n",
    "    config = CROP_CONFIGS[crop_id]\n",
    "    dataset_path = config['dataset_path']\n",
    "    class_mapping = config['class_mapping']\n",
    "    use_train_folder = config.get('use_train_folder', False)\n",
    "    \n",
    "    if use_train_folder:\n",
    "        dataset_path = dataset_path / 'train'\n",
    "    \n",
    "    if not dataset_path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found: {dataset_path}\")\n",
    "    \n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_counts = {}\n",
    "    \n",
    "    print(f\"\\nüìÇ Scanning {config['name']} from: {dataset_path}\")\n",
    "    \n",
    "    for folder_name, label_dict in class_mapping.items():\n",
    "        folder_path = dataset_path / folder_name\n",
    "        if not folder_path.exists():\n",
    "            print(f\"  ‚ö†Ô∏è Folder not found: {folder_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Get image files\n",
    "        img_files = list(folder_path.glob('*.jpg')) + list(folder_path.glob('*.jpeg')) + \\\n",
    "                    list(folder_path.glob('*.png')) + list(folder_path.glob('*.JPG')) + \\\n",
    "                    list(folder_path.glob('*.JPEG')) + list(folder_path.glob('*.PNG'))\n",
    "        \n",
    "        # Limit samples per class\n",
    "        if len(img_files) > MAX_SAMPLES_PER_CLASS:\n",
    "            img_files = random.sample(img_files, MAX_SAMPLES_PER_CLASS)\n",
    "        \n",
    "        class_counts[folder_name] = len(img_files)\n",
    "        \n",
    "        for img_path in img_files:\n",
    "            image_paths.append(str(img_path))\n",
    "            label = [label_dict.get('N', 0), label_dict.get('P', 0), \n",
    "                     label_dict.get('K', 0), label_dict.get('Mg', 0)]\n",
    "            labels.append(label)\n",
    "    \n",
    "    print(f\"  üìä Class distribution: {class_counts}\")\n",
    "    print(f\"  ‚úÖ Found {len(image_paths)} images\")\n",
    "    \n",
    "    return image_paths, np.array(labels, dtype=np.float32)\n",
    "\n",
    "\n",
    "# Data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"‚úÖ PyTorch Dataset and transforms ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0828acd6",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "197bfc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to C:\\Users\\asus/.cache\\torch\\hub\\checkpoints\\efficientnet_b0_rwightman-7f5810bc.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20.5M/20.5M [00:03<00:00, 6.46MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model created: 4,371,968 parameters\n",
      "   Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class NutrientDeficiencyModel(nn.Module):\n",
    "    \"\"\"PyTorch model for nutrient deficiency detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone='efficientnet_b0', num_outputs=4, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained backbone\n",
    "        if backbone == 'efficientnet_b0':\n",
    "            self.backbone = models.efficientnet_b0(weights='DEFAULT' if pretrained else None)\n",
    "            num_features = self.backbone.classifier[1].in_features\n",
    "            self.backbone.classifier = nn.Identity()  # Remove original classifier\n",
    "        elif backbone == 'resnet50':\n",
    "            self.backbone = models.resnet50(weights='DEFAULT' if pretrained else None)\n",
    "            num_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        elif backbone == 'mobilenet_v3_large':\n",
    "            self.backbone = models.mobilenet_v3_large(weights='DEFAULT' if pretrained else None)\n",
    "            num_features = self.backbone.classifier[0].in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {backbone}\")\n",
    "        \n",
    "        # Custom classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_outputs),\n",
    "            nn.Sigmoid()  # For multi-label classification\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "\n",
    "def create_model(backbone='efficientnet_b0', num_outputs=4, device='cpu'):\n",
    "    \"\"\"Create and initialize model.\"\"\"\n",
    "    model = NutrientDeficiencyModel(backbone, num_outputs, pretrained=True)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Test model creation\n",
    "test_model = create_model(device=device)\n",
    "total_params = sum(p.numel() for p in test_model.parameters())\n",
    "print(f\"‚úÖ Model created: {total_params:,} parameters\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a94db7",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c241121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training function ready\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def train_crop_model(crop_id, epochs=50, batch_size=16, backbone='efficientnet_b0', lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train a model for a specific crop using PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        crop_id: Crop identifier (e.g., 'rice', 'tomato')\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        backbone: Model backbone\n",
    "        lr: Learning rate\n",
    "    \"\"\"\n",
    "    config = CROP_CONFIGS[crop_id]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üå± Training model for: {config['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = MODEL_OUTPUT / crop_id\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get image paths and labels\n",
    "    image_paths, labels = get_image_paths_and_labels(crop_id)\n",
    "    \n",
    "    if len(image_paths) == 0:\n",
    "        print(f\"‚ùå No images found for {crop_id}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Split into train/val\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        image_paths, labels, test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    print(f\"\\nüìä Train: {len(train_paths)}, Validation: {len(val_paths)}\")\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = NutrientDataset(train_paths, train_labels, train_transform)\n",
    "    val_dataset = NutrientDataset(val_paths, val_labels, val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(backbone=backbone, device=device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.BCELoss()  # Binary cross-entropy for multi-label\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4, verbose=True)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    print(f\"\\nüü¢ Training for {epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy (threshold at 0.5)\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                correct += (predicted == targets).all(dim=1).sum().item()\n",
    "                total += targets.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"  Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), output_dir / f'{crop_id}_best.pth')\n",
    "            print(f\"  üíæ Saved best model (val_loss: {val_loss:.4f})\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(output_dir / f'{crop_id}_best.pth'))\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == targets).all(dim=1).sum().item()\n",
    "            total += targets.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    \n",
    "    print(f\"\\nüìà Final Evaluation:\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'crop_id': crop_id,\n",
    "        'crop_name': config['name'],\n",
    "        'backbone': backbone,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "        'val_accuracy': float(val_acc),\n",
    "        'val_loss': float(val_loss),\n",
    "        'trained_at': datetime.now().isoformat(),\n",
    "        'train_samples': len(train_paths),\n",
    "        'val_samples': len(val_paths),\n",
    "        'device': str(device),\n",
    "        'framework': 'pytorch',\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / 'metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Model saved to: {output_dir}\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del model, train_loader, val_loader\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return history, val_acc\n",
    "\n",
    "print(\"‚úÖ Training function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad0824e",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Train a Single Crop (Demo)\n",
    "\n",
    "Run this cell to train a model for a single crop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e09364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üéØ SELECT CROP TO TRAIN\n",
    "# ========================================\n",
    "CROP_TO_TRAIN = 'rice'  # Change this: rice, tomato, wheat, maize\n",
    "EPOCHS = 30             # Increase for better results (50-100 recommended)\n",
    "BATCH_SIZE = 16         # Adjust based on your GPU memory (16-32)\n",
    "BACKBONE = 'efficientnet_b0'  # or 'resnet50', 'mobilenet_v3_large'\n",
    "\n",
    "# Verify crop exists\n",
    "if CROP_TO_TRAIN in CROP_CONFIGS:\n",
    "    print(f\"üöÄ Starting training for: {CROP_TO_TRAIN}\")\n",
    "    print(f\"üéØ Using device: {device}\")\n",
    "    \n",
    "    history, val_acc = train_crop_model(\n",
    "        CROP_TO_TRAIN, \n",
    "        epochs=EPOCHS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        backbone=BACKBONE\n",
    "    )\n",
    "else:\n",
    "    print(f\"‚ùå Unknown crop: {CROP_TO_TRAIN}\")\n",
    "    print(f\"Available: {list(CROP_CONFIGS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ebc824",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Train All Crops (Full Run)\n",
    "\n",
    "‚ö†Ô∏è This will take a while! Only run if you want to train models for all crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üöÄ TRAIN ALL CROPS\n",
    "# ========================================\n",
    "\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "results_summary = {}\n",
    "\n",
    "for crop_id in CROP_CONFIGS.keys():\n",
    "    try:\n",
    "        print(f\"\\n\\n{'#'*70}\")\n",
    "        print(f\"# Training crop {list(CROP_CONFIGS.keys()).index(crop_id)+1}/{len(CROP_CONFIGS)}: {crop_id}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        history, val_acc = train_crop_model(crop_id, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        if val_acc is not None:\n",
    "            results_summary[crop_id] = {\n",
    "                'accuracy': float(val_acc),\n",
    "                'status': 'success'\n",
    "            }\n",
    "        else:\n",
    "            results_summary[crop_id] = {'status': 'skipped', 'error': 'No images found'}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to train {crop_id}: {e}\")\n",
    "        results_summary[crop_id] = {'status': 'failed', 'error': str(e)}\n",
    "\n",
    "# Save summary\n",
    "with open(MODEL_OUTPUT / 'training_summary.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for crop, res in results_summary.items():\n",
    "    if res['status'] == 'success':\n",
    "        print(f\"  ‚úÖ {crop}: Accuracy={res['accuracy']:.4f}\")\n",
    "    elif res['status'] == 'skipped':\n",
    "        print(f\"  ‚è≠Ô∏è {crop}: SKIPPED - {res.get('error', 'Unknown')}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {crop}: FAILED - {res.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe3d0c",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ View Trained Models\n",
    "\n",
    "After training, your models are saved in:\n",
    "`E:/FasalVaidya/backend/ml/models_pytorch/<crop_id>/`\n",
    "\n",
    "Each folder contains:\n",
    "- `<crop>_best.pth` - Best model weights\n",
    "- `metadata.json` - Training info and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02884a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List trained models\n",
    "print(\"üì¶ Trained PyTorch models:\")\n",
    "if MODEL_OUTPUT.exists():\n",
    "    for crop_dir in MODEL_OUTPUT.iterdir():\n",
    "        if crop_dir.is_dir():\n",
    "            files = list(crop_dir.glob('*.pth'))\n",
    "            if files:\n",
    "                print(f\"  {crop_dir.name}/\")\n",
    "                for f in files:\n",
    "                    size_mb = f.stat().st_size / (1024*1024)\n",
    "                    print(f\"    - {f.name} ({size_mb:.1f} MB)\")\n",
    "                # Show metadata if exists\n",
    "                metadata_file = crop_dir / 'metadata.json'\n",
    "                if metadata_file.exists():\n",
    "                    with open(metadata_file) as f:\n",
    "                        meta = json.load(f)\n",
    "                    print(f\"      Accuracy: {meta.get('val_accuracy', 'N/A'):.4f}\")\n",
    "                    print(f\"      Device: {meta.get('device', 'N/A')}\")\n",
    "else:\n",
    "    print(\"  No models found yet. Run training first!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tf (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
