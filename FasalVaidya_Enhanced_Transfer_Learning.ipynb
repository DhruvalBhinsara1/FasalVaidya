{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e188b6",
   "metadata": {},
   "source": [
    "## üì¶ Setup & Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b15cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow>=2.15.0 kaggle opendatasets scikit-learn matplotlib seaborn tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import threading\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"CPU cores available: {multiprocessing.cpu_count()}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# =============================================================\n",
    "# ‚è±Ô∏è SESSION TIME TRACKER (Important for 3-hour limit!)\n",
    "# =============================================================\n",
    "SESSION_START_TIME = datetime.now()\n",
    "TRAINING_START_TIME = None\n",
    "\n",
    "def get_session_time():\n",
    "    \"\"\"Get elapsed session time\"\"\"\n",
    "    elapsed = datetime.now() - SESSION_START_TIME\n",
    "    hours = elapsed.seconds // 3600\n",
    "    minutes = (elapsed.seconds % 3600) // 60\n",
    "    return f\"{hours}h {minutes}m\"\n",
    "\n",
    "def get_eta(current_epoch, total_epochs, epoch_time):\n",
    "    \"\"\"Calculate ETA for training completion\"\"\"\n",
    "    remaining_epochs = total_epochs - current_epoch\n",
    "    eta_seconds = remaining_epochs * epoch_time\n",
    "    eta = timedelta(seconds=int(eta_seconds))\n",
    "    return str(eta)\n",
    "\n",
    "def check_time_limit(warn_minutes=150):\n",
    "    \"\"\"Warn if approaching 3-hour limit (180 min)\"\"\"\n",
    "    elapsed = (datetime.now() - SESSION_START_TIME).seconds // 60\n",
    "    remaining = 180 - elapsed\n",
    "    if elapsed >= warn_minutes:\n",
    "        print(f\"‚ö†Ô∏è WARNING: {remaining} minutes remaining before typical Colab disconnect!\")\n",
    "        print(f\"   Consider saving checkpoints and downloading results now.\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Session started at: {SESSION_START_TIME.strftime('%H:%M:%S')}\")\n",
    "print(f\"   Target: Complete training within 1-1.5 hours\")\n",
    "print(f\"   Checkpoints auto-save to Drive (training resumes from checkpoint)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ddc489",
   "metadata": {},
   "source": [
    "## üîë Configuration\n",
    "\n",
    "### Set your crop type and paths here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== UNIFIED MODEL - ALL CROPS ==========\n",
    "# Root path to your \"Leaf Nutrient Data Sets\" folder on Google Drive\n",
    "NUTRIENT_DATASETS_ROOT = '/content/drive/MyDrive/Leaf Nutrient Data Sets'\n",
    "\n",
    "# ALL 12 CROPS - Combined into ONE model automatically!\n",
    "CROP_DATASETS = {\n",
    "    'rice': 'Rice Nutrients',\n",
    "    'wheat': 'Wheat Nitrogen',\n",
    "    'tomato': 'Tomato Nutrients',\n",
    "    'maize': 'Maize Nutrients',\n",
    "    'banana': 'Banana leaves Nutrient',\n",
    "    'coffee': 'Coffee Nutrients',\n",
    "    'cucumber': 'Cucumber Nutrients',\n",
    "    'eggplant': 'EggPlant Nutrients',\n",
    "    'ashgourd': 'Ashgourd Nutrients',\n",
    "    'bittergourd': 'Bittergourd Nutrients',\n",
    "    'ridgegourd': 'Ridgegourd',\n",
    "    'snakegourd': 'Snakegourd Nutrients'\n",
    "}\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# üöÄ FAST MVP PROTOTYPING MODE (Train in 1-1.5 hours)\n",
    "# =============================================================\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16  # User requested batch size 16\n",
    "\n",
    "# Training epochs (Optimized for 1-1.5 hour completion)\n",
    "PLANTVILLAGE_EPOCHS = 5    # Stage 2: Quick transfer learning\n",
    "UNIFIED_EPOCHS = 10        # Stage 3: Nutrient detection\n",
    "\n",
    "# Learning rates (Optimized for fast convergence)\n",
    "LEARNING_RATE_STAGE2 = 1e-3  # Aggressive learning rate\n",
    "LEARNING_RATE_STAGE3 = 5e-4  # Fast fine-tuning\n",
    "\n",
    "# Regularization\n",
    "DROPOUT_RATE = 0.25  # Balanced dropout\n",
    "\n",
    "# Multiprocessing settings\n",
    "NUM_WORKERS = multiprocessing.cpu_count()  # Use all CPU cores\n",
    "PREFETCH_BUFFER = 4  # Prefetch batches\n",
    "\n",
    "# Enable mixed precision training (2x speedup on T4)\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"üöÄ Mixed precision enabled (FP16)\")\n",
    "\n",
    "# Output paths (persistent on Drive for checkpoint resume)\n",
    "OUTPUT_DIR = '/content/fasalvaidya_unified_model'\n",
    "DRIVE_CHECKPOINT_DIR = '/content/drive/MyDrive/FasalVaidya_Checkpoints'\n",
    "GRADCAM_DIR = '/content/gradcam_outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(DRIVE_CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(GRADCAM_DIR, exist_ok=True)\n",
    "\n",
    "print(\"‚ö° FAST MVP PROTOTYPING MODE (1-1.5 hour target)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training ONE model for ALL {len(CROP_DATASETS)} crops\")\n",
    "print(f\"\\nüöÄ RAPID PROTOTYPING SETTINGS:\")\n",
    "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   - Mixed precision: FP16 (2x faster)\")\n",
    "print(f\"   - XLA/JIT compilation: Enabled\")\n",
    "print(f\"   - Multiprocessing workers: {NUM_WORKERS}\")\n",
    "print(f\"   - Prefetch buffer: {PREFETCH_BUFFER}\")\n",
    "print(f\"   - Stage 2: {PLANTVILLAGE_EPOCHS} epochs\")\n",
    "print(f\"   - Stage 3: {UNIFIED_EPOCHS} epochs\")\n",
    "print(f\"   - Grad-CAM: Enabled (visualize during training)\")\n",
    "print(f\"\\n‚ö° Expected total time: ~1-1.5 hours for full training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e354d5dc",
   "metadata": {},
   "source": [
    "## üî• Grad-CAM Visualization Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101aa52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# üî• GRAD-CAM VISUALIZATION MODULE\n",
    "# =============================================================\n",
    "# Generates heatmaps showing what the model focuses on\n",
    "\n",
    "import cv2\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"Grad-CAM implementation for model interpretability\"\"\"\n",
    "    \n",
    "    def __init__(self, model, layer_name=None):\n",
    "        \"\"\"\n",
    "        Initialize Grad-CAM\n",
    "        Args:\n",
    "            model: Keras model\n",
    "            layer_name: Name of conv layer to visualize (auto-detect if None)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.layer_name = layer_name or self._find_target_layer()\n",
    "        \n",
    "    def _find_target_layer(self):\n",
    "        \"\"\"Auto-detect the last conv layer in the model\"\"\"\n",
    "        for layer in reversed(self.model.layers):\n",
    "            if hasattr(layer, 'layers'):  # Sequential or Functional inside\n",
    "                for sublayer in reversed(layer.layers):\n",
    "                    if len(sublayer.output_shape) == 4:  # Conv layer\n",
    "                        return sublayer.name\n",
    "            if len(layer.output_shape) == 4:\n",
    "                return layer.name\n",
    "        # Fallback for MobileNetV2\n",
    "        return 'Conv_1'  # Last conv layer in MobileNetV2\n",
    "    \n",
    "    def get_gradcam_heatmap(self, img_array, pred_index=None):\n",
    "        \"\"\"\n",
    "        Generate Grad-CAM heatmap\n",
    "        Args:\n",
    "            img_array: Preprocessed image (1, H, W, 3)\n",
    "            pred_index: Class index to visualize (None = predicted class)\n",
    "        Returns:\n",
    "            heatmap: (H, W) numpy array\n",
    "        \"\"\"\n",
    "        # Find the target layer\n",
    "        try:\n",
    "            # For Sequential model with base model as first layer\n",
    "            if hasattr(self.model.layers[0], 'get_layer'):\n",
    "                target_layer = self.model.layers[0].get_layer(self.layer_name)\n",
    "            else:\n",
    "                target_layer = self.model.get_layer(self.layer_name)\n",
    "        except:\n",
    "            # Fallback: use output of first layer (base model)\n",
    "            target_layer = self.model.layers[0]\n",
    "            if hasattr(target_layer, 'layers'):\n",
    "                for layer in reversed(target_layer.layers):\n",
    "                    if len(layer.output_shape) == 4:\n",
    "                        target_layer = layer\n",
    "                        break\n",
    "        \n",
    "        # Create gradient model\n",
    "        grad_model = tf.keras.models.Model(\n",
    "            inputs=self.model.input,\n",
    "            outputs=[target_layer.output, self.model.output]\n",
    "        )\n",
    "        \n",
    "        # Compute gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            conv_outputs, predictions = grad_model(img_array)\n",
    "            if pred_index is None:\n",
    "                pred_index = tf.argmax(predictions[0])\n",
    "            class_channel = predictions[:, pred_index]\n",
    "        \n",
    "        grads = tape.gradient(class_channel, conv_outputs)\n",
    "        \n",
    "        # Global average pooling of gradients\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "        \n",
    "        # Weight feature maps by importance\n",
    "        conv_outputs = conv_outputs[0]\n",
    "        heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "        heatmap = tf.squeeze(heatmap)\n",
    "        \n",
    "        # Normalize heatmap\n",
    "        heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-8)\n",
    "        return heatmap.numpy()\n",
    "    \n",
    "    def overlay_heatmap(self, img, heatmap, alpha=0.4):\n",
    "        \"\"\"\n",
    "        Overlay heatmap on original image\n",
    "        Args:\n",
    "            img: Original image (H, W, 3) in [0, 255]\n",
    "            heatmap: Grad-CAM heatmap (H, W)\n",
    "            alpha: Overlay transparency\n",
    "        Returns:\n",
    "            superimposed: Image with heatmap overlay\n",
    "        \"\"\"\n",
    "        # Resize heatmap to image size\n",
    "        heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "        \n",
    "        # Convert to RGB heatmap\n",
    "        heatmap = np.uint8(255 * heatmap)\n",
    "        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Superimpose\n",
    "        superimposed = heatmap * alpha + img * (1 - alpha)\n",
    "        superimposed = np.clip(superimposed, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        return superimposed\n",
    "\n",
    "\n",
    "def visualize_gradcam_batch(model, images, labels, class_names, num_samples=4, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize Grad-CAM for a batch of images\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        images: Batch of images (B, H, W, 3) - preprocessed\n",
    "        labels: One-hot labels (B, num_classes)\n",
    "        class_names: List of class names\n",
    "        num_samples: Number of samples to visualize\n",
    "        save_path: Path to save the figure\n",
    "    \"\"\"\n",
    "    gradcam = GradCAM(model)\n",
    "    \n",
    "    num_samples = min(num_samples, len(images))\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        img = images[i]\n",
    "        label = labels[i]\n",
    "        \n",
    "        # Get prediction\n",
    "        pred = model.predict(np.expand_dims(img, 0), verbose=0)\n",
    "        pred_idx = np.argmax(pred[0])\n",
    "        true_idx = np.argmax(label)\n",
    "        confidence = pred[0][pred_idx]\n",
    "        \n",
    "        # Generate heatmap\n",
    "        try:\n",
    "            heatmap = gradcam.get_gradcam_heatmap(np.expand_dims(img, 0), pred_idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Grad-CAM error for sample {i}: {e}\")\n",
    "            heatmap = np.zeros((7, 7))\n",
    "        \n",
    "        # Convert image back to displayable format\n",
    "        display_img = ((img + 1) * 127.5).astype(np.uint8)  # Undo MobileNet preprocessing\n",
    "        \n",
    "        # Original image\n",
    "        axes[i, 0].imshow(display_img)\n",
    "        axes[i, 0].set_title(f\"True: {class_names[true_idx][:20]}\", fontsize=10)\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Heatmap\n",
    "        axes[i, 1].imshow(heatmap, cmap='jet')\n",
    "        axes[i, 1].set_title(\"Grad-CAM Heatmap\", fontsize=10)\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        overlay = gradcam.overlay_heatmap(display_img, heatmap)\n",
    "        axes[i, 2].imshow(overlay)\n",
    "        correct = \"‚úÖ\" if pred_idx == true_idx else \"‚ùå\"\n",
    "        axes[i, 2].set_title(f\"Pred: {class_names[pred_idx][:15]} ({confidence:.1%}) {correct}\", fontsize=10)\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"üíæ Grad-CAM saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Grad-CAM module loaded!\")\n",
    "print(\"   - Auto-detects last conv layer\")\n",
    "print(\"   - Generates heatmaps showing model attention\")\n",
    "print(\"   - Overlays on original images for interpretability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ab98b",
   "metadata": {},
   "source": [
    "## üíæ Mount Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2242a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Robust check for already-mounted Drive\n",
    "drive_path = '/content/drive'\n",
    "is_mounted = False\n",
    "\n",
    "if os.path.exists(drive_path):\n",
    "    # Check if directory has content (indicates already mounted)\n",
    "    try:\n",
    "        if os.listdir(drive_path):\n",
    "            is_mounted = True\n",
    "            print(\"‚úÖ Google Drive already mounted!\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if not is_mounted:\n",
    "    print(\"üìÅ Mounting Google Drive...\")\n",
    "    # Clean up if directory exists but is empty\n",
    "    if os.path.exists(drive_path) and not os.listdir(drive_path):\n",
    "        os.rmdir(drive_path)\n",
    "    drive.mount(drive_path)\n",
    "    print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "\n",
    "# =============================================================\n",
    "# üîç SMART PATH DETECTION: Search All Possible Locations\n",
    "# =============================================================\n",
    "print(\"\\nüîç Searching for 'Leaf Nutrient Data Sets' folder...\")\n",
    "\n",
    "# List of possible locations to check\n",
    "search_paths = [\n",
    "    '/content/drive/MyDrive/Leaf Nutrient Data Sets',\n",
    "    '/content/drive/Shareddrives/Leaf Nutrient Data Sets',\n",
    "    '/content/drive/Shared drives/Leaf Nutrient Data Sets',\n",
    "]\n",
    "\n",
    "# Also search for shortcuts and nested locations\n",
    "mydrive_base = '/content/drive/MyDrive'\n",
    "if os.path.exists(mydrive_base):\n",
    "    # Search in .shortcut-targets-by-id (where \"Shared with me\" shortcuts appear)\n",
    "    shortcut_dir = os.path.join(mydrive_base, '.shortcut-targets-by-id')\n",
    "    if os.path.exists(shortcut_dir):\n",
    "        try:\n",
    "            for folder_id in os.listdir(shortcut_dir):\n",
    "                target_path = os.path.join(shortcut_dir, folder_id, 'Leaf Nutrient Data Sets')\n",
    "                if os.path.exists(target_path):\n",
    "                    search_paths.append(target_path)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Try each location\n",
    "found_location = None\n",
    "\n",
    "for search_path in search_paths:\n",
    "    if os.path.exists(search_path):\n",
    "        # Verify it has crop folders\n",
    "        try:\n",
    "            contents = os.listdir(search_path)\n",
    "            crop_folders = [f for f in contents if os.path.isdir(os.path.join(search_path, f))]\n",
    "            if len(crop_folders) >= 5:  # Should have at least 5 crop folders\n",
    "                print(f\"‚úÖ Found at: {search_path}\")\n",
    "                print(f\"   Contains {len(crop_folders)} folders\")\n",
    "                found_location = search_path\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if found_location:\n",
    "    NUTRIENT_DATASETS_ROOT = found_location\n",
    "    print(f\"\\n‚úÖ Using dataset location: {NUTRIENT_DATASETS_ROOT}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå 'Leaf Nutrient Data Sets' folder NOT FOUND!\")\n",
    "    print(f\"\\nüìÇ What's in your Drive:\")\n",
    "    try:\n",
    "        mydrive_items = os.listdir(mydrive_base)[:10]  # Show first 10 items\n",
    "        for item in mydrive_items:\n",
    "            item_path = os.path.join(mydrive_base, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                print(f\"   üìÅ {item}\")\n",
    "            else:\n",
    "                print(f\"   üìÑ {item}\")\n",
    "    except:\n",
    "        print(\"   (Could not list Drive contents)\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è FOLDER IS IN 'SHARED WITH ME' - NOT ACCESSIBLE!\")\n",
    "    print(f\"\\n‚úÖ SOLUTION: Add shortcut to My Drive\")\n",
    "    print(f\"   1. Open Google Drive in browser: https://drive.google.com\")\n",
    "    print(f\"   2. Click 'Shared with me' in left sidebar\")\n",
    "    print(f\"   3. Right-click 'Leaf Nutrient Data Sets' folder\")\n",
    "    print(f\"   4. Select 'Add shortcut to Drive' or 'Organize' > 'Add shortcut'\")\n",
    "    print(f\"   5. Choose 'My Drive' root (don't put it in a subfolder)\")\n",
    "    print(f\"   6. Click 'Add' or 'Add shortcut'\")\n",
    "    print(f\"   7. Come back here and re-run this cell\")\n",
    "    print(f\"\\nüí° After adding shortcut, the folder will appear at:\")\n",
    "    print(f\"   /content/drive/MyDrive/Leaf Nutrient Data Sets\")\n",
    "\n",
    "# Verify ALL crop datasets exist (only if folder found)\n",
    "if found_location:\n",
    "    print(\"\\nüîç Verifying crop datasets...\")\n",
    "    missing_crops = []\n",
    "    for crop, folder_name in CROP_DATASETS.items():\n",
    "        crop_path = os.path.join(NUTRIENT_DATASETS_ROOT, folder_name)\n",
    "        if os.path.exists(crop_path):\n",
    "            num_classes = len([d for d in os.listdir(crop_path) if os.path.isdir(os.path.join(crop_path, d))])\n",
    "            print(f\"‚úÖ {crop.upper()}: {num_classes} classes\")\n",
    "        else:\n",
    "            print(f\"‚ùå {crop.upper()}: NOT FOUND\")\n",
    "            missing_crops.append(crop)\n",
    "\n",
    "    if missing_crops:\n",
    "        print(f\"\\n‚ö†Ô∏è WARNING: {len(missing_crops)} crop(s) not found: {', '.join(missing_crops)}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ All {len(CROP_DATASETS)} crop datasets verified!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0539d26",
   "metadata": {},
   "source": [
    "## üöÄ Speed Optimization: Copy Data to Local Disk\n",
    "\n",
    "**This is the BIGGEST speed boost!** Copying from Drive to local SSD speeds up training 10-50x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# üöÄ MASSIVE SPEEDUP: Copy Data from Drive to Local SSD\n",
    "# =============================================================\n",
    "# Reading from Google Drive is SLOW (network I/O)\n",
    "# Copying to /content/ uses Colab's fast local SSD = 10-50x faster!\n",
    "\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "def copy_folder_to_local(src_path, dest_name, force_copy=False):\n",
    "    \"\"\"Copy folder from Drive to local SSD for fast I/O\"\"\"\n",
    "    local_path = f\"/content/{dest_name}\"\n",
    "    \n",
    "    # Skip if already exists (for session resume)\n",
    "    if os.path.exists(local_path) and not force_copy:\n",
    "        num_items = len(os.listdir(local_path))\n",
    "        if num_items > 0:\n",
    "            print(f\"‚úÖ {dest_name} already on local disk ({num_items} items)\")\n",
    "            return local_path\n",
    "    \n",
    "    if not os.path.exists(src_path):\n",
    "        print(f\"‚ö†Ô∏è Source not found: {src_path}\")\n",
    "        return src_path  # Return original path as fallback\n",
    "    \n",
    "    print(f\"üöÄ Copying {dest_name} to local SSD...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    if os.path.exists(local_path):\n",
    "        shutil.rmtree(local_path)\n",
    "    \n",
    "    shutil.copytree(src_path, local_path)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    size_mb = sum(os.path.getsize(os.path.join(root, f)) \n",
    "                  for root, _, files in os.walk(local_path) \n",
    "                  for f in files) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"‚úÖ Copied {dest_name}: {size_mb:.0f}MB in {elapsed:.1f}s\")\n",
    "    return local_path\n",
    "\n",
    "# Copy Nutrient datasets to local SSD\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ COPYING DATASETS TO LOCAL SSD (One-time per session)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚è≥ This takes 2-5 minutes but saves HOURS of training time!\\n\")\n",
    "\n",
    "LOCAL_NUTRIENT_ROOT = '/content/local_nutrient_datasets'\n",
    "os.makedirs(LOCAL_NUTRIENT_ROOT, exist_ok=True)\n",
    "\n",
    "copy_success = 0\n",
    "copy_failed = []\n",
    "\n",
    "for crop, folder_name in CROP_DATASETS.items():\n",
    "    src = os.path.join(NUTRIENT_DATASETS_ROOT, folder_name)\n",
    "    dst = os.path.join(LOCAL_NUTRIENT_ROOT, folder_name)\n",
    "    \n",
    "    if os.path.exists(src):\n",
    "        if not os.path.exists(dst):\n",
    "            try:\n",
    "                shutil.copytree(src, dst)\n",
    "                print(f\"  ‚úÖ {crop}: Copied\")\n",
    "                copy_success += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå {crop}: Failed ({e})\")\n",
    "                copy_failed.append(crop)\n",
    "        else:\n",
    "            print(f\"  ‚è© {crop}: Already local\")\n",
    "            copy_success += 1\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è {crop}: Not found on Drive\")\n",
    "        copy_failed.append(crop)\n",
    "\n",
    "# Update root path to local copy\n",
    "NUTRIENT_DATASETS_ROOT = LOCAL_NUTRIENT_ROOT\n",
    "\n",
    "print(f\"\\n‚úÖ {copy_success}/{len(CROP_DATASETS)} crops copied to local SSD\")\n",
    "if copy_failed:\n",
    "    print(f\"‚ö†Ô∏è Failed: {', '.join(copy_failed)}\")\n",
    "print(\"üöÄ Training will now be 10-50x FASTER!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061a425",
   "metadata": {},
   "source": [
    "## üå± Stage 1: Download PlantVillage Dataset from Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c0d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Kaggle credentials\n",
    "# IMPORTANT: You need to manually download kaggle.json FIRST!\n",
    "# \n",
    "# üìù HOW TO GET kaggle.json:\n",
    "# 1. Go to https://www.kaggle.com/settings\n",
    "# 2. Scroll down to \"API\" section\n",
    "# 3. Click \"Create New Token\" button\n",
    "# 4. This will DOWNLOAD a file called \"kaggle.json\" to your computer\n",
    "# 5. Find the downloaded file (usually in your Downloads folder)\n",
    "# 6. Then come back here and upload it when prompted below\n",
    "#\n",
    "# ‚ö†Ô∏è NOTE: If you only see the API key on screen but no download happened,\n",
    "#    click \"Create New Token\" again - it should download the file\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üì§ UPLOAD YOUR kaggle.json FILE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüìù If you haven't downloaded it yet:\")\n",
    "print(\"   1. Go to: https://www.kaggle.com/settings\")\n",
    "print(\"   2. Scroll to 'API' section\")\n",
    "print(\"   3. Click 'Create New Token' (downloads kaggle.json)\")\n",
    "print(\"   4. Find the file in your Downloads folder\")\n",
    "print(\"   5. Click 'Choose Files' below and select it\")\n",
    "print(\"\\n‚è≥ Waiting for your kaggle.json file...\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Verify the file was uploaded\n",
    "if 'kaggle.json' not in uploaded:\n",
    "    print(\"\\n‚ùå ERROR: kaggle.json was not uploaded!\")\n",
    "    print(\"   Please make sure you selected the correct file.\")\n",
    "    raise FileNotFoundError(\"kaggle.json not found in uploaded files\")\n",
    "\n",
    "# Move kaggle.json to the correct location\n",
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"\\n‚úÖ Kaggle credentials configured successfully!\")\n",
    "print(\"üìÅ File saved to: ~/.kaggle/kaggle.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PlantVillage dataset from Kaggle (SKIP IF ALREADY EXISTS)\n",
    "import opendatasets as od\n",
    "\n",
    "PLANTVILLAGE_URL = 'https://www.kaggle.com/datasets/emmarex/plantdisease'\n",
    "PLANTVILLAGE_PATH = '/content/plantvillage'\n",
    "\n",
    "# Known possible paths after download\n",
    "POSSIBLE_PATHS = [\n",
    "    os.path.join(PLANTVILLAGE_PATH, 'plantdisease', 'PlantVillage'),\n",
    "    os.path.join(PLANTVILLAGE_PATH, 'PlantVillage'),\n",
    "    os.path.join(PLANTVILLAGE_PATH, 'plantdisease', 'plantvillage', 'PlantVillage'),\n",
    "]\n",
    "\n",
    "def find_plantvillage_dataset():\n",
    "    \"\"\"Find PlantVillage dataset if it exists\"\"\"\n",
    "    for path in POSSIBLE_PATHS:\n",
    "        if os.path.exists(path) and os.path.isdir(path):\n",
    "            subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "            if len(subdirs) >= 15:\n",
    "                sample_dir = os.path.join(path, subdirs[0])\n",
    "                sample_files = [f for f in os.listdir(sample_dir)\n",
    "                              if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                if len(sample_files) > 0:\n",
    "                    return path\n",
    "    return None\n",
    "\n",
    "# Check if dataset already exists\n",
    "existing_path = find_plantvillage_dataset()\n",
    "\n",
    "if existing_path:\n",
    "    print(\"‚úÖ PlantVillage dataset ALREADY EXISTS! Skipping download...\")\n",
    "    print(f\"üìÅ Using cached dataset at: {existing_path}\")\n",
    "    PLANTVILLAGE_PATH = existing_path\n",
    "else:\n",
    "    print(\"üì• Downloading PlantVillage dataset (54,305 images)...\")\n",
    "    print(\"‚è≥ This will take 3-5 minutes (first time only)...\")\n",
    "    \n",
    "    od.download(PLANTVILLAGE_URL, data_dir=PLANTVILLAGE_PATH)\n",
    "    \n",
    "    print(\"\\nüîç Locating dataset structure...\")\n",
    "    \n",
    "    # Find the dataset path\n",
    "    dataset_root = find_plantvillage_dataset()\n",
    "    \n",
    "    if not dataset_root:\n",
    "        # Search recursively as fallback\n",
    "        for root, dirs, files in os.walk(PLANTVILLAGE_PATH):\n",
    "            if len(dirs) >= 15:\n",
    "                has_images = False\n",
    "                for d in dirs[:3]:\n",
    "                    dir_path = os.path.join(root, d)\n",
    "                    if os.path.isdir(dir_path):\n",
    "                        dir_files = os.listdir(dir_path)\n",
    "                        if any(f.lower().endswith(('.jpg', '.jpeg', '.png')) for f in dir_files):\n",
    "                            has_images = True\n",
    "                            break\n",
    "                if has_images:\n",
    "                    dataset_root = root\n",
    "                    break\n",
    "    \n",
    "    if dataset_root:\n",
    "        PLANTVILLAGE_PATH = dataset_root\n",
    "    else:\n",
    "        raise FileNotFoundError(\"‚ùå PlantVillage dataset not found after download\")\n",
    "\n",
    "# Verify dataset\n",
    "class_dirs = [d for d in os.listdir(PLANTVILLAGE_PATH) \n",
    "              if os.path.isdir(os.path.join(PLANTVILLAGE_PATH, d))]\n",
    "num_classes = len(class_dirs)\n",
    "\n",
    "print(f\"\\n‚úÖ PlantVillage dataset ready!\")\n",
    "print(f\"üìÅ Path: {PLANTVILLAGE_PATH}\")\n",
    "print(f\"üåø Classes: {num_classes}\")\n",
    "\n",
    "# Quick image count\n",
    "total_images = sum(len([f for f in os.listdir(os.path.join(PLANTVILLAGE_PATH, cls))\n",
    "                        if f.lower().endswith(('.jpg', '.jpeg', '.png'))]) \n",
    "                   for cls in class_dirs[:5])\n",
    "print(f\"üìä Sample: First 5 classes have {total_images:,} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac424fc",
   "metadata": {},
   "source": [
    "## üìä Data Exploration & Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze PlantVillage dataset\n",
    "plantvillage_classes = sorted(os.listdir(PLANTVILLAGE_PATH))\n",
    "print(f\"üå± PlantVillage Dataset:\")\n",
    "print(f\"Total classes: {len(plantvillage_classes)}\")\n",
    "print(f\"\\nSample classes:\")\n",
    "for cls in plantvillage_classes[:5]:\n",
    "    class_path = os.path.join(PLANTVILLAGE_PATH, cls)\n",
    "    if os.path.isdir(class_path):\n",
    "        num_images = len(os.listdir(class_path))\n",
    "        print(f\"  - {cls}: {num_images} images\")\n",
    "\n",
    "# Build unified dataset info\n",
    "print(f\"\\nüåæ UNIFIED Nutrient Dataset (ALL {len(CROP_DATASETS)} crops):\")\n",
    "total_classes = 0\n",
    "total_images = 0\n",
    "\n",
    "for crop, folder_name in CROP_DATASETS.items():\n",
    "    crop_path = os.path.join(NUTRIENT_DATASETS_ROOT, folder_name)\n",
    "    if os.path.exists(crop_path):\n",
    "        crop_classes = [d for d in os.listdir(crop_path) if os.path.isdir(os.path.join(crop_path, d))]\n",
    "        crop_images = sum([len([f for f in os.listdir(os.path.join(crop_path, cls)) \n",
    "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))]) \n",
    "                          for cls in crop_classes])\n",
    "        total_classes += len(crop_classes)\n",
    "        total_images += crop_images\n",
    "        print(f\"  {crop.upper()}: {len(crop_classes)} classes, {crop_images} images\")\n",
    "\n",
    "print(f\"\\nüìä UNIFIED TOTALS:\")\n",
    "print(f\"  Total classes: {total_classes}\")\n",
    "print(f\"  Total images: {total_images}\")\n",
    "print(f\"  Class format: {{crop}}_{{deficiency}} (e.g., rice_N, wheat_healthy)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db7afbe",
   "metadata": {},
   "source": [
    "## üî® Create Data Pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# üöÄ OPTIMIZED DATA PIPELINE (Multiprocessing + Threading)\n",
    "# =============================================================\n",
    "# Parallel data loading with progress tracking\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def create_dataset(data_dir, img_size, batch_size, validation_split=0.2, subset=None):\n",
    "    \"\"\"Create dataset with progress tracking\"\"\"\n",
    "    print(f\"üì¶ Loading {subset} dataset from {data_dir}...\")\n",
    "    return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=validation_split,\n",
    "        subset=subset,\n",
    "        seed=42,\n",
    "        image_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        label_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "@tf.function\n",
    "def augment_training(image, label):\n",
    "    \"\"\"Training augmentation - optimized for speed\"\"\"\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, 0.15)\n",
    "    image = tf.image.random_contrast(image, 0.9, 1.1)\n",
    "    return image, label\n",
    "\n",
    "@tf.function  \n",
    "def normalize_mobilenet(image, label):\n",
    "    \"\"\"Normalize for MobileNetV2 [-1, 1]\"\"\"\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "def build_pipeline(dataset, is_training=True):\n",
    "    \"\"\"\n",
    "    Optimized pipeline with multiprocessing\n",
    "    \"\"\"\n",
    "    # Use all CPU cores for parallel processing\n",
    "    options = tf.data.Options()\n",
    "    options.threading.private_threadpool_size = NUM_WORKERS\n",
    "    options.threading.max_intra_op_parallelism = NUM_WORKERS\n",
    "    dataset = dataset.with_options(options)\n",
    "    \n",
    "    if is_training:\n",
    "        dataset = dataset.map(augment_training, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.map(normalize_mobilenet, num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.prefetch(buffer_size=PREFETCH_BUFFER)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# üìä TQDM PROGRESS CALLBACK (Real-time ETA)\n",
    "# =============================================================\n",
    "class TQDMProgressCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Custom callback with tqdm progress bars and ETA tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, total_epochs, stage_name=\"Training\"):\n",
    "        super().__init__()\n",
    "        self.total_epochs = total_epochs\n",
    "        self.stage_name = stage_name\n",
    "        self.epoch_pbar = None\n",
    "        self.batch_pbar = None\n",
    "        self.epoch_times = []\n",
    "        self.stage_start_time = None\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.stage_start_time = time.time()\n",
    "        print(f\"\\nüöÄ {self.stage_name} Started\")\n",
    "        print(f\"   Target: {self.total_epochs} epochs\")\n",
    "        self.epoch_pbar = tqdm(\n",
    "            total=self.total_epochs,\n",
    "            desc=f\"üìà {self.stage_name}\",\n",
    "            unit=\"epoch\",\n",
    "            position=0,\n",
    "            leave=True,\n",
    "            bar_format='{l_bar}{bar:30}{r_bar}{bar:-10b}'\n",
    "        )\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "        # Get total batches\n",
    "        if hasattr(self.model, '_train_counter'):\n",
    "            total_batches = self.params.get('steps', 0)\n",
    "        else:\n",
    "            total_batches = self.params.get('steps', 0)\n",
    "        \n",
    "        self.batch_pbar = tqdm(\n",
    "            total=total_batches,\n",
    "            desc=f\"  Epoch {epoch+1}/{self.total_epochs}\",\n",
    "            unit=\"batch\",\n",
    "            position=1,\n",
    "            leave=False,\n",
    "            bar_format='{l_bar}{bar:20}{r_bar}'\n",
    "        )\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.batch_pbar:\n",
    "            self.batch_pbar.update(1)\n",
    "            # Update metrics in progress bar\n",
    "            loss = logs.get('loss', 0)\n",
    "            acc = logs.get('accuracy', 0)\n",
    "            self.batch_pbar.set_postfix({\n",
    "                'loss': f'{loss:.4f}',\n",
    "                'acc': f'{acc:.3f}'\n",
    "            })\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.batch_pbar:\n",
    "            self.batch_pbar.close()\n",
    "        \n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "        \n",
    "        # Calculate ETA\n",
    "        avg_epoch_time = np.mean(self.epoch_times)\n",
    "        remaining_epochs = self.total_epochs - (epoch + 1)\n",
    "        eta_seconds = remaining_epochs * avg_epoch_time\n",
    "        eta = str(timedelta(seconds=int(eta_seconds)))\n",
    "        \n",
    "        # Total elapsed time\n",
    "        total_elapsed = time.time() - self.stage_start_time\n",
    "        elapsed_str = str(timedelta(seconds=int(total_elapsed)))\n",
    "        \n",
    "        # Update epoch progress bar\n",
    "        self.epoch_pbar.update(1)\n",
    "        \n",
    "        val_loss = logs.get('val_loss', 0)\n",
    "        val_acc = logs.get('val_accuracy', 0)\n",
    "        train_acc = logs.get('accuracy', 0)\n",
    "        \n",
    "        self.epoch_pbar.set_postfix({\n",
    "            'val_acc': f'{val_acc:.3f}',\n",
    "            'train_acc': f'{train_acc:.3f}',\n",
    "            'ETA': eta,\n",
    "            'elapsed': elapsed_str\n",
    "        })\n",
    "        \n",
    "        # Print detailed epoch summary\n",
    "        print(f\"\\n   ‚úÖ Epoch {epoch+1}: val_acc={val_acc:.4f}, val_loss={val_loss:.4f}, \"\n",
    "              f\"time={epoch_time:.1f}s, ETA={eta}\")\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.epoch_pbar:\n",
    "            self.epoch_pbar.close()\n",
    "        total_time = time.time() - self.stage_start_time\n",
    "        print(f\"\\n‚úÖ {self.stage_name} Complete!\")\n",
    "        print(f\"   Total time: {str(timedelta(seconds=int(total_time)))}\")\n",
    "        print(f\"   Avg epoch time: {np.mean(self.epoch_times):.1f}s\")\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# üî• GRAD-CAM CALLBACK (Visualize during training)\n",
    "# =============================================================\n",
    "class GradCAMCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Generate Grad-CAM visualizations during training\"\"\"\n",
    "    \n",
    "    def __init__(self, validation_data, class_names, save_dir, frequency=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            validation_data: Validation dataset\n",
    "            class_names: List of class names\n",
    "            save_dir: Directory to save Grad-CAM images\n",
    "            frequency: Generate Grad-CAM every N epochs\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.class_names = class_names\n",
    "        self.save_dir = save_dir\n",
    "        self.frequency = frequency\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.frequency == 0:\n",
    "            print(f\"\\nüî• Generating Grad-CAM for epoch {epoch + 1}...\")\n",
    "            \n",
    "            # Get a batch of validation images\n",
    "            for images, labels in self.validation_data.take(1):\n",
    "                save_path = os.path.join(self.save_dir, f'gradcam_epoch_{epoch+1}.png')\n",
    "                \n",
    "                # Run in thread to not block training\n",
    "                def generate_gradcam():\n",
    "                    try:\n",
    "                        visualize_gradcam_batch(\n",
    "                            self.model, \n",
    "                            images.numpy()[:4], \n",
    "                            labels.numpy()[:4],\n",
    "                            self.class_names,\n",
    "                            num_samples=4,\n",
    "                            save_path=save_path\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Grad-CAM error: {e}\")\n",
    "                \n",
    "                # Use threading to avoid blocking\n",
    "                thread = threading.Thread(target=generate_gradcam)\n",
    "                thread.start()\n",
    "                break\n",
    "\n",
    "\n",
    "# Create PlantVillage datasets\n",
    "print(\"üì¶ Creating PlantVillage datasets...\")\n",
    "print(f\"   Using {NUM_WORKERS} CPU workers for parallel loading\")\n",
    "\n",
    "train_plantvillage_raw = create_dataset(\n",
    "    PLANTVILLAGE_PATH, IMG_SIZE, BATCH_SIZE, \n",
    "    validation_split=0.2, subset='training'\n",
    ")\n",
    "val_plantvillage_raw = create_dataset(\n",
    "    PLANTVILLAGE_PATH, IMG_SIZE, BATCH_SIZE,\n",
    "    validation_split=0.2, subset='validation'\n",
    ")\n",
    "\n",
    "# Apply optimized pipeline\n",
    "train_plantvillage = build_pipeline(train_plantvillage_raw, is_training=True)\n",
    "val_plantvillage = build_pipeline(val_plantvillage_raw, is_training=False)\n",
    "\n",
    "train_batches = tf.data.experimental.cardinality(train_plantvillage_raw).numpy()\n",
    "val_batches = tf.data.experimental.cardinality(val_plantvillage_raw).numpy()\n",
    "\n",
    "print(f\"\\n‚úÖ PlantVillage datasets ready\")\n",
    "print(f\"   Training: {train_batches} batches √ó {BATCH_SIZE} = ~{train_batches * BATCH_SIZE:,} images\")\n",
    "print(f\"   Validation: {val_batches} batches √ó {BATCH_SIZE} = ~{val_batches * BATCH_SIZE:,} images\")\n",
    "print(f\"   ‚ö° Multiprocessing: {NUM_WORKERS} workers\")\n",
    "print(f\"   üé® Training augmentation: flip, brightness, contrast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c8d714",
   "metadata": {},
   "source": [
    "## ‚úÖ Pre-Training Validation\n",
    "\n",
    "Run this cell to verify everything is set up correctly before training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ PRE-TRAINING VALIDATION (T4 Optimized)\n",
    "print(\"=\" * 60)\n",
    "print(\"üîç PRE-TRAINING VALIDATION\")\n",
    "print(f\"‚è±Ô∏è Session time: {get_session_time()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "errors = []\n",
    "\n",
    "# 1. GPU Check\n",
    "print(\"\\n1Ô∏è‚É£ GPU Check...\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    gpu_name = gpus[0].name\n",
    "    print(f\"   ‚úÖ GPU: {gpu_name}\")\n",
    "    # Check if T4\n",
    "    try:\n",
    "        !nvidia-smi --query-gpu=name --format=csv,noheader\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    errors.append(\"No GPU detected!\")\n",
    "    print(\"   ‚ö†Ô∏è No GPU - training will be MUCH slower\")\n",
    "\n",
    "# 2. Quick data test\n",
    "print(\"\\n2Ô∏è‚É£ Data Pipeline Check...\")\n",
    "try:\n",
    "    import time\n",
    "    start = time.time()\n",
    "    for batch_images, batch_labels in train_plantvillage.take(1):\n",
    "        load_time = time.time() - start\n",
    "        print(f\"   ‚úÖ Batch shape: {batch_images.shape}\")\n",
    "        print(f\"   ‚úÖ Labels shape: {batch_labels.shape}\")\n",
    "        print(f\"   ‚úÖ Image dtype: {batch_images.dtype}\")\n",
    "        print(f\"   ‚ö° First batch load: {load_time:.2f}s\")\n",
    "        break\n",
    "except Exception as e:\n",
    "    errors.append(f\"Data pipeline error: {e}\")\n",
    "    print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "# 3. Memory check\n",
    "print(\"\\n3Ô∏è‚É£ Memory Status...\")\n",
    "try:\n",
    "    import psutil\n",
    "    ram_total = psutil.virtual_memory().total / (1024**3)\n",
    "    ram_avail = psutil.virtual_memory().available / (1024**3)\n",
    "    print(f\"   ‚úÖ RAM: {ram_avail:.1f}GB available / {ram_total:.1f}GB total\")\n",
    "    if ram_avail < 3:\n",
    "        print(\"   ‚ö†Ô∏è Low RAM - data copied to local SSD helps prevent crashes\")\n",
    "except:\n",
    "    print(\"   ‚ÑπÔ∏è Could not check RAM\")\n",
    "\n",
    "# 4. GPU Memory check\n",
    "print(\"\\n4Ô∏è‚É£ GPU Memory Check...\")\n",
    "try:\n",
    "    !nvidia-smi --query-gpu=memory.total,memory.free --format=csv,noheader\n",
    "except:\n",
    "    print(\"   ‚ÑπÔ∏è Could not query GPU memory\")\n",
    "\n",
    "# 5. Existing checkpoints\n",
    "print(\"\\n5Ô∏è‚É£ Checkpoint Status...\")\n",
    "stage2_exists = os.path.exists(os.path.join(DRIVE_CHECKPOINT_DIR, 'stage2_plantvillage_best.keras'))\n",
    "stage3_exists = os.path.exists(os.path.join(DRIVE_CHECKPOINT_DIR, 'unified_nutrient_best.keras'))\n",
    "print(f\"   Stage 2 checkpoint: {'‚úÖ Found (will resume)' if stage2_exists else '‚ùå None (fresh start)'}\")\n",
    "print(f\"   Stage 3 checkpoint: {'‚úÖ Found (will resume)' if stage3_exists else '‚ùå None (fresh start)'}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if errors:\n",
    "    print(\"‚ùå ISSUES FOUND:\")\n",
    "    for e in errors:\n",
    "        print(f\"   ‚Ä¢ {e}\")\n",
    "else:\n",
    "    print(\"‚úÖ ALL CHECKS PASSED!\")\n",
    "    print(f\"\\nüöÄ T4 GPU OPTIMIZED SETTINGS:\")\n",
    "    print(f\"   ‚Ä¢ Batch size: {BATCH_SIZE} (fills 16GB VRAM)\")\n",
    "    print(f\"   ‚Ä¢ Mixed precision: FP16\")\n",
    "    print(f\"   ‚Ä¢ JIT/XLA compilation: Enabled\")\n",
    "    print(f\"   ‚Ä¢ AUTOTUNE prefetch: Enabled\")\n",
    "    print(f\"   ‚Ä¢ Data location: Local SSD (fast I/O)\")\n",
    "    print(f\"\\n‚ö° Expected: ~1-2 min/epoch (10x faster than Drive I/O)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a853f",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Stage 2: Build Model with MobileNetV2 Base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed2908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes, input_shape=(224, 224, 3), freeze_base=True, dropout_rate=0.3):\n",
    "    \"\"\"Create MobileNetV2-based model optimized for T4 GPU\"\"\"\n",
    "    \n",
    "    # Load MobileNetV2 with ImageNet weights\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    base_model.trainable = not freeze_base\n",
    "    \n",
    "    # Balanced classification head (prevents overfitting)\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(256, activation='relu', \n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(dropout_rate * 0.8),  # Slightly less on second dropout\n",
    "        # Float32 output for numerical stability with mixed precision\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', dtype='float32')\n",
    "    ])\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Get number of PlantVillage classes\n",
    "num_plantvillage_classes = len(plantvillage_classes)\n",
    "\n",
    "# =============================================================\n",
    "# üîÑ CHECKPOINT RESUME: Load existing model if available\n",
    "# =============================================================\n",
    "STAGE2_CHECKPOINT = os.path.join(DRIVE_CHECKPOINT_DIR, 'stage2_plantvillage_best.keras')\n",
    "STAGE2_LOCAL = os.path.join(OUTPUT_DIR, 'stage2_plantvillage_best.keras')\n",
    "\n",
    "model_stage2 = None\n",
    "resume_stage2 = False\n",
    "\n",
    "# Check for existing checkpoint (Drive first, then local)\n",
    "for checkpoint_path in [STAGE2_CHECKPOINT, STAGE2_LOCAL]:\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            print(f\"üîÑ Found existing Stage 2 checkpoint!\")\n",
    "            print(f\"   Loading from: {checkpoint_path}\")\n",
    "            model_stage2 = tf.keras.models.load_model(checkpoint_path)\n",
    "            \n",
    "            # Verify it has correct output shape\n",
    "            if model_stage2.output_shape[-1] == num_plantvillage_classes:\n",
    "                resume_stage2 = True\n",
    "                print(f\"‚úÖ Resuming from checkpoint ({num_plantvillage_classes} classes)\")\n",
    "                \n",
    "                # Evaluate current performance\n",
    "                print(\"üìä Evaluating checkpoint...\")\n",
    "                val_loss, val_acc = model_stage2.evaluate(val_plantvillage, verbose=0)\n",
    "                print(f\"   Current val_accuracy: {val_acc:.4f}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Checkpoint has different classes ({model_stage2.output_shape[-1]} vs {num_plantvillage_classes})\")\n",
    "                model_stage2 = None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not load checkpoint: {e}\")\n",
    "            model_stage2 = None\n",
    "\n",
    "# Create new model if no valid checkpoint\n",
    "if model_stage2 is None:\n",
    "    print(f\"üèóÔ∏è Creating NEW model for PlantVillage ({num_plantvillage_classes} classes)...\")\n",
    "    model_stage2, base_model = create_model(\n",
    "        num_plantvillage_classes, \n",
    "        freeze_base=True,\n",
    "        dropout_rate=DROPOUT_RATE\n",
    "    )\n",
    "else:\n",
    "    base_model = model_stage2.layers[0]\n",
    "\n",
    "# Compile with JIT compilation for 10-20% speedup\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_STAGE2)\n",
    "\n",
    "model_stage2.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    "    jit_compile=True  # XLA compilation - 10-20% faster on T4\n",
    ")\n",
    "\n",
    "# Count trainable params\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in model_stage2.trainable_weights])\n",
    "print(f\"\\nüîí Base model frozen: {not base_model.trainable}\")\n",
    "print(f\"üìä Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"üíæ Mixed precision: FP16 enabled\")\n",
    "print(f\"‚ö° JIT/XLA compilation: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa4eee",
   "metadata": {},
   "source": [
    "## üéØ Stage 2: Train on PlantVillage Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0dc4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting Stage 2: PlantVillage Fine-tuning\")\n",
    "print(f\"‚è±Ô∏è Epochs: {PLANTVILLAGE_EPOCHS} | LR: {LEARNING_RATE_STAGE2} | Batch: {BATCH_SIZE}\")\n",
    "if resume_stage2:\n",
    "    print(\"üîÑ RESUMING from previous checkpoint\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Callbacks with TQDM progress and Grad-CAM\n",
    "callbacks_stage2 = [\n",
    "    # TQDM Progress with ETA\n",
    "    TQDMProgressCallback(PLANTVILLAGE_EPOCHS, stage_name=\"Stage 2: PlantVillage\"),\n",
    "    \n",
    "    # Grad-CAM visualization every 2 epochs\n",
    "    GradCAMCallback(\n",
    "        validation_data=val_plantvillage,\n",
    "        class_names=plantvillage_classes,\n",
    "        save_dir=os.path.join(GRADCAM_DIR, 'stage2'),\n",
    "        frequency=2\n",
    "    ),\n",
    "    \n",
    "    # Early stopping\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        min_delta=0.005\n",
    "    ),\n",
    "    \n",
    "    # Reduce LR on plateau\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.3,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Save best to local (fast)\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(OUTPUT_DIR, 'stage2_plantvillage_best.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    ),\n",
    "    \n",
    "    # Save best to Drive (persistent)\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(DRIVE_CHECKPOINT_DIR, 'stage2_plantvillage_best.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train with progress tracking\n",
    "TRAINING_START_TIME = datetime.now()\n",
    "\n",
    "history_stage2 = model_stage2.fit(\n",
    "    train_plantvillage,\n",
    "    validation_data=val_plantvillage,\n",
    "    epochs=PLANTVILLAGE_EPOCHS,\n",
    "    callbacks=callbacks_stage2,\n",
    "    verbose=0  # Disable default output, use TQDM instead\n",
    ")\n",
    "\n",
    "# Calculate training stats\n",
    "best_val_acc = max(history_stage2.history['val_accuracy'])\n",
    "best_val_loss = min(history_stage2.history['val_loss'])\n",
    "final_train_acc = history_stage2.history['accuracy'][-1]\n",
    "training_time = datetime.now() - TRAINING_START_TIME\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ Stage 2 completed in {training_time}\")\n",
    "print(f\"üìà Best val accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"üìâ Best val loss: {best_val_loss:.4f}\")\n",
    "print(f\"üìä Final train accuracy: {final_train_acc:.4f}\")\n",
    "\n",
    "# Check for overfitting/underfitting\n",
    "gap = final_train_acc - best_val_acc\n",
    "if gap > 0.15:\n",
    "    print(f\"‚ö†Ô∏è Overfitting detected (train-val gap: {gap:.2%})\")\n",
    "elif best_val_acc < 0.7:\n",
    "    print(f\"‚ö†Ô∏è Possible underfitting (val_acc: {best_val_acc:.2%})\")\n",
    "else:\n",
    "    print(f\"‚úÖ Good generalization (train-val gap: {gap:.2%})\")\n",
    "\n",
    "print(f\"\\nüíæ Checkpoints saved to:\")\n",
    "print(f\"   Local: {OUTPUT_DIR}\")\n",
    "print(f\"   Drive: {DRIVE_CHECKPOINT_DIR}\")\n",
    "print(f\"   üî• Grad-CAM: {GRADCAM_DIR}/stage2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0802c5",
   "metadata": {},
   "source": [
    "## üìà Stage 2 Results Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history_stage2.history['accuracy'], 'b-', label='Train')\n",
    "axes[0].plot(history_stage2.history['val_accuracy'], 'r-', label='Val')\n",
    "axes[0].set_title('Stage 2: Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_stage2.history['loss'], 'b-', label='Train')\n",
    "axes[1].plot(history_stage2.history['val_loss'], 'r-', label='Val')\n",
    "axes[1].set_title('Stage 2: Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'stage2_training_history.png'), dpi=150)\n",
    "plt.savefig(os.path.join(DRIVE_CHECKPOINT_DIR, 'stage2_training_history.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Memory cleanup (preserve model references)\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"üßπ Memory cleaned (models preserved for Stage 3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651b0c7",
   "metadata": {},
   "source": [
    "## üîÑ Stage 3: Build UNIFIED Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3160d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build unified dataset by combining all crops\n",
    "print(\"üèóÔ∏è Building UNIFIED dataset...\")\n",
    "\n",
    "UNIFIED_DATASET_PATH = '/content/unified_nutrient_dataset'\n",
    "\n",
    "# Check if unified dataset already exists\n",
    "if os.path.exists(UNIFIED_DATASET_PATH):\n",
    "    existing_classes = [d for d in os.listdir(UNIFIED_DATASET_PATH) \n",
    "                        if os.path.isdir(os.path.join(UNIFIED_DATASET_PATH, d))]\n",
    "    if len(existing_classes) > 10:\n",
    "        print(f\"‚úÖ Already exists with {len(existing_classes)} classes!\")\n",
    "        unified_classes = existing_classes\n",
    "    else:\n",
    "        import shutil\n",
    "        shutil.rmtree(UNIFIED_DATASET_PATH)\n",
    "        os.makedirs(UNIFIED_DATASET_PATH)\n",
    "        unified_classes = []\n",
    "else:\n",
    "    os.makedirs(UNIFIED_DATASET_PATH)\n",
    "    unified_classes = []\n",
    "\n",
    "if len(unified_classes) == 0:\n",
    "    skipped_crops = []\n",
    "    \n",
    "    for crop, folder_name in CROP_DATASETS.items():\n",
    "        crop_path = os.path.join(NUTRIENT_DATASETS_ROOT, folder_name)\n",
    "        \n",
    "        if not os.path.exists(crop_path):\n",
    "            skipped_crops.append(crop)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            crop_classes = [d for d in os.listdir(crop_path) \n",
    "                            if os.path.isdir(os.path.join(crop_path, d))]\n",
    "        except:\n",
    "            skipped_crops.append(crop)\n",
    "            continue\n",
    "        \n",
    "        for cls in crop_classes:\n",
    "            try:\n",
    "                clean_cls = cls.replace(f\"{crop}_\", \"\").replace(f\"{crop}__\", \"\")\n",
    "                unified_class_name = f\"{crop}_{clean_cls}\"\n",
    "                \n",
    "                src_dir = os.path.join(crop_path, cls)\n",
    "                dst_dir = os.path.join(UNIFIED_DATASET_PATH, unified_class_name)\n",
    "                \n",
    "                if not os.path.exists(dst_dir):\n",
    "                    os.symlink(src_dir, dst_dir)\n",
    "                    unified_classes.append(unified_class_name)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(f\"  ‚úÖ {crop.upper()}: {len([c for c in unified_classes if c.startswith(crop)])} classes\")\n",
    "    \n",
    "    if skipped_crops:\n",
    "        print(f\"‚ö†Ô∏è Skipped: {', '.join(skipped_crops)}\")\n",
    "\n",
    "if len(unified_classes) == 0:\n",
    "    raise RuntimeError(\"‚ùå No classes! Check Google Drive paths.\")\n",
    "\n",
    "class_names = sorted(unified_classes)\n",
    "num_unified_classes = len(class_names)\n",
    "\n",
    "print(f\"\\n‚úÖ Unified dataset: {num_unified_classes} classes\")\n",
    "\n",
    "# Create MEMORY-SAFE datasets\n",
    "print(\"üì¶ Creating datasets (MEMORY-SAFE)...\")\n",
    "\n",
    "train_nutrient_raw = create_dataset(\n",
    "    UNIFIED_DATASET_PATH, IMG_SIZE, BATCH_SIZE,\n",
    "    validation_split=0.2, subset='training'\n",
    ")\n",
    "val_nutrient_raw = create_dataset(\n",
    "    UNIFIED_DATASET_PATH, IMG_SIZE, BATCH_SIZE,\n",
    "    validation_split=0.2, subset='validation'\n",
    ")\n",
    "\n",
    "train_nutrient = build_pipeline(train_nutrient_raw, is_training=True)\n",
    "val_nutrient = build_pipeline(val_nutrient_raw, is_training=False)\n",
    "\n",
    "print(f\"‚úÖ Datasets ready (MEMORY-SAFE)\")\n",
    "print(f\"   Training: {tf.data.experimental.cardinality(train_nutrient_raw).numpy()} batches\")\n",
    "print(f\"   Validation: {tf.data.experimental.cardinality(val_nutrient_raw).numpy()} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf0ec80",
   "metadata": {},
   "source": [
    "## üîß Stage 3: Adapt Model for Unified Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb2a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# üîÑ STAGE 3: Adapt Model for Unified Classes (with Resume)\n",
    "# =============================================================\n",
    "if 'num_unified_classes' not in locals() or num_unified_classes == 0:\n",
    "    raise RuntimeError(\"‚ö†Ô∏è Run 'Build UNIFIED Dataset' cell first!\")\n",
    "\n",
    "print(f\"üîß Setting up Stage 3 for {num_unified_classes} unified classes...\")\n",
    "\n",
    "# Check for existing Stage 3 checkpoint\n",
    "STAGE3_CHECKPOINT = os.path.join(DRIVE_CHECKPOINT_DIR, 'unified_nutrient_best.keras')\n",
    "STAGE3_LOCAL = os.path.join(OUTPUT_DIR, 'unified_nutrient_best.keras')\n",
    "\n",
    "model_stage3 = None\n",
    "resume_stage3 = False\n",
    "initial_epoch = 0\n",
    "\n",
    "# Try to load existing checkpoint\n",
    "for checkpoint_path in [STAGE3_CHECKPOINT, STAGE3_LOCAL]:\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            print(f\"üîÑ Found existing Stage 3 checkpoint!\")\n",
    "            print(f\"   Loading from: {checkpoint_path}\")\n",
    "            model_stage3 = tf.keras.models.load_model(checkpoint_path)\n",
    "            \n",
    "            # Verify correct output shape\n",
    "            if model_stage3.output_shape[-1] == num_unified_classes:\n",
    "                resume_stage3 = True\n",
    "                print(f\"‚úÖ Checkpoint valid ({num_unified_classes} classes)\")\n",
    "                \n",
    "                # Evaluate current performance\n",
    "                print(\"üìä Evaluating checkpoint...\")\n",
    "                results = model_stage3.evaluate(val_nutrient, verbose=0)\n",
    "                print(f\"   Current - Loss: {results[0]:.4f}, Accuracy: {results[1]:.4f}\")\n",
    "                \n",
    "                # Check training history for initial_epoch\n",
    "                history_path = os.path.join(DRIVE_CHECKPOINT_DIR, 'stage3_history.json')\n",
    "                if os.path.exists(history_path):\n",
    "                    with open(history_path, 'r') as f:\n",
    "                        prev_history = json.load(f)\n",
    "                        initial_epoch = len(prev_history.get('accuracy', []))\n",
    "                        print(f\"   Resuming from epoch {initial_epoch}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Class mismatch ({model_stage3.output_shape[-1]} vs {num_unified_classes})\")\n",
    "                print(\"   Creating new model (classes changed)\")\n",
    "                model_stage3 = None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not load: {e}\")\n",
    "            model_stage3 = None\n",
    "\n",
    "# Create new model if needed\n",
    "if model_stage3 is None:\n",
    "    print(f\"üèóÔ∏è Creating NEW unified model...\")\n",
    "    \n",
    "    # Get base model from Stage 2\n",
    "    base_model_stage2 = model_stage2.layers[0]\n",
    "    base_model_stage2.trainable = False  # Keep frozen initially\n",
    "    \n",
    "    # Balanced classification head (prevents overfitting)\n",
    "    model_stage3 = tf.keras.Sequential([\n",
    "        base_model_stage2,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dropout(DROPOUT_RATE),\n",
    "        tf.keras.layers.Dense(384, activation='relu',\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(DROPOUT_RATE * 0.8),\n",
    "        tf.keras.layers.Dense(num_unified_classes, activation='softmax', dtype='float32')\n",
    "    ], name='unified_nutrient_model')\n",
    "\n",
    "# Compile with JIT for speed\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_STAGE3)\n",
    "\n",
    "model_stage3.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top3_acc')],\n",
    "    jit_compile=True  # XLA compilation - 10-20% faster\n",
    ")\n",
    "\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in model_stage3.trainable_weights])\n",
    "print(f\"\\nüìä Trainable params: {trainable_params:,}\")\n",
    "print(f\"üéØ Output classes: {num_unified_classes}\")\n",
    "print(f\"‚ö° JIT/XLA compilation: Enabled\")\n",
    "print(f\"üîÑ Resume training: {'Yes (epoch ' + str(initial_epoch) + ')' if resume_stage3 else 'No (fresh start)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb2661",
   "metadata": {},
   "source": [
    "## üéØ Stage 3: Train on UNIFIED Nutrient Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting Stage 3: UNIFIED Nutrient Detection\")\n",
    "print(f\"üåæ Training ALL {len(CROP_DATASETS)} crops | Epochs: {UNIFIED_EPOCHS} | LR: {LEARNING_RATE_STAGE3}\")\n",
    "if resume_stage3:\n",
    "    print(f\"üîÑ RESUMING from epoch {initial_epoch}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Callbacks with TQDM progress and Grad-CAM\n",
    "callbacks_stage3 = [\n",
    "    # TQDM Progress with ETA\n",
    "    TQDMProgressCallback(UNIFIED_EPOCHS, stage_name=\"Stage 3: Unified Nutrients\"),\n",
    "    \n",
    "    # Grad-CAM visualization every 3 epochs\n",
    "    GradCAMCallback(\n",
    "        validation_data=val_nutrient,\n",
    "        class_names=class_names,\n",
    "        save_dir=os.path.join(GRADCAM_DIR, 'stage3'),\n",
    "        frequency=3\n",
    "    ),\n",
    "    \n",
    "    # Early stopping - balanced patience\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        min_delta=0.001\n",
    "    ),\n",
    "    \n",
    "    # Reduce LR on plateau\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Save best to local (fast)\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(OUTPUT_DIR, 'unified_nutrient_best.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    ),\n",
    "    \n",
    "    # Save best to Drive (persistent)\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(DRIVE_CHECKPOINT_DIR, 'unified_nutrient_best.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train with progress tracking\n",
    "stage3_start = datetime.now()\n",
    "\n",
    "history_stage3 = model_stage3.fit(\n",
    "    train_nutrient,\n",
    "    validation_data=val_nutrient,\n",
    "    epochs=UNIFIED_EPOCHS,\n",
    "    initial_epoch=initial_epoch,\n",
    "    callbacks=callbacks_stage3,\n",
    "    verbose=0  # Disable default output, use TQDM instead\n",
    ")\n",
    "\n",
    "# Save training history for resume\n",
    "history_dict = {k: [float(v) for v in vals] for k, vals in history_stage3.history.items()}\n",
    "with open(os.path.join(DRIVE_CHECKPOINT_DIR, 'stage3_history.json'), 'w') as f:\n",
    "    json.dump(history_dict, f)\n",
    "\n",
    "# Calculate final stats\n",
    "best_val_acc = max(history_stage3.history['val_accuracy'])\n",
    "best_val_loss = min(history_stage3.history['val_loss'])\n",
    "best_top3_acc = max(history_stage3.history['val_top3_acc'])\n",
    "final_train_acc = history_stage3.history['accuracy'][-1]\n",
    "stage3_time = datetime.now() - stage3_start\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ Stage 3 completed in {stage3_time}\")\n",
    "print(f\"üìà Best val accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"üéØ Best top-3 accuracy: {best_top3_acc:.4f}\")\n",
    "print(f\"üìâ Best val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Total training time\n",
    "if TRAINING_START_TIME:\n",
    "    total_training_time = datetime.now() - TRAINING_START_TIME\n",
    "    print(f\"\\n‚è±Ô∏è TOTAL TRAINING TIME: {total_training_time}\")\n",
    "\n",
    "# Check for overfitting/underfitting\n",
    "gap = final_train_acc - best_val_acc\n",
    "if gap > 0.20:\n",
    "    print(f\"\\n‚ö†Ô∏è Overfitting detected (gap: {gap:.2%})\")\n",
    "elif best_val_acc < 0.5:\n",
    "    print(f\"\\n‚ö†Ô∏è Possible underfitting (val_acc: {best_val_acc:.2%})\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Good generalization (gap: {gap:.2%})\")\n",
    "\n",
    "print(f\"\\nüíæ Model & history saved to Drive\")\n",
    "print(f\"üî• Grad-CAM visualizations: {GRADCAM_DIR}/stage3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fcb26",
   "metadata": {},
   "source": [
    "## üìà Stage 3 Results Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce165f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history_stage3.history['accuracy'], 'b-', label='Train')\n",
    "axes[0].plot(history_stage3.history['val_accuracy'], 'r-', label='Val')\n",
    "axes[0].set_title('Stage 3: Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_stage3.history['loss'], 'b-', label='Train')\n",
    "axes[1].plot(history_stage3.history['val_loss'], 'r-', label='Val')\n",
    "axes[1].set_title('Stage 3: Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'stage3_training_history.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10140073",
   "metadata": {},
   "source": [
    "## üîç Model Evaluation & Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation (skip heavy confusion matrix for speed)\n",
    "print(\"üîç Evaluating UNIFIED model...\")\n",
    "results = model_stage3.evaluate(val_nutrient, verbose=0)\n",
    "\n",
    "print(f\"\\nüìä Validation Metrics:\")\n",
    "print(f\"   Loss: {results[0]:.4f}\")\n",
    "print(f\"   Accuracy: {results[1]:.4f}\")\n",
    "print(f\"   Top-3 Accuracy: {results[2]:.4f}\")\n",
    "\n",
    "# Quick per-crop accuracy (sample-based for speed)\n",
    "print(f\"\\nüåæ Per-Crop Performance (quick check):\")\n",
    "y_true, y_pred = [], []\n",
    "for images, labels in val_nutrient.take(20):  # Sample only\n",
    "    predictions = model_stage3.predict(images, verbose=0)\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "for crop in list(CROP_DATASETS.keys())[:6]:  # First 6 crops\n",
    "    crop_classes = [cls for cls in class_names if cls.startswith(f\"{crop}_\")]\n",
    "    if not crop_classes:\n",
    "        continue\n",
    "    crop_indices = [class_names.index(cls) for cls in crop_classes]\n",
    "    crop_mask = np.isin(y_true, crop_indices)\n",
    "    if crop_mask.sum() > 0:\n",
    "        crop_acc = (np.array(y_true)[crop_mask] == np.array(y_pred)[crop_mask]).mean()\n",
    "        print(f\"   {crop.upper():12s}: {crop_acc:.1%}\")\n",
    "\n",
    "# Save classification report\n",
    "report = classification_report(y_true, y_pred, target_names=[class_names[i] for i in sorted(set(y_true))], output_dict=True, zero_division=0)\n",
    "with open(os.path.join(OUTPUT_DIR, 'unified_classification_report.json'), 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446fc16",
   "metadata": {},
   "source": [
    "## üî• Final Grad-CAM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# üî• FINAL GRAD-CAM ANALYSIS\n",
    "# =============================================================\n",
    "# Generate comprehensive Grad-CAM visualizations for the final model\n",
    "\n",
    "print(\"üî• Generating Final Grad-CAM Analysis...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get multiple batches for comprehensive analysis\n",
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in val_nutrient.take(3):\n",
    "    all_images.extend(images.numpy())\n",
    "    all_labels.extend(labels.numpy())\n",
    "\n",
    "all_images = np.array(all_images)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Generate Grad-CAM for different crops\n",
    "print(\"\\nüìä Analyzing model attention per crop...\")\n",
    "\n",
    "gradcam = GradCAM(model_stage3)\n",
    "\n",
    "# Sample images from different crops\n",
    "crops_analyzed = []\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "sample_idx = 0\n",
    "for i, (img, label) in enumerate(zip(all_images, all_labels)):\n",
    "    if sample_idx >= 12:\n",
    "        break\n",
    "    \n",
    "    true_idx = np.argmax(label)\n",
    "    class_name = class_names[true_idx]\n",
    "    crop = class_name.split('_')[0]\n",
    "    \n",
    "    # Skip if we already have this crop\n",
    "    if crop in crops_analyzed and len(crops_analyzed) < 8:\n",
    "        continue\n",
    "    crops_analyzed.append(crop)\n",
    "    \n",
    "    # Get prediction and heatmap\n",
    "    pred = model_stage3.predict(np.expand_dims(img, 0), verbose=0)\n",
    "    pred_idx = np.argmax(pred[0])\n",
    "    \n",
    "    try:\n",
    "        heatmap = gradcam.get_gradcam_heatmap(np.expand_dims(img, 0), pred_idx)\n",
    "        display_img = ((img + 1) * 127.5).astype(np.uint8)\n",
    "        overlay = gradcam.overlay_heatmap(display_img, heatmap)\n",
    "        \n",
    "        axes[sample_idx].imshow(overlay)\n",
    "        correct = \"‚úÖ\" if pred_idx == true_idx else \"‚ùå\"\n",
    "        axes[sample_idx].set_title(f\"{class_name[:20]}\\n{pred[0][pred_idx]:.1%} {correct}\", fontsize=9)\n",
    "        axes[sample_idx].axis('off')\n",
    "        sample_idx += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(sample_idx, 12):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.suptitle(\"üî• Grad-CAM: Model Attention Analysis (Final Model)\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save to both locations\n",
    "final_gradcam_path = os.path.join(OUTPUT_DIR, 'final_gradcam_analysis.png')\n",
    "plt.savefig(final_gradcam_path, dpi=150, bbox_inches='tight')\n",
    "plt.savefig(os.path.join(DRIVE_CHECKPOINT_DIR, 'final_gradcam_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Final Grad-CAM analysis complete!\")\n",
    "print(f\"   Analyzed crops: {', '.join(set(crops_analyzed))}\")\n",
    "print(f\"   Saved to: {final_gradcam_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b395d",
   "metadata": {},
   "source": [
    "## üíæ Export to TensorFlow Lite for Mobile Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf78c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Converting to TensorFlow Lite...\")\n",
    "print(f\"‚è±Ô∏è Session time: {get_session_time()}\")\n",
    "check_time_limit()  # Warn if approaching 3-hour limit\n",
    "\n",
    "# Load best model\n",
    "best_model_path = os.path.join(OUTPUT_DIR, 'unified_nutrient_best.keras')\n",
    "if not os.path.exists(best_model_path):\n",
    "    # Try Drive checkpoint\n",
    "    best_model_path = os.path.join(DRIVE_CHECKPOINT_DIR, 'unified_nutrient_best.keras')\n",
    "    \n",
    "best_model = tf.keras.models.load_model(best_model_path)\n",
    "\n",
    "# Convert to TFLite with FP16 quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "print(\"‚öôÔ∏è Converting with FP16 quantization...\")\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save to both local and Drive\n",
    "tflite_path = os.path.join(OUTPUT_DIR, 'fasalvaidya_unified.tflite')\n",
    "tflite_drive_path = os.path.join(DRIVE_CHECKPOINT_DIR, 'fasalvaidya_unified.tflite')\n",
    "\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "with open(tflite_drive_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "keras_size = os.path.getsize(best_model_path) / (1024 * 1024)\n",
    "tflite_size = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n‚úÖ Conversion complete!\")\n",
    "print(f\"üìä Keras: {keras_size:.1f}MB ‚Üí TFLite: {tflite_size:.1f}MB ({(1-tflite_size/keras_size)*100:.0f}% smaller)\")\n",
    "print(f\"üöÄ Single model for {len(CROP_DATASETS)} crops!\")\n",
    "print(f\"\\nüíæ Saved to:\")\n",
    "print(f\"   Local: {tflite_path}\")\n",
    "print(f\"   Drive: {tflite_drive_path} (persistent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6488f02a",
   "metadata": {},
   "source": [
    "## üß™ Test TFLite Model Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b10789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick TFLite verification\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"üîç TFLite Model:\")\n",
    "print(f\"   Input: {input_details[0]['shape']} ({input_details[0]['dtype']})\")\n",
    "print(f\"   Output: {output_details[0]['shape']} ({num_unified_classes} classes)\")\n",
    "\n",
    "# Quick test\n",
    "for images, labels in val_nutrient.take(1):\n",
    "    test_image = images[0].numpy()\n",
    "    input_data = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    \n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    pred_idx = np.argmax(output[0])\n",
    "    true_idx = np.argmax(labels[0].numpy())\n",
    "    \n",
    "    print(f\"\\nüß™ Quick test:\")\n",
    "    print(f\"   True: {class_names[true_idx]}\")\n",
    "    print(f\"   Pred: {class_names[pred_idx]} ({output[0][pred_idx]:.1%})\")\n",
    "    print(f\"   {'‚úÖ CORRECT' if pred_idx == true_idx else '‚ùå INCORRECT'}\")\n",
    "    break\n",
    "\n",
    "print(\"\\n‚úÖ TFLite model verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ffab2",
   "metadata": {},
   "source": [
    "## üì§ Save Model Metadata & Class Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f144a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata and labels (to both local and Drive)\n",
    "print(\"üìù Saving metadata...\")\n",
    "\n",
    "crop_class_mapping = {crop: [c for c in class_names if c.startswith(f\"{crop}_\")] \n",
    "                      for crop in CROP_DATASETS.keys()}\n",
    "\n",
    "metadata = {\n",
    "    'model_type': 'unified_multi_crop',\n",
    "    'model_version': '2.0',\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'architecture': 'MobileNetV2',\n",
    "    'supported_crops': list(CROP_DATASETS.keys()),\n",
    "    'num_crops': len(CROP_DATASETS),\n",
    "    'input_shape': [IMG_SIZE, IMG_SIZE, 3],\n",
    "    'num_classes': num_unified_classes,\n",
    "    'class_names': class_names,\n",
    "    'crop_class_mapping': crop_class_mapping,\n",
    "    'metrics': {'accuracy': float(results[1]), 'top3_accuracy': float(results[2])},\n",
    "    'preprocessing': {'method': 'MobileNetV2', 'normalization': '[-1, 1]'},\n",
    "    'training_config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'plantvillage_epochs': PLANTVILLAGE_EPOCHS,\n",
    "        'unified_epochs': UNIFIED_EPOCHS,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'optimizations': ['mixed_precision_fp16', 'jit_compile', 'autotune_prefetch']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to both locations\n",
    "for save_dir in [OUTPUT_DIR, DRIVE_CHECKPOINT_DIR]:\n",
    "    with open(os.path.join(save_dir, 'unified_model_metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    with open(os.path.join(save_dir, 'labels.txt'), 'w') as f:\n",
    "        f.write('\\n'.join(class_names))\n",
    "\n",
    "print(f\"‚úÖ Saved: metadata.json, labels.txt\")\n",
    "print(f\"üìä {len(CROP_DATASETS)} crops, {num_unified_classes} classes\")\n",
    "print(f\"üíæ Saved to both local and Drive (persistent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3d8a9c",
   "metadata": {},
   "source": [
    "## üì¶ Download Models to Local Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ddb7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and download zip\n",
    "import shutil\n",
    "\n",
    "# Session summary\n",
    "print(\"=\" * 60)\n",
    "print(\"üéâ TRAINING SESSION COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚è±Ô∏è Total session time: {get_session_time()}\")\n",
    "print(f\"üìä Final validation accuracy: {results[1]:.4f}\")\n",
    "print(f\"üéØ Final top-3 accuracy: {results[2]:.4f}\")\n",
    "\n",
    "zip_filename = 'fasalvaidya_unified_model'\n",
    "shutil.make_archive(f'/content/{zip_filename}', 'zip', OUTPUT_DIR)\n",
    "\n",
    "print(f\"\\nüì¶ Created: {zip_filename}.zip\")\n",
    "print(f\"\\nüìÇ Contents:\")\n",
    "print(f\"   üì± fasalvaidya_unified.tflite ({tflite_size:.1f}MB)\")\n",
    "print(f\"   üíæ unified_nutrient_best.keras\")\n",
    "print(f\"   üìÑ unified_model_metadata.json\")\n",
    "print(f\"   üè∑Ô∏è labels.txt ({num_unified_classes} classes)\")\n",
    "print(f\"\\nüåæ Supports: {', '.join(list(CROP_DATASETS.keys())[:6])}...\")\n",
    "\n",
    "print(f\"\\nüíæ ALSO SAVED TO DRIVE (persistent):\")\n",
    "print(f\"   {DRIVE_CHECKPOINT_DIR}\")\n",
    "print(f\"   ‚úÖ Can resume training if disconnected!\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download(f'/content/{zip_filename}.zip')\n",
    "print(f\"\\n‚¨áÔ∏è Download started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a73ed9",
   "metadata": {},
   "source": [
    "## üéâ UNIFIED Model Training Complete!\n",
    "\n",
    "### üöÄ Performance Optimizations Applied\n",
    "\n",
    "This notebook is optimized for **1-1.5 hour training** with real-time progress tracking:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **TQDM Progress** | Real-time ETA, batch progress, metrics display |\n",
    "| **Grad-CAM** | Visualizes model attention during training |\n",
    "| **Multiprocessing** | Uses all CPU cores for data loading |\n",
    "| **Mixed Precision** | FP16 training (2x faster on GPU) |\n",
    "| **Threading** | Grad-CAM runs in background threads |\n",
    "| **XLA/JIT** | TensorFlow graph optimization |\n",
    "\n",
    "### ‚è±Ô∏è Training Time Budget\n",
    "\n",
    "| Stage | Estimated Time |\n",
    "|-------|---------------|\n",
    "| Setup & Data Copy | 5-10 min |\n",
    "| Stage 2 (PlantVillage, 5 epochs) | 15-25 min |\n",
    "| Stage 3 (Unified, 10 epochs) | 30-50 min |\n",
    "| Export & Analysis | 5-10 min |\n",
    "| **Total** | **~1-1.5 hours** |\n",
    "\n",
    "### üî• Grad-CAM Outputs\n",
    "\n",
    "Grad-CAM visualizations are saved at:\n",
    "- `stage2/gradcam_epoch_*.png` - PlantVillage training\n",
    "- `stage3/gradcam_epoch_*.png` - Unified training  \n",
    "- `final_gradcam_analysis.png` - Comprehensive analysis\n",
    "\n",
    "### üìä Progress Tracking Features\n",
    "\n",
    "- **Per-batch progress bar** with loss/accuracy\n",
    "- **Per-epoch progress bar** with ETA\n",
    "- **Real-time ETA** based on moving average\n",
    "- **Elapsed time** tracking\n",
    "- **Session time** monitoring (3-hour Colab limit)\n",
    "\n",
    "### üì¶ Output Files\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `fasalvaidya_unified.tflite` | Mobile-optimized model |\n",
    "| `unified_nutrient_best.keras` | Full Keras model |\n",
    "| `unified_model_metadata.json` | Model info & class mappings |\n",
    "| `labels.txt` | All class labels |\n",
    "| `final_gradcam_analysis.png` | Model attention visualization |\n",
    "\n",
    "### üîÑ Settings Used\n",
    "\n",
    "```python\n",
    "BATCH_SIZE = 16\n",
    "PLANTVILLAGE_EPOCHS = 5\n",
    "UNIFIED_EPOCHS = 10\n",
    "NUM_WORKERS = multiprocessing.cpu_count()\n",
    "Mixed Precision: FP16\n",
    "JIT/XLA: Enabled\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
