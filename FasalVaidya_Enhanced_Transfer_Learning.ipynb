{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "72e188b6",
      "metadata": {
        "id": "72e188b6"
      },
      "source": [
        "## üì¶ Setup & Environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce35c3c4",
      "metadata": {},
      "source": [
        "# üöÄ FasalVaidya: Unified Multi-Crop Nutrient Deficiency Detection\n",
        "\n",
        "## ‚ö†Ô∏è CRITICAL: First-Time Setup\n",
        "\n",
        "**BEFORE RUNNING ANY CELLS:**\n",
        "\n",
        "1. **üîÑ Restart Runtime** (REQUIRED if you've run this notebook before)\n",
        "   - Click: **Runtime ‚Üí Restart Runtime** (or press `Ctrl+M` then `.`)\n",
        "   - This clears any cached settings from previous sessions\n",
        "   - Ensures float32 policy is properly set\n",
        "\n",
        "2. **üì± Run cells sequentially** from top to bottom\n",
        "   - Each cell depends on previous cells\n",
        "   - Don't skip cells or run out of order\n",
        "\n",
        "3. **‚è±Ô∏è Expected Training Time:**\n",
        "   - Data copy: ~3-5 minutes (one-time)\n",
        "   - Stage 2 (PlantVillage): ~15-20 minutes\n",
        "   - Stage 3 (Nutrient): ~30-40 minutes\n",
        "   - **Total: ~50-65 minutes** on T4 GPU\n",
        "\n",
        "## üìã What This Notebook Does\n",
        "\n",
        "**Stage 1:** Download PlantVillage dataset (optional transfer learning base)\n",
        "**Stage 2:** Train on PlantVillage for general leaf disease recognition\n",
        "**Stage 3:** Fine-tune on unified 4-crop nutrient deficiency dataset\n",
        "\n",
        "## üéØ Optimizations Included\n",
        "\n",
        "- ‚úÖ **Float32 precision** (no mixed precision issues)\n",
        "- ‚úÖ **Local SSD data** (10-50x faster than Drive I/O)\n",
        "- ‚úÖ **XLA/JIT compilation** (10-20% speedup)\n",
        "- ‚úÖ **AUTOTUNE prefetch** (maximizes GPU utilization)\n",
        "- ‚úÖ **Optimized batch size** (32 for better GPU usage)\n",
        "- ‚úÖ **Smart augmentation** (flip, brightness, contrast, saturation, hue)\n",
        "- ‚úÖ **Checkpoint resume** (can continue if interrupted)\n",
        "\n",
        "## üåæ Supported Crops\n",
        "\n",
        "- üåæ **Rice** (Nitrogen, Phosphorus, Potassium deficiencies)\n",
        "- üåæ **Wheat** (Nitrogen deficiency)\n",
        "- üçÖ **Tomato** (Multi-nutrient deficiencies)\n",
        "- üåΩ **Maize** (Nitrogen-deficient and healthy)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xej3BOY_52ci",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xej3BOY_52ci",
        "outputId": "c7dee48f-e0d6-43b8-b4b2-b43125989aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b15cf59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b15cf59",
        "outputId": "2f6b2588-223d-46af-a69c-b8221f632a83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "CPU cores available: 2\n",
            "\n",
            "‚è±Ô∏è Session started at: 10:45:28\n",
            "   Target: Complete training within 1-1.5 hours\n",
            "   Checkpoints auto-save to Drive (training resumes from checkpoint)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q tensorflow>=2.15.0 kaggle opendatasets scikit-learn matplotlib seaborn tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import json\n",
        "import threading\n",
        "import multiprocessing\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(f\"CPU cores available: {multiprocessing.cpu_count()}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# =============================================================\n",
        "# ‚è±Ô∏è SESSION TIME TRACKER (Important for 3-hour limit!)\n",
        "# =============================================================\n",
        "SESSION_START_TIME = datetime.now()\n",
        "TRAINING_START_TIME = None\n",
        "\n",
        "def get_session_time():\n",
        "    \"\"\"Get elapsed session time\"\"\"\n",
        "    elapsed = datetime.now() - SESSION_START_TIME\n",
        "    hours = elapsed.seconds // 3600\n",
        "    minutes = (elapsed.seconds % 3600) // 60\n",
        "    return f\"{hours}h {minutes}m\"\n",
        "\n",
        "def get_eta(current_epoch, total_epochs, epoch_time):\n",
        "    \"\"\"Calculate ETA for training completion\"\"\"\n",
        "    remaining_epochs = total_epochs - current_epoch\n",
        "    eta_seconds = remaining_epochs * epoch_time\n",
        "    eta = timedelta(seconds=int(eta_seconds))\n",
        "    return str(eta)\n",
        "\n",
        "def check_time_limit(warn_minutes=150):\n",
        "    \"\"\"Warn if approaching 3-hour limit (180 min)\"\"\"\n",
        "    elapsed = (datetime.now() - SESSION_START_TIME).seconds // 60\n",
        "    remaining = 180 - elapsed\n",
        "    if elapsed >= warn_minutes:\n",
        "        print(f\"‚ö†Ô∏è WARNING: {remaining} minutes remaining before typical Colab disconnect!\")\n",
        "        print(f\"   Consider saving checkpoints and downloading results now.\")\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è Session started at: {SESSION_START_TIME.strftime('%H:%M:%S')}\")\n",
        "print(f\"   Target: Complete training within 1-1.5 hours\")\n",
        "print(f\"   Checkpoints auto-save to Drive (training resumes from checkpoint)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2ddc489",
      "metadata": {
        "id": "e2ddc489"
      },
      "source": [
        "## üîë Configuration\n",
        "\n",
        "### Set your crop type and paths here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a4e1728",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a4e1728",
        "outputId": "b631d39a-68ab-4100-f615-7add4eb48dc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Mixed precision enabled (FP16)\n",
            "‚ö° FAST MVP PROTOTYPING MODE (1-1.5 hour target)\n",
            "============================================================\n",
            "Training ONE model for ALL 12 crops\n",
            "\n",
            "üöÄ RAPID PROTOTYPING SETTINGS:\n",
            "   - Batch size: 16\n",
            "   - Mixed precision: FP16 (2x faster)\n",
            "   - XLA/JIT compilation: Enabled\n",
            "   - Multiprocessing workers: 2\n",
            "   - Prefetch buffer: 4\n",
            "   - Stage 2: 5 epochs\n",
            "   - Stage 3: 10 epochs\n",
            "   - Grad-CAM: Enabled (visualize during training)\n",
            "\n",
            "‚ö° Expected total time: ~1-1.5 hours for full training\n"
          ]
        }
      ],
      "source": [
        "# =============================================================\n",
        "# üöÄ OPTIMAL CONFIGURATION FOR FASTEST TRAINING\n",
        "# =============================================================\n",
        "\n",
        "# Root path to your \"Leaf Nutrient Data Sets\" folder on Google Drive\n",
        "NUTRIENT_DATASETS_ROOT = '/content/drive/MyDrive/Leaf Nutrient Data Sets'\n",
        "\n",
        "# üöÄ FAST MVP: Only 4 crops for quick training!\n",
        "CROP_DATASETS = {\n",
        "    'rice': 'Rice Nutrients',\n",
        "    'wheat': 'Wheat Nitrogen',\n",
        "    'tomato': 'Tomato Nutrients',\n",
        "    'maize': 'Maize Nutrients',\n",
        "}\n",
        "\n",
        "# =============================================================\n",
        "# üéØ OPTIMAL TRAINING HYPERPARAMETERS\n",
        "# =============================================================\n",
        "IMG_SIZE = 224  # MobileNetV2 native resolution\n",
        "BATCH_SIZE = 32  # Increased from 16 for better GPU utilization\n",
        "\n",
        "# Training epochs (Aggressive but effective)\n",
        "PLANTVILLAGE_EPOCHS = 8    # Stage 2: Quick transfer learning\n",
        "UNIFIED_EPOCHS = 15        # Stage 3: Nutrient detection (increased for better convergence)\n",
        "\n",
        "# Learning rates (Tuned for fast convergence)\n",
        "LEARNING_RATE_STAGE2 = 1e-3  # Aggressive learning\n",
        "LEARNING_RATE_STAGE3 = 5e-4  # Fine-tuning rate\n",
        "\n",
        "# Regularization\n",
        "DROPOUT_RATE = 0.3  # Increased to prevent overfitting\n",
        "\n",
        "# =============================================================\n",
        "# üöÄ PERFORMANCE OPTIMIZATIONS\n",
        "# =============================================================\n",
        "# Enable XLA compilation for up to 2x speedup\n",
        "tf.config.optimizer.set_jit(True)\n",
        "\n",
        "# GPU memory growth (prevents OOM errors)\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"‚úÖ Enabled memory growth for {len(gpus)} GPU(s)\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"‚ö†Ô∏è GPU config warning: {e}\")\n",
        "\n",
        "# Multiprocessing settings (use all available cores)\n",
        "NUM_WORKERS = multiprocessing.cpu_count()\n",
        "PREFETCH_BUFFER = tf.data.AUTOTUNE  # Let TF auto-tune prefetch size\n",
        "\n",
        "# CRITICAL: Use float32 for full precision (no mixed precision issues)\n",
        "tf.keras.mixed_precision.set_global_policy('float32')\n",
        "print(\"‚úÖ Using float32 policy for training (no mixed precision)\")\n",
        "\n",
        "# Output paths (persistent on Drive for checkpoint resume)\n",
        "OUTPUT_DIR = '/content/fasalvaidya_unified_model'\n",
        "DRIVE_CHECKPOINT_DIR = '/content/drive/MyDrive/FasalVaidya_Checkpoints'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(DRIVE_CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# =============================================================\n",
        "# üìä CONFIGURATION SUMMARY\n",
        "# =============================================================\n",
        "print(\"‚ö° OPTIMIZED TRAINING CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üåæ Crops: {len(CROP_DATASETS)} ({', '.join(CROP_DATASETS.keys())})\")\n",
        "print(f\"\\nüéØ Training Settings:\")\n",
        "print(f\"   ‚Ä¢ Image size: {IMG_SIZE}√ó{IMG_SIZE}\")\n",
        "print(f\"   ‚Ä¢ Batch size: {BATCH_SIZE} (optimized for GPU)\")\n",
        "print(f\"   ‚Ä¢ Precision: float32 (no mixed precision)\")\n",
        "print(f\"   ‚Ä¢ XLA compilation: {'‚úÖ Enabled' if tf.config.optimizer.get_jit() else '‚ùå Disabled'}\")\n",
        "print(f\"\\n‚öôÔ∏è Performance:\")\n",
        "print(f\"   ‚Ä¢ CPU workers: {NUM_WORKERS}\")\n",
        "print(f\"   ‚Ä¢ Prefetch: AUTOTUNE\")\n",
        "print(f\"   ‚Ä¢ Memory growth: {'‚úÖ Enabled' if gpus else '‚ö†Ô∏è No GPU detected'}\")\n",
        "print(f\"\\nüìà Epochs:\")\n",
        "print(f\"   ‚Ä¢ Stage 2 (PlantVillage): {PLANTVILLAGE_EPOCHS}\")\n",
        "print(f\"   ‚Ä¢ Stage 3 (Nutrient): {UNIFIED_EPOCHS}\")\n",
        "print(f\"\\n‚ö° Expected training time: ~45-70 minutes total\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "159ab98b",
      "metadata": {
        "id": "159ab98b"
      },
      "source": [
        "## üíæ Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d2242a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d2242a9",
        "outputId": "8ffb9d40-1fb2-4087-d55b-69e657976ed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Google Drive already mounted!\n",
            "\n",
            "üîç Searching for 'Leaf Nutrient Data Sets' folder...\n",
            "‚úÖ Found at: /content/drive/MyDrive/Leaf Nutrient Data Sets\n",
            "   Contains 12 folders\n",
            "\n",
            "‚úÖ Using dataset location: /content/drive/MyDrive/Leaf Nutrient Data Sets\n",
            "\n",
            "üîç Verifying crop datasets...\n",
            "‚úÖ RICE: 3 classes\n",
            "‚úÖ WHEAT: 3 classes\n",
            "‚úÖ TOMATO: 2 classes\n",
            "‚úÖ MAIZE: 2 classes\n",
            "‚úÖ BANANA: 3 classes\n",
            "‚úÖ COFFEE: 4 classes\n",
            "‚úÖ CUCUMBER: 4 classes\n",
            "‚úÖ EGGPLANT: 4 classes\n",
            "‚úÖ ASHGOURD: 7 classes\n",
            "‚úÖ BITTERGOURD: 9 classes\n",
            "‚úÖ RIDGEGOURD: 4 classes\n",
            "‚úÖ SNAKEGOURD: 5 classes\n",
            "\n",
            "‚úÖ All 12 crop datasets verified!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Robust check for already-mounted Drive\n",
        "drive_path = '/content/drive'\n",
        "is_mounted = False\n",
        "\n",
        "if os.path.exists(drive_path):\n",
        "    # Check if directory has content (indicates already mounted)\n",
        "    try:\n",
        "        if os.listdir(drive_path):\n",
        "            is_mounted = True\n",
        "            print(\"‚úÖ Google Drive already mounted!\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "if not is_mounted:\n",
        "    print(\"üìÅ Mounting Google Drive...\")\n",
        "    # Clean up if directory exists but is empty\n",
        "    if os.path.exists(drive_path) and not os.listdir(drive_path):\n",
        "        os.rmdir(drive_path)\n",
        "    drive.mount(drive_path)\n",
        "    print(\"‚úÖ Google Drive mounted successfully!\")\n",
        "\n",
        "# =============================================================\n",
        "# üîç SMART PATH DETECTION: Search All Possible Locations\n",
        "# =============================================================\n",
        "print(\"\\nüîç Searching for 'Leaf Nutrient Data Sets' folder...\")\n",
        "\n",
        "# List of possible locations to check\n",
        "search_paths = [\n",
        "    '/content/drive/MyDrive/Leaf Nutrient Data Sets',\n",
        "    '/content/drive/Shareddrives/Leaf Nutrient Data Sets',\n",
        "    '/content/drive/Shared drives/Leaf Nutrient Data Sets',\n",
        "]\n",
        "\n",
        "# Also search for shortcuts and nested locations\n",
        "mydrive_base = '/content/drive/MyDrive'\n",
        "if os.path.exists(mydrive_base):\n",
        "    # Search in .shortcut-targets-by-id (where \"Shared with me\" shortcuts appear)\n",
        "    shortcut_dir = os.path.join(mydrive_base, '.shortcut-targets-by-id')\n",
        "    if os.path.exists(shortcut_dir):\n",
        "        try:\n",
        "            for folder_id in os.listdir(shortcut_dir):\n",
        "                target_path = os.path.join(shortcut_dir, folder_id, 'Leaf Nutrient Data Sets')\n",
        "                if os.path.exists(target_path):\n",
        "                    search_paths.append(target_path)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Try each location\n",
        "found_location = None\n",
        "\n",
        "for search_path in search_paths:\n",
        "    if os.path.exists(search_path):\n",
        "        # Verify it has crop folders\n",
        "        try:\n",
        "            contents = os.listdir(search_path)\n",
        "            crop_folders = [f for f in contents if os.path.isdir(os.path.join(search_path, f))]\n",
        "            if len(crop_folders) >= 5:  # Should have at least 5 crop folders\n",
        "                print(f\"‚úÖ Found at: {search_path}\")\n",
        "                print(f\"   Contains {len(crop_folders)} folders\")\n",
        "                found_location = search_path\n",
        "                break\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "if found_location:\n",
        "    NUTRIENT_DATASETS_ROOT = found_location\n",
        "    print(f\"\\n‚úÖ Using dataset location: {NUTRIENT_DATASETS_ROOT}\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå 'Leaf Nutrient Data Sets' folder NOT FOUND!\")\n",
        "    print(f\"\\nüìÇ What's in your Drive:\")\n",
        "    try:\n",
        "        mydrive_items = os.listdir(mydrive_base)[:10]  # Show first 10 items\n",
        "        for item in mydrive_items:\n",
        "            item_path = os.path.join(mydrive_base, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                print(f\"   üìÅ {item}\")\n",
        "            else:\n",
        "                print(f\"   üìÑ {item}\")\n",
        "    except:\n",
        "        print(\"   (Could not list Drive contents)\")\n",
        "\n",
        "    print(f\"\\n‚ö†Ô∏è FOLDER IS IN 'SHARED WITH ME' - NOT ACCESSIBLE!\")\n",
        "    print(f\"\\n‚úÖ SOLUTION: Add shortcut to My Drive\")\n",
        "    print(f\"   1. Open Google Drive in browser: https://drive.google.com\")\n",
        "    print(f\"   2. Click 'Shared with me' in left sidebar\")\n",
        "    print(f\"   3. Right-click 'Leaf Nutrient Data Sets' folder\")\n",
        "    print(f\"   4. Select 'Add shortcut to Drive' or 'Organize' > 'Add shortcut'\")\n",
        "    print(f\"   5. Choose 'My Drive' root (don't put it in a subfolder)\")\n",
        "    print(f\"   6. Click 'Add' or 'Add shortcut'\")\n",
        "    print(f\"   7. Come back here and re-run this cell\")\n",
        "    print(f\"\\nüí° After adding shortcut, the folder will appear at:\")\n",
        "    print(f\"   /content/drive/MyDrive/Leaf Nutrient Data Sets\")\n",
        "\n",
        "# Verify ALL crop datasets exist (only if folder found)\n",
        "if found_location:\n",
        "    print(\"\\nüîç Verifying crop datasets...\")\n",
        "    missing_crops = []\n",
        "    for crop, folder_name in CROP_DATASETS.items():\n",
        "        crop_path = os.path.join(NUTRIENT_DATASETS_ROOT, folder_name)\n",
        "        if os.path.exists(crop_path):\n",
        "            num_classes = len([d for d in os.listdir(crop_path) if os.path.isdir(os.path.join(crop_path, d))])\n",
        "            print(f\"‚úÖ {crop.upper()}: {num_classes} classes\")\n",
        "        else:\n",
        "            print(f\"‚ùå {crop.upper()}: NOT FOUND\")\n",
        "            missing_crops.append(crop)\n",
        "\n",
        "    if missing_crops:\n",
        "        print(f\"\\n‚ö†Ô∏è WARNING: {len(missing_crops)} crop(s) not found: {', '.join(missing_crops)}\")\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ All {len(CROP_DATASETS)} crop datasets verified!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0539d26",
      "metadata": {
        "id": "e0539d26"
      },
      "source": [
        "## üöÄ Speed Optimization: Copy Data to Local Disk\n",
        "\n",
        "**This is the BIGGEST speed boost!** Copying from Drive to local SSD speeds up training 10-50x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd8f76e0",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd8f76e0",
        "outputId": "2ab9e0e6-ffa9-446b-b6aa-868956c77a75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üöÄ COPYING DATASETS TO LOCAL SSD (One-time per session)\n",
            "============================================================\n",
            "‚è≥ This takes 2-5 minutes but saves HOURS of training time!\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-637155573.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  ‚úÖ {crop}: Copied\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mcopy_success\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[1;32m    601\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    534\u001b[0m                         \u001b[0mcopy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrcobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdstname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0msrcentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m                 copytree(srcobj, dstname, symlinks, ignore, copy_function,\n\u001b[0m\u001b[1;32m    537\u001b[0m                          ignore_dangling_symlinks, dirs_exist_ok)\n\u001b[1;32m    538\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[1;32m    601\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0;31m# Will raise a SpecialFileError for unsupported file types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m                 \u001b[0mcopy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrcobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdstname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;31m# catch the Error from the recursive copytree so that we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;31m# continue with other files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0m_USE_CP_SENDFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                             \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# ...in oder to have a more informative exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# =============================================================\n",
        "# üöÄ OPTIMIZED LOCAL SSD COPY (10-50x FASTER I/O)\n",
        "# =============================================================\n",
        "# Reading from Google Drive is SLOW (network I/O)\n",
        "# Copying to /content/ uses Colab's fast local SSD\n",
        "# This one-time copy saves HOURS during training!\n",
        "\n",
        "import shutil\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "def copy_to_local_ssd(src_path, dest_name):\n",
        "    \"\"\"Optimized copy to local SSD with progress tracking\"\"\"\n",
        "    local_path = f\"/content/{dest_name}\"\n",
        "    \n",
        "    # Skip if already exists and populated\n",
        "    if os.path.exists(local_path):\n",
        "        try:\n",
        "            num_items = sum(1 for _ in Path(local_path).rglob('*') if _.is_file())\n",
        "            if num_items > 100:  # Sanity check\n",
        "                size_mb = sum(f.stat().st_size for f in Path(local_path).rglob('*') if f.is_file()) / (1024 * 1024)\n",
        "                print(f\"‚úÖ {dest_name}: Already on SSD ({num_items:,} files, {size_mb:.0f}MB)\")\n",
        "                return local_path\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    if not os.path.exists(src_path):\n",
        "        print(f\"‚ö†Ô∏è {dest_name}: Source not found at {src_path}\")\n",
        "        return src_path\n",
        "    \n",
        "    print(f\"üöÄ Copying {dest_name} to local SSD...\", end=\" \", flush=True)\n",
        "    start = time.time()\n",
        "    \n",
        "    # Remove existing if corrupted\n",
        "    if os.path.exists(local_path):\n",
        "        shutil.rmtree(local_path)\n",
        "    \n",
        "    # Fast copy with symlink preservation\n",
        "    shutil.copytree(src_path, local_path, symlinks=True)\n",
        "    \n",
        "    # Calculate stats\n",
        "    elapsed = time.time() - start\n",
        "    num_files = sum(1 for _ in Path(local_path).rglob('*') if _.is_file())\n",
        "    size_mb = sum(f.stat().st_size for f in Path(local_path).rglob('*') if f.is_file()) / (1024 * 1024)\n",
        "    \n",
        "    print(f\"‚úÖ {num_files:,} files, {size_mb:.0f}MB in {elapsed:.1f}s\")\n",
        "    return local_path\n",
        "\n",
        "# =============================================================\n",
        "# üì¶ COPY ALL NUTRIENT DATASETS TO LOCAL SSD\n",
        "# =============================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ COPYING DATASETS TO LOCAL SSD\")\n",
        "print(\"=\" * 70)\n",
        "print(\"‚è≥ One-time setup (2-5 min) - saves HOURS during training!\\n\")\n",
        "\n",
        "LOCAL_NUTRIENT_ROOT = '/content/local_nutrient_datasets'\n",
        "os.makedirs(LOCAL_NUTRIENT_ROOT, exist_ok=True)\n",
        "\n",
        "copy_success = []\n",
        "copy_failed = []\n",
        "\n",
        "for crop, folder_name in CROP_DATASETS.items():\n",
        "    src = os.path.join(NUTRIENT_DATASETS_ROOT, folder_name)\n",
        "    dst = os.path.join(LOCAL_NUTRIENT_ROOT, folder_name)\n",
        "    \n",
        "    try:\n",
        "        if os.path.exists(src):\n",
        "            # Check if already copied\n",
        "            if os.path.exists(dst):\n",
        "                num_files = sum(1 for _ in Path(dst).rglob('*.jpg')) + sum(1 for _ in Path(dst).rglob('*.png'))\n",
        "                if num_files > 50:  # Sanity check\n",
        "                    print(f\"‚úÖ {crop.upper()}: Already on SSD ({num_files:,} images)\")\n",
        "                    copy_success.append(crop)\n",
        "                    continue\n",
        "            \n",
        "            # Copy to local\n",
        "            print(f\"üöÄ {crop.upper()}: Copying...\", end=\" \", flush=True)\n",
        "            start = time.time()\n",
        "            \n",
        "            if os.path.exists(dst):\n",
        "                shutil.rmtree(dst)\n",
        "            \n",
        "            shutil.copytree(src, dst)\n",
        "            \n",
        "            num_files = sum(1 for _ in Path(dst).rglob('*.jpg')) + sum(1 for _ in Path(dst).rglob('*.png'))\n",
        "            elapsed = time.time() - start\n",
        "            \n",
        "            print(f\"‚úÖ {num_files:,} images in {elapsed:.1f}s\")\n",
        "            copy_success.append(crop)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è {crop.upper()}: Not found on Drive\")\n",
        "            copy_failed.append(crop)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {crop.upper()}: Failed - {e}\")\n",
        "        copy_failed.append(crop)\n",
        "\n",
        "# Update root path to local SSD\n",
        "NUTRIENT_DATASETS_ROOT = LOCAL_NUTRIENT_ROOT\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"‚úÖ {len(copy_success)}/{len(CROP_DATASETS)} crops ready on local SSD\")\n",
        "if copy_failed:\n",
        "    print(f\"‚ö†Ô∏è Failed: {', '.join(copy_failed)}\")\n",
        "print(f\"üöÄ Training will now be 10-50x FASTER!\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1061a425",
      "metadata": {
        "id": "1061a425"
      },
      "source": [
        "## üå± Stage 1: Download PlantVillage Dataset from Kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f30c0d73",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f30c0d73"
      },
      "outputs": [],
      "source": [
        "# Setup Kaggle credentials\n",
        "# IMPORTANT: You need to manually download kaggle.json FIRST!\n",
        "#\n",
        "# üìù HOW TO GET kaggle.json:\n",
        "# 1. Go to https://www.kaggle.com/settings\n",
        "# 2. Scroll down to \"API\" section\n",
        "# 3. Click \"Create New Token\" button\n",
        "# 4. This will DOWNLOAD a file called \"kaggle.json\" to your computer\n",
        "# 5. Find the downloaded file (usually in your Downloads folder)\n",
        "# 6. Then come back here and upload it when prompted below\n",
        "#\n",
        "# ‚ö†Ô∏è NOTE: If you only see the API key on screen but no download happened,\n",
        "#    click \"Create New Token\" again - it should download the file\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üì§ UPLOAD YOUR kaggle.json FILE\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüìù If you haven't downloaded it yet:\")\n",
        "print(\"   1. Go to: https://www.kaggle.com/settings\")\n",
        "print(\"   2. Scroll to 'API' section\")\n",
        "print(\"   3. Click 'Create New Token' (downloads kaggle.json)\")\n",
        "print(\"   4. Find the file in your Downloads folder\")\n",
        "print(\"   5. Click 'Choose Files' below and select it\")\n",
        "print(\"\\n‚è≥ Waiting for your kaggle.json file...\\n\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Verify the file was uploaded\n",
        "if 'kaggle.json' not in uploaded:\n",
        "    print(\"\\n‚ùå ERROR: kaggle.json was not uploaded!\")\n",
        "    print(\"   Please make sure you selected the correct file.\")\n",
        "    raise FileNotFoundError(\"kaggle.json not found in uploaded files\")\n",
        "\n",
        "# Move kaggle.json to the correct location\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"\\n‚úÖ Kaggle credentials configured successfully!\")\n",
        "print(\"üìÅ File saved to: ~/.kaggle/kaggle.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b862f5c7",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b862f5c7"
      },
      "outputs": [],
      "source": [
        "# Download PlantVillage dataset from Kaggle (SKIP IF ALREADY EXISTS)\n",
        "import opendatasets as od\n",
        "\n",
        "PLANTVILLAGE_URL = 'https://www.kaggle.com/datasets/emmarex/plantdisease'\n",
        "PLANTVILLAGE_PATH = '/content/plantvillage'\n",
        "\n",
        "# Known possible paths after download\n",
        "POSSIBLE_PATHS = [\n",
        "    os.path.join(PLANTVILLAGE_PATH, 'plantdisease', 'PlantVillage'),\n",
        "    os.path.join(PLANTVILLAGE_PATH, 'PlantVillage'),\n",
        "    os.path.join(PLANTVILLAGE_PATH, 'plantdisease', 'plantvillage', 'PlantVillage'),\n",
        "]\n",
        "\n",
        "def find_plantvillage_dataset():\n",
        "    \"\"\"Find PlantVillage dataset if it exists\"\"\"\n",
        "    for path in POSSIBLE_PATHS:\n",
        "        if os.path.exists(path) and os.path.isdir(path):\n",
        "            subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "            if len(subdirs) >= 15:\n",
        "                sample_dir = os.path.join(path, subdirs[0])\n",
        "                sample_files = [f for f in os.listdir(sample_dir)\n",
        "                              if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "                if len(sample_files) > 0:\n",
        "                    return path\n",
        "    return None\n",
        "\n",
        "# Check if dataset already exists\n",
        "existing_path = find_plantvillage_dataset()\n",
        "\n",
        "if existing_path:\n",
        "    print(\"‚úÖ PlantVillage dataset ALREADY EXISTS! Skipping download...\")\n",
        "    print(f\"üìÅ Using cached dataset at: {existing_path}\")\n",
        "    PLANTVILLAGE_PATH = existing_path\n",
        "else:\n",
        "    print(\"üì• Downloading PlantVillage dataset (54,305 images)...\")\n",
        "    print(\"‚è≥ This will take 3-5 minutes (first time only)...\")\n",
        "\n",
        "    od.download(PLANTVILLAGE_URL, data_dir=PLANTVILLAGE_PATH)\n",
        "\n",
        "    print(\"\\nüîç Locating dataset structure...\")\n",
        "\n",
        "    # Find the dataset path\n",
        "    dataset_root = find_plantvillage_dataset()\n",
        "\n",
        "    if not dataset_root:\n",
        "        # Search recursively as fallback\n",
        "        for root, dirs, files in os.walk(PLANTVILLAGE_PATH):\n",
        "            if len(dirs) >= 15:\n",
        "                has_images = False\n",
        "                for d in dirs[:3]:\n",
        "                    dir_path = os.path.join(root, d)\n",
        "                    if os.path.isdir(dir_path):\n",
        "                        dir_files = os.listdir(dir_path)\n",
        "                        if any(f.lower().endswith(('.jpg', '.jpeg', '.png')) for f in dir_files):\n",
        "                            has_images = True\n",
        "                            break\n",
        "                if has_images:\n",
        "                    dataset_root = root\n",
        "                    break\n",
        "\n",
        "    if dataset_root:\n",
        "        PLANTVILLAGE_PATH = dataset_root\n",
        "    else:\n",
        "        raise FileNotFoundError(\"‚ùå PlantVillage dataset not found after download\")\n",
        "\n",
        "# Verify dataset\n",
        "class_dirs = [d for d in os.listdir(PLANTVILLAGE_PATH)\n",
        "              if os.path.isdir(os.path.join(PLANTVILLAGE_PATH, d))]\n",
        "num_classes = len(class_dirs)\n",
        "\n",
        "print(f\"\\n‚úÖ PlantVillage dataset ready!\")\n",
        "print(f\"üìÅ Path: {PLANTVILLAGE_PATH}\")\n",
        "print(f\"üåø Classes: {num_classes}\")\n",
        "\n",
        "# Quick image count\n",
        "total_images = sum(len([f for f in os.listdir(os.path.join(PLANTVILLAGE_PATH, cls))\n",
        "                        if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                   for cls in class_dirs[:5])\n",
        "print(f\"üìä Sample: First 5 classes have {total_images:,} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ac424fc",
      "metadata": {
        "id": "2ac424fc"
      },
      "source": [
        "## üìä Data Exploration & Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3552f787",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3552f787"
      },
      "outputs": [],
      "source": [
        "# Analyze PlantVillage dataset\n",
        "plantvillage_classes = sorted(os.listdir(PLANTVILLAGE_PATH))\n",
        "print(f\"üå± PlantVillage Dataset:\")\n",
        "print(f\"Total classes: {len(plantvillage_classes)}\")\n",
        "print(f\"\\nSample classes:\")\n",
        "for cls in plantvillage_classes[:5]:\n",
        "    class_path = os.path.join(PLANTVILLAGE_PATH, cls)\n",
        "    if os.path.isdir(class_path):\n",
        "        num_images = len(os.listdir(class_path))\n",
        "        print(f\"  - {cls}: {num_images} images\")\n",
        "\n",
        "# Build unified dataset info\n",
        "print(f\"\\nüåæ UNIFIED Nutrient Dataset (ALL {len(CROP_DATASETS)} crops):\")\n",
        "total_classes = 0\n",
        "total_images = 0\n",
        "\n",
        "for crop, folder_name in CROP_DATASETS.items():\n",
        "    crop_path = os.path.join(NUTRIENT_DATASETS_ROOT, folder_name)\n",
        "    if os.path.exists(crop_path):\n",
        "        crop_classes = [d for d in os.listdir(crop_path) if os.path.isdir(os.path.join(crop_path, d))]\n",
        "        crop_images = sum([len([f for f in os.listdir(os.path.join(crop_path, cls))\n",
        "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                          for cls in crop_classes])\n",
        "        total_classes += len(crop_classes)\n",
        "        total_images += crop_images\n",
        "        print(f\"  {crop.upper()}: {len(crop_classes)} classes, {crop_images} images\")\n",
        "\n",
        "print(f\"\\nüìä UNIFIED TOTALS:\")\n",
        "print(f\"  Total classes: {total_classes}\")\n",
        "print(f\"  Total images: {total_images}\")\n",
        "print(f\"  Class format: {{crop}}_{{deficiency}} (e.g., rice_N, wheat_healthy)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7db7afbe",
      "metadata": {
        "id": "7db7afbe"
      },
      "source": [
        "## üî® Create Data Pipelines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22c6a8ab",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "22c6a8ab"
      },
      "outputs": [],
      "source": [
        "# =============================================================\n",
        "# üöÄ OPTIMIZED DATA PIPELINE (Maximum Performance)\n",
        "# =============================================================\n",
        "# Key optimizations:\n",
        "# 1. AUTOTUNE for all parallel operations\n",
        "# 2. cache() after initial load (keeps data in memory)\n",
        "# 3. Proper prefetch with AUTOTUNE\n",
        "# 4. Efficient augmentation pipeline\n",
        "# 5. Consistent normalization\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def create_optimized_dataset(data_dir, img_size, batch_size, validation_split=0.2, subset=None):\n",
        "    \"\"\"\n",
        "    Create dataset with optimal settings for fastest training\n",
        "    \"\"\"\n",
        "    print(f\"üì¶ Loading {subset} data from {os.path.basename(data_dir)}...\")\n",
        "    \n",
        "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "        data_dir,\n",
        "        validation_split=validation_split,\n",
        "        subset=subset,\n",
        "        seed=42,\n",
        "        image_size=(img_size, img_size),\n",
        "        batch_size=batch_size,\n",
        "        label_mode='categorical',\n",
        "        shuffle=True\n",
        "    )\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "@tf.function(jit_compile=True)  # XLA compilation for speedup\n",
        "def augment_image(image, label):\n",
        "    \"\"\"Fast augmentation pipeline with XLA compilation\"\"\"\n",
        "    # Random flip (horizontal only - vertical doesn't make sense for leaves)\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    \n",
        "    # Brightness and contrast (simulates different lighting)\n",
        "    image = tf.image.random_brightness(image, 0.2)\n",
        "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
        "    \n",
        "    # Random saturation (simulates different leaf health)\n",
        "    image = tf.image.random_saturation(image, 0.8, 1.2)\n",
        "    \n",
        "    # Random hue shift (slight color variation)\n",
        "    image = tf.image.random_hue(image, 0.05)\n",
        "    \n",
        "    # Ensure values stay in valid range\n",
        "    image = tf.clip_by_value(image, 0.0, 255.0)\n",
        "    \n",
        "    return image, label\n",
        "\n",
        "@tf.function(jit_compile=True)  # XLA compilation\n",
        "def normalize_for_mobilenet(image, label):\n",
        "    \"\"\"Normalize to MobileNetV2 input range [-1, 1]\"\"\"\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
        "    return image, label\n",
        "\n",
        "def build_optimized_pipeline(dataset, is_training=True, use_cache=True):\n",
        "    \"\"\"\n",
        "    Build high-performance data pipeline\n",
        "    \n",
        "    Architecture:\n",
        "    1. Parallel map operations (AUTOTUNE)\n",
        "    2. Cache after initial load (optional)\n",
        "    3. Augmentation (training only)\n",
        "    4. Normalization\n",
        "    5. Prefetch (AUTOTUNE)\n",
        "    \"\"\"\n",
        "    # Configure threading for max parallelism\n",
        "    options = tf.data.Options()\n",
        "    options.threading.private_threadpool_size = NUM_WORKERS\n",
        "    options.threading.max_intra_op_parallelism = 1\n",
        "    options.deterministic = False  # Allow non-deterministic for speed\n",
        "    dataset = dataset.with_options(options)\n",
        "    \n",
        "    # Cache after initial load (keeps preprocessed data in memory)\n",
        "    # Only use for smaller datasets to avoid OOM\n",
        "    if use_cache and not is_training:  # Cache validation set only\n",
        "        dataset = dataset.cache()\n",
        "    \n",
        "    # Apply augmentation (training only)\n",
        "    if is_training:\n",
        "        dataset = dataset.map(augment_image, num_parallel_calls=AUTOTUNE)\n",
        "    \n",
        "    # Normalize for MobileNetV2\n",
        "    dataset = dataset.map(normalize_for_mobilenet, num_parallel_calls=AUTOTUNE)\n",
        "    \n",
        "    # Prefetch batches for GPU (critical for performance)\n",
        "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# üìä TQDM PROGRESS CALLBACK (Real-time tracking with ETA)\n",
        "# =============================================================\n",
        "class TQDMProgressCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Enhanced callback with progress bars and ETA\"\"\"\n",
        "    \n",
        "    def __init__(self, total_epochs, stage_name=\"Training\"):\n",
        "        super().__init__()\n",
        "        self.total_epochs = total_epochs\n",
        "        self.stage_name = stage_name\n",
        "        self.epoch_pbar = None\n",
        "        self.batch_pbar = None\n",
        "        self.epoch_times = []\n",
        "        self.stage_start_time = None\n",
        "    \n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.stage_start_time = time.time()\n",
        "        print(f\"\\nüöÄ {self.stage_name} Started\")\n",
        "        self.epoch_pbar = tqdm(\n",
        "            total=self.total_epochs,\n",
        "            desc=f\"üìà {self.stage_name}\",\n",
        "            unit=\"epoch\",\n",
        "            position=0,\n",
        "            leave=True,\n",
        "            bar_format='{l_bar}{bar:30}{r_bar}'\n",
        "        )\n",
        "    \n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.epoch_start_time = time.time()\n",
        "        total_batches = self.params.get('steps', 0)\n",
        "        \n",
        "        self.batch_pbar = tqdm(\n",
        "            total=total_batches,\n",
        "            desc=f\"  Epoch {epoch+1}/{self.total_epochs}\",\n",
        "            unit=\"batch\",\n",
        "            position=1,\n",
        "            leave=False,\n",
        "            bar_format='{l_bar}{bar:25}{r_bar}'\n",
        "        )\n",
        "    \n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        if self.batch_pbar:\n",
        "            self.batch_pbar.update(1)\n",
        "            self.batch_pbar.set_postfix({\n",
        "                'loss': f\"{logs.get('loss', 0):.4f}\",\n",
        "                'acc': f\"{logs.get('accuracy', 0):.3f}\"\n",
        "            })\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.batch_pbar:\n",
        "            self.batch_pbar.close()\n",
        "        \n",
        "        epoch_time = time.time() - self.epoch_start_time\n",
        "        self.epoch_times.append(epoch_time)\n",
        "        \n",
        "        # Calculate ETA\n",
        "        avg_time = np.mean(self.epoch_times)\n",
        "        remaining = self.total_epochs - (epoch + 1)\n",
        "        eta_seconds = remaining * avg_time\n",
        "        eta = str(timedelta(seconds=int(eta_seconds)))\n",
        "        \n",
        "        # Update progress\n",
        "        self.epoch_pbar.update(1)\n",
        "        self.epoch_pbar.set_postfix({\n",
        "            'val_acc': f\"{logs.get('val_accuracy', 0):.3f}\",\n",
        "            'val_loss': f\"{logs.get('val_loss', 0):.4f}\",\n",
        "            'ETA': eta\n",
        "        })\n",
        "        \n",
        "        print(f\"\\n   ‚úÖ Epoch {epoch+1}: val_acc={logs.get('val_accuracy', 0):.4f}, \"\n",
        "              f\"val_loss={logs.get('val_loss', 0):.4f}, time={epoch_time:.1f}s\")\n",
        "    \n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.epoch_pbar:\n",
        "            self.epoch_pbar.close()\n",
        "        total_time = time.time() - self.stage_start_time\n",
        "        print(f\"\\n‚úÖ {self.stage_name} Complete in {str(timedelta(seconds=int(total_time)))}\")\n",
        "        print(f\"   Avg epoch: {np.mean(self.epoch_times):.1f}s\")\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# üì¶ CREATE PLANTVILLAGE DATASETS\n",
        "# =============================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üì¶ CREATING PLANTVILLAGE DATASETS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "train_plantvillage_raw = create_optimized_dataset(\n",
        "    PLANTVILLAGE_PATH, IMG_SIZE, BATCH_SIZE,\n",
        "    validation_split=0.2, subset='training'\n",
        ")\n",
        "val_plantvillage_raw = create_optimized_dataset(\n",
        "    PLANTVILLAGE_PATH, IMG_SIZE, BATCH_SIZE,\n",
        "    validation_split=0.2, subset='validation'\n",
        ")\n",
        "\n",
        "# Apply optimized pipeline\n",
        "print(\"üîß Building optimized pipelines...\")\n",
        "train_plantvillage = build_optimized_pipeline(train_plantvillage_raw, is_training=True, use_cache=False)\n",
        "val_plantvillage = build_optimized_pipeline(val_plantvillage_raw, is_training=False, use_cache=True)\n",
        "\n",
        "# Get dataset info\n",
        "train_batches = tf.data.experimental.cardinality(train_plantvillage_raw).numpy()\n",
        "val_batches = tf.data.experimental.cardinality(val_plantvillage_raw).numpy()\n",
        "num_classes = len(train_plantvillage_raw.class_names)\n",
        "\n",
        "print(f\"\\n‚úÖ PlantVillage Datasets Ready\")\n",
        "print(f\"   Classes: {num_classes}\")\n",
        "print(f\"   Training: {train_batches} batches √ó {BATCH_SIZE} = ~{train_batches * BATCH_SIZE:,} images\")\n",
        "print(f\"   Validation: {val_batches} batches √ó {BATCH_SIZE} = ~{val_batches * BATCH_SIZE:,} images\")\n",
        "print(f\"   ‚ö° Optimizations: AUTOTUNE, XLA, {NUM_WORKERS} workers, cache (val)\")\n",
        "print(f\"   üé® Augmentations: flip, brightness, contrast, saturation, hue\")\n",
        "print(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91c8d714",
      "metadata": {
        "id": "91c8d714"
      },
      "source": [
        "## ‚úÖ Pre-Training Validation\n",
        "\n",
        "Run this cell to verify everything is set up correctly before training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5e4b9de",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c5e4b9de"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ PRE-TRAINING VALIDATION (T4 Optimized)\n",
        "print(\"=\" * 60)\n",
        "print(\"üîç PRE-TRAINING VALIDATION\")\n",
        "print(f\"‚è±Ô∏è Session time: {get_session_time()}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "errors = []\n",
        "\n",
        "# 1. GPU Check\n",
        "print(\"\\n1Ô∏è‚É£ GPU Check...\")\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    gpu_name = gpus[0].name\n",
        "    print(f\"   ‚úÖ GPU: {gpu_name}\")\n",
        "    # Check if T4\n",
        "    try:\n",
        "        !nvidia-smi --query-gpu=name --format=csv,noheader\n",
        "    except:\n",
        "        pass\n",
        "else:\n",
        "    errors.append(\"No GPU detected!\")\n",
        "    print(\"   ‚ö†Ô∏è No GPU - training will be MUCH slower\")\n",
        "\n",
        "# 2. Quick data test\n",
        "print(\"\\n2Ô∏è‚É£ Data Pipeline Check...\")\n",
        "try:\n",
        "    import time\n",
        "    start = time.time()\n",
        "    for batch_images, batch_labels in train_plantvillage.take(1):\n",
        "        load_time = time.time() - start\n",
        "        print(f\"   ‚úÖ Batch shape: {batch_images.shape}\")\n",
        "        print(f\"   ‚úÖ Labels shape: {batch_labels.shape}\")\n",
        "        print(f\"   ‚úÖ Image dtype: {batch_images.dtype}\")\n",
        "        print(f\"   ‚ö° First batch load: {load_time:.2f}s\")\n",
        "        break\n",
        "except Exception as e:\n",
        "    errors.append(f\"Data pipeline error: {e}\")\n",
        "    print(f\"   ‚ùå Error: {e}\")\n",
        "\n",
        "# 3. Memory check\n",
        "print(\"\\n3Ô∏è‚É£ Memory Status...\")\n",
        "try:\n",
        "    import psutil\n",
        "    ram_total = psutil.virtual_memory().total / (1024**3)\n",
        "    ram_avail = psutil.virtual_memory().available / (1024**3)\n",
        "    print(f\"   ‚úÖ RAM: {ram_avail:.1f}GB available / {ram_total:.1f}GB total\")\n",
        "    if ram_avail < 3:\n",
        "        print(\"   ‚ö†Ô∏è Low RAM - data copied to local SSD helps prevent crashes\")\n",
        "except:\n",
        "    print(\"   ‚ÑπÔ∏è Could not check RAM\")\n",
        "\n",
        "# 4. GPU Memory check\n",
        "print(\"\\n4Ô∏è‚É£ GPU Memory Check...\")\n",
        "try:\n",
        "    !nvidia-smi --query-gpu=memory.total,memory.free --format=csv,noheader\n",
        "except:\n",
        "    print(\"   ‚ÑπÔ∏è Could not query GPU memory\")\n",
        "\n",
        "# 5. Existing checkpoints\n",
        "print(\"\\n5Ô∏è‚É£ Checkpoint Status...\")\n",
        "stage2_exists = os.path.exists(os.path.join(DRIVE_CHECKPOINT_DIR, 'stage2_plantvillage_best.keras'))\n",
        "stage3_exists = os.path.exists(os.path.join(DRIVE_CHECKPOINT_DIR, 'unified_nutrient_best.keras'))\n",
        "print(f\"   Stage 2 checkpoint: {'‚úÖ Found (will resume)' if stage2_exists else '‚ùå None (fresh start)'}\")\n",
        "print(f\"   Stage 3 checkpoint: {'‚úÖ Found (will resume)' if stage3_exists else '‚ùå None (fresh start)'}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "if errors:\n",
        "    print(\"‚ùå ISSUES FOUND:\")\n",
        "    for e in errors:\n",
        "        print(f\"   ‚Ä¢ {e}\")\n",
        "else:\n",
        "    print(\"‚úÖ ALL CHECKS PASSED!\")\n",
        "    print(f\"\\nüöÄ T4 GPU OPTIMIZED SETTINGS:\")\n",
        "    print(f\"   ‚Ä¢ Batch size: {BATCH_SIZE} (fills 16GB VRAM)\")\n",
        "    print(f\"   ‚Ä¢ Mixed precision: FP16\")\n",
        "    print(f\"   ‚Ä¢ JIT/XLA compilation: Enabled\")\n",
        "    print(f\"   ‚Ä¢ AUTOTUNE prefetch: Enabled\")\n",
        "    print(f\"   ‚Ä¢ Data location: Local SSD (fast I/O)\")\n",
        "    print(f\"\\n‚ö° Expected: ~1-2 min/epoch (10x faster than Drive I/O)\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "509a853f",
      "metadata": {
        "id": "509a853f"
      },
      "source": [
        "## üèóÔ∏è Stage 2: Build Model with MobileNetV2 Base\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18ed2908",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "18ed2908"
      },
      "outputs": [],
      "source": [
        "# =============================================================\n",
        "# üèóÔ∏è OPTIMIZED MODEL ARCHITECTURE\n",
        "# =============================================================\n",
        "def create_model(num_classes, input_shape=(224, 224, 3), freeze_base=True):\n",
        "    \"\"\"\n",
        "    Create MobileNetV2-based model optimized for fast training\n",
        "    \n",
        "    Architecture:\n",
        "    - MobileNetV2 base (pretrained on ImageNet)\n",
        "    - GlobalAveragePooling2D (reduce parameters)\n",
        "    - Dense(256) with L2 regularization\n",
        "    - BatchNormalization\n",
        "    - Dropout layers\n",
        "    - Dense(num_classes) with softmax\n",
        "    \"\"\"\n",
        "    \n",
        "    # Load MobileNetV2 with ImageNet weights\n",
        "    base_model = tf.keras.applications.MobileNetV2(\n",
        "        input_shape=input_shape,\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        pooling=None  # We'll add our own pooling\n",
        "    )\n",
        "    \n",
        "    base_model.trainable = not freeze_base\n",
        "    \n",
        "    # Build classification head\n",
        "    model = tf.keras.Sequential([\n",
        "        base_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dropout(DROPOUT_RATE),\n",
        "        tf.keras.layers.Dense(\n",
        "            256, \n",
        "            activation='relu',\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
        "        ),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(DROPOUT_RATE * 0.8),\n",
        "        # Float32 output for numerical stability\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax', dtype='float32')\n",
        "    ])\n",
        "    \n",
        "    return model, base_model\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# üéØ STAGE 2: CREATE PLANTVILLAGE MODEL\n",
        "# =============================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üèóÔ∏è BUILDING STAGE 2 MODEL (PlantVillage)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get number of PlantVillage classes\n",
        "num_plantvillage_classes = len(train_plantvillage_raw.class_names)\n",
        "print(f\"   Classes: {num_plantvillage_classes}\")\n",
        "\n",
        "# Check for existing checkpoint\n",
        "STAGE2_CHECKPOINT = os.path.join(DRIVE_CHECKPOINT_DIR, 'stage2_plantvillage_best.keras')\n",
        "STAGE2_LOCAL = os.path.join(OUTPUT_DIR, 'stage2_plantvillage_best.keras')\n",
        "\n",
        "model_stage2 = None\n",
        "resume_stage2 = False\n",
        "\n",
        "# Try to load existing checkpoint\n",
        "for checkpoint_path in [STAGE2_CHECKPOINT, STAGE2_LOCAL]:\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            print(f\"\\nüîÑ Found existing Stage 2 checkpoint!\")\n",
        "            print(f\"   Loading from: {os.path.basename(checkpoint_path)}\")\n",
        "            model_stage2 = tf.keras.models.load_model(checkpoint_path)\n",
        "            \n",
        "            # Verify correct output shape\n",
        "            if model_stage2.output_shape[-1] == num_plantvillage_classes:\n",
        "                resume_stage2 = True\n",
        "                print(f\"‚úÖ Checkpoint valid ({num_plantvillage_classes} classes)\")\n",
        "                \n",
        "                # Evaluate current performance\n",
        "                print(\"üìä Evaluating checkpoint...\")\n",
        "                results = model_stage2.evaluate(val_plantvillage, verbose=0)\n",
        "                print(f\"   Current - Loss: {results[0]:.4f}, Accuracy: {results[1]:.4f}\")\n",
        "                \n",
        "                if results[1] >= 0.85:\n",
        "                    print(\"‚úÖ Checkpoint performance is good - will use for Stage 3\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Class mismatch - creating new model\")\n",
        "                model_stage2 = None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not load: {e}\")\n",
        "            model_stage2 = None\n",
        "\n",
        "# Create new model if needed\n",
        "if model_stage2 is None:\n",
        "    print(f\"\\nüèóÔ∏è Creating NEW Stage 2 model...\")\n",
        "    model_stage2, base_model_stage2 = create_model(\n",
        "        num_classes=num_plantvillage_classes,\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "        freeze_base=True\n",
        "    )\n",
        "    \n",
        "    # Compile with optimizer\n",
        "    model_stage2.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_STAGE2),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top3_acc')],\n",
        "        jit_compile=True  # XLA compilation for speedup\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Model created\")\n",
        "\n",
        "# Show model summary\n",
        "trainable_params = sum([tf.keras.backend.count_params(w) for w in model_stage2.trainable_weights])\n",
        "total_params = sum([tf.keras.backend.count_params(w) for w in model_stage2.weights])\n",
        "\n",
        "print(f\"\\nüìä Model Architecture:\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Frozen parameters: {total_params - trainable_params:,}\")\n",
        "print(f\"   Dropout rate: {DROPOUT_RATE}\")\n",
        "print(f\"   Base model: MobileNetV2 (ImageNet)\")\n",
        "print(f\"   Output classes: {num_plantvillage_classes}\")\n",
        "print(f\"   Precision: float32\")\n",
        "print(f\"   XLA/JIT: Enabled\")\n",
        "print(f\"\\nüîÑ Resume training: {'Yes' if resume_stage2 else 'No (fresh start)'}\")\n",
        "print(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ffa4eee",
      "metadata": {
        "id": "1ffa4eee"
      },
      "source": [
        "## üéØ Stage 2: Train on PlantVillage Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db0dc4ac",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "db0dc4ac"
      },
      "outputs": [],
      "source": [
        "print(\"üöÄ Starting Stage 2: PlantVillage Fine-tuning\")\n",
        "print(f\"‚è±Ô∏è Epochs: {PLANTVILLAGE_EPOCHS} | LR: {LEARNING_RATE_STAGE2} | Batch: {BATCH_SIZE}\")\n",
        "if resume_stage2:\n",
        "    print(\"üîÑ RESUMING from previous checkpoint\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Callbacks with TQDM progress\n",
        "callbacks_stage2 = [\n",
        "    # TQDM Progress with ETA\n",
        "    TQDMProgressCallback(PLANTVILLAGE_EPOCHS, stage_name=\"Stage 2: PlantVillage\"),\n",
        "\n",
        "    # Early stopping\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=3,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        min_delta=0.005\n",
        "    ),\n",
        "\n",
        "    # Reduce LR on plateau\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.3,\n",
        "        patience=2,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Save best to local (fast)\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        os.path.join(OUTPUT_DIR, 'stage2_plantvillage_best.keras'),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=0\n",
        "    ),\n",
        "\n",
        "    # Save best to Drive (persistent)\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        os.path.join(DRIVE_CHECKPOINT_DIR, 'stage2_plantvillage_best.keras'),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=0\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train with progress tracking\n",
        "TRAINING_START_TIME = datetime.now()\n",
        "\n",
        "history_stage2 = model_stage2.fit(\n",
        "    train_plantvillage,\n",
        "    validation_data=val_plantvillage,\n",
        "    epochs=PLANTVILLAGE_EPOCHS,\n",
        "    callbacks=callbacks_stage2,\n",
        "    verbose=0  # Disable default output, use TQDM instead\n",
        ")\n",
        "\n",
        "# Calculate training stats\n",
        "best_val_acc = max(history_stage2.history['val_accuracy'])\n",
        "best_val_loss = min(history_stage2.history['val_loss'])\n",
        "final_train_acc = history_stage2.history['accuracy'][-1]\n",
        "training_time = datetime.now() - TRAINING_START_TIME\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(f\"‚úÖ Stage 2 completed in {training_time}\")\n",
        "print(f\"üìà Best val accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"üìâ Best val loss: {best_val_loss:.4f}\")\n",
        "print(f\"üìä Final train accuracy: {final_train_acc:.4f}\")\n",
        "\n",
        "# Check for overfitting/underfitting\n",
        "gap = final_train_acc - best_val_acc\n",
        "if gap > 0.15:\n",
        "    print(f\"‚ö†Ô∏è Overfitting detected (train-val gap: {gap:.2%})\")\n",
        "elif best_val_acc < 0.7:\n",
        "    print(f\"‚ö†Ô∏è Possible underfitting (val_acc: {best_val_acc:.2%})\")\n",
        "else:\n",
        "    print(f\"‚úÖ Good generalization (train-val gap: {gap:.2%})\")\n",
        "\n",
        "print(f\"\\nüíæ Checkpoints saved to:\")\n",
        "print(f\"   Local: {OUTPUT_DIR}\")\n",
        "print(f\"   Drive: {DRIVE_CHECKPOINT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e0802c5",
      "metadata": {
        "id": "1e0802c5"
      },
      "source": [
        "## üìà Stage 2 Results Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a687e2ad",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a687e2ad"
      },
      "outputs": [],
      "source": [
        "# Quick training visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(history_stage2.history['accuracy'], 'b-', label='Train')\n",
        "axes[0].plot(history_stage2.history['val_accuracy'], 'r-', label='Val')\n",
        "axes[0].set_title('Stage 2: Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(history_stage2.history['loss'], 'b-', label='Train')\n",
        "axes[1].plot(history_stage2.history['val_loss'], 'r-', label='Val')\n",
        "axes[1].set_title('Stage 2: Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'stage2_training_history.png'), dpi=150)\n",
        "plt.savefig(os.path.join(DRIVE_CHECKPOINT_DIR, 'stage2_training_history.png'), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Memory cleanup (preserve model references)\n",
        "import gc\n",
        "gc.collect()\n",
        "print(\"üßπ Memory cleaned (models preserved for Stage 3)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9651b0c7",
      "metadata": {
        "id": "9651b0c7"
      },
      "source": [
        "## üîÑ Stage 3: Build UNIFIED Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3160d02",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f3160d02"
      },
      "outputs": [],
      "source": [
        "# =============================================================\n",
        "# üîÑ STAGE 3: BUILD UNIFIED NUTRIENT DATASET\n",
        "# =============================================================\n",
        "# Combine all crop datasets into one unified structure:\n",
        "# crop_deficiency format (e.g., rice_N, wheat_P, tomato_healthy)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üèóÔ∏è BUILDING UNIFIED NUTRIENT DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "UNIFIED_DATASET_PATH = '/content/unified_nutrient_dataset'\n",
        "\n",
        "# =============================================================\n",
        "# üîß SMART FOLDER DETECTION (Handles multiple structures)\n",
        "# =============================================================\n",
        "def detect_nutrient_classes(crop_path):\n",
        "    \"\"\"\n",
        "    Intelligently detect nutrient classes from various folder structures:\n",
        "    \n",
        "    Structure 1 (Flat):\n",
        "        crop_folder/N/, crop_folder/P/, crop_folder/K/\n",
        "        \n",
        "    Structure 2 (Split):\n",
        "        crop_folder/train/N/, crop_folder/test/N/, crop_folder/val/N/\n",
        "        \n",
        "    Returns: {class_name: [list_of_source_folders]}\n",
        "    \"\"\"\n",
        "    nutrient_classes = {}\n",
        "    \n",
        "    if not os.path.exists(crop_path):\n",
        "        return nutrient_classes\n",
        "    \n",
        "    subfolders = [d for d in os.listdir(crop_path) \n",
        "                  if os.path.isdir(os.path.join(crop_path, d))]\n",
        "    \n",
        "    # Detect if using train/test/val splits\n",
        "    split_keywords = {'train', 'test', 'val', 'validation'}\n",
        "    has_splits = any(f.lower() in split_keywords for f in subfolders)\n",
        "    \n",
        "    if has_splits:\n",
        "        # Dataset has train/test/val structure\n",
        "        for split_folder in subfolders:\n",
        "            if split_folder.lower() in split_keywords:\n",
        "                split_path = os.path.join(crop_path, split_folder)\n",
        "                \n",
        "                for class_name in os.listdir(split_path):\n",
        "                    class_path = os.path.join(split_path, class_name)\n",
        "                    \n",
        "                    if not os.path.isdir(class_path):\n",
        "                        continue\n",
        "                    \n",
        "                    # Check if folder contains images\n",
        "                    files = os.listdir(class_path)[:20]  # Sample first 20\n",
        "                    has_images = any(f.lower().endswith(('.jpg', '.jpeg', '.png')) for f in files)\n",
        "                    \n",
        "                    if has_images:\n",
        "                        if class_name not in nutrient_classes:\n",
        "                            nutrient_classes[class_name] = []\n",
        "                        nutrient_classes[class_name].append(class_path)\n",
        "    else:\n",
        "        # Flat structure - subfolders are the classes\n",
        "        for class_name in subfolders:\n",
        "            class_path = os.path.join(crop_path, class_name)\n",
        "            \n",
        "            # Verify it contains images\n",
        "            files = os.listdir(class_path)[:20]\n",
        "            has_images = any(f.lower().endswith(('.jpg', '.jpeg', '.png')) for f in files)\n",
        "            \n",
        "            if has_images:\n",
        "                nutrient_classes[class_name] = [class_path]\n",
        "    \n",
        "    return nutrient_classes\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# üöÄ BUILD UNIFIED DATASET (With validation)\n",
        "# =============================================================\n",
        "# Check if already built\n",
        "if os.path.exists(UNIFIED_DATASET_PATH):\n",
        "    existing_classes = [d for d in os.listdir(UNIFIED_DATASET_PATH)\n",
        "                        if os.path.isdir(os.path.join(UNIFIED_DATASET_PATH, d))]\n",
        "    \n",
        "    # Validate class names (should be crop_deficiency format)\n",
        "    valid_classes = [c for c in existing_classes if '_' in c and len(c.split('_')) == 2]\n",
        "    \n",
        "    if len(valid_classes) >= len(CROP_DATASETS) * 2:  # At least 2 classes per crop\n",
        "        print(f\"‚úÖ Unified dataset already exists with {len(valid_classes)} classes\")\n",
        "        print(f\"   Classes: {', '.join(sorted(valid_classes)[:10])}{'...' if len(valid_classes) > 10 else ''}\")\n",
        "        unified_classes = valid_classes\n",
        "        needs_rebuild = False\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Existing dataset incomplete ({len(valid_classes)} classes) - rebuilding...\")\n",
        "        shutil.rmtree(UNIFIED_DATASET_PATH)\n",
        "        needs_rebuild = True\n",
        "else:\n",
        "    needs_rebuild = True\n",
        "\n",
        "# Build if needed\n",
        "if needs_rebuild:\n",
        "    os.makedirs(UNIFIED_DATASET_PATH, exist_ok=True)\n",
        "    unified_classes = []\n",
        "    skipped_crops = []\n",
        "    \n",
        "    print(\"üìÇ Combining crop datasets into unified structure...\\n\")\n",
        "    \n",
        "    for crop, folder_name in CROP_DATASETS.items():\n",
        "        crop_path = os.path.join(NUTRIENT_DATASETS_ROOT, folder_name)\n",
        "        \n",
        "        if not os.path.exists(crop_path):\n",
        "            print(f\"   ‚ö†Ô∏è {crop.upper()}: Not found\")\n",
        "            skipped_crops.append(crop)\n",
        "            continue\n",
        "        \n",
        "        print(f\"   üåæ {crop.upper()}: Processing...\")\n",
        "        \n",
        "        try:\n",
        "            # Detect nutrient classes\n",
        "            nutrient_classes = detect_nutrient_classes(crop_path)\n",
        "            \n",
        "            if not nutrient_classes:\n",
        "                print(f\"      ‚ö†Ô∏è No valid classes found\")\n",
        "                skipped_crops.append(crop)\n",
        "                continue\n",
        "            \n",
        "            # Process each nutrient deficiency class\n",
        "            for class_name, source_paths in nutrient_classes.items():\n",
        "                # Clean class name (remove crop prefix if exists)\n",
        "                clean_name = class_name.replace(f\"{crop}_\", \"\").replace(f\"{crop}__\", \"\")\n",
        "                unified_class = f\"{crop}_{clean_name}\"\n",
        "                \n",
        "                # Create destination folder\n",
        "                dst_dir = os.path.join(UNIFIED_DATASET_PATH, unified_class)\n",
        "                os.makedirs(dst_dir, exist_ok=True)\n",
        "                \n",
        "                # Copy images from all source paths (train + test + val)\n",
        "                total_copied = 0\n",
        "                for src_dir in source_paths:\n",
        "                    src_name = os.path.basename(os.path.dirname(src_dir))  # train/test/val\n",
        "                    \n",
        "                    for img_file in os.listdir(src_dir):\n",
        "                        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                            src_file = os.path.join(src_dir, img_file)\n",
        "                            # Prevent filename collisions with prefix\n",
        "                            dst_file = os.path.join(dst_dir, f\"{src_name}_{img_file}\")\n",
        "                            \n",
        "                            if not os.path.exists(dst_file):\n",
        "                                shutil.copy2(src_file, dst_file)\n",
        "                                total_copied += 1\n",
        "                \n",
        "                if total_copied > 0 and unified_class not in unified_classes:\n",
        "                    unified_classes.append(unified_class)\n",
        "            \n",
        "            # Show crop summary\n",
        "            crop_classes = [c for c in unified_classes if c.startswith(f\"{crop}_\")]\n",
        "            class_names_short = [c.replace(f'{crop}_', '') for c in crop_classes]\n",
        "            print(f\"      ‚úÖ {len(crop_classes)} classes: {', '.join(class_names_short)}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"      ‚ùå Error: {e}\")\n",
        "            skipped_crops.append(crop)\n",
        "    \n",
        "    if skipped_crops:\n",
        "        print(f\"\\n‚ö†Ô∏è Skipped crops: {', '.join(skipped_crops)}\")\n",
        "\n",
        "# Validate final dataset\n",
        "if len(unified_classes) == 0:\n",
        "    # Try reading from existing\n",
        "    unified_classes = [d for d in os.listdir(UNIFIED_DATASET_PATH)\n",
        "                       if os.path.isdir(os.path.join(UNIFIED_DATASET_PATH, d)) and '_' in d]\n",
        "\n",
        "if len(unified_classes) == 0:\n",
        "    raise RuntimeError(\"‚ùå No valid classes found! Check dataset structure.\")\n",
        "\n",
        "# Sort and validate\n",
        "class_names = sorted(unified_classes)\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Final validation - no train/test/val in class names\n",
        "bad_classes = [c for c in class_names if any(x in c.lower() for x in ['_train', '_test', '_val'])]\n",
        "if bad_classes:\n",
        "    raise RuntimeError(f\"‚ùå Invalid class names detected: {bad_classes[:3]}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"‚úÖ UNIFIED DATASET READY\")\n",
        "print(f\"   Total classes: {num_classes}\")\n",
        "print(f\"   Format: crop_deficiency (e.g., rice_N, wheat_healthy)\")\n",
        "print(f\"   Classes: {', '.join(class_names[:8])}{'...' if num_classes > 8 else ''}\")\n",
        "print(f\"   Location: {UNIFIED_DATASET_PATH} (LOCAL SSD)\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# =============================================================\n",
        "# üì¶ CREATE OPTIMIZED UNIFIED DATASETS\n",
        "# =============================================================\n",
        "print(\"üì¶ Creating unified nutrient datasets...\")\n",
        "\n",
        "train_nutrient_raw = create_optimized_dataset(\n",
        "    UNIFIED_DATASET_PATH, IMG_SIZE, BATCH_SIZE,\n",
        "    validation_split=0.2, subset='training'\n",
        ")\n",
        "val_nutrient_raw = create_optimized_dataset(\n",
        "    UNIFIED_DATASET_PATH, IMG_SIZE, BATCH_SIZE,\n",
        "    validation_split=0.2, subset='validation'\n",
        ")\n",
        "\n",
        "# Apply optimized pipeline (no cache for larger dataset)\n",
        "print(\"üîß Building optimized pipelines...\")\n",
        "train_nutrient = build_optimized_pipeline(train_nutrient_raw, is_training=True, use_cache=False)\n",
        "val_nutrient = build_optimized_pipeline(val_nutrient_raw, is_training=False, use_cache=False)\n",
        "\n",
        "train_batches = tf.data.experimental.cardinality(train_nutrient_raw).numpy()\n",
        "val_batches = tf.data.experimental.cardinality(val_nutrient_raw).numpy()\n",
        "\n",
        "print(f\"\\n‚úÖ Unified Nutrient Datasets Ready\")\n",
        "print(f\"   Classes: {num_classes}\")\n",
        "print(f\"   Training: {train_batches} batches √ó {BATCH_SIZE} = ~{train_batches * BATCH_SIZE:,} images\")\n",
        "print(f\"   Validation: {val_batches} batches √ó {BATCH_SIZE} = ~{val_batches * BATCH_SIZE:,} images\")\n",
        "print(f\"   ‚ö° Optimizations: AUTOTUNE, XLA, {NUM_WORKERS} workers\")\n",
        "print(f\"   üé® Augmentations: flip, brightness, contrast, saturation, hue\")\n",
        "print(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc94fe04",
      "metadata": {},
      "source": [
        "## üìä Dataset Verification & Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "008bf8a4",
      "metadata": {},
      "source": [
        "## ‚öñÔ∏è Balance Dataset (Fix Model Bias)\n",
        "\n",
        "**Problem:** If some classes have many more images than others (e.g., maize has 1000 images while wheat has 200), the model becomes biased towards the majority class.\n",
        "\n",
        "**Solution:** Balance classes by either:\n",
        "- **Undersampling:** Reduce majority classes to match minority\n",
        "- **Oversampling:** Duplicate/augment minority classes to match majority\n",
        "\n",
        "We'll use a hybrid approach for optimal results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb01d584",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================\n",
        "# ‚öñÔ∏è BALANCE DATASET - FIX MODEL BIAS\n",
        "# =============================================================\n",
        "# Ensures equal representation of all classes to prevent bias\n",
        "\n",
        "import random\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "from collections import Counter\n",
        "\n",
        "def augment_image_pil(img_path, save_path, augmentation_idx):\n",
        "    \"\"\"\n",
        "    Create augmented version of image using PIL\n",
        "    Lighter augmentation than training pipeline (preserves image identity)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img = Image.open(img_path)\n",
        "        \n",
        "        # Different augmentation based on index\n",
        "        if augmentation_idx % 5 == 0:\n",
        "            # Horizontal flip\n",
        "            img = ImageOps.mirror(img)\n",
        "        elif augmentation_idx % 5 == 1:\n",
        "            # Brightness adjustment\n",
        "            enhancer = ImageEnhance.Brightness(img)\n",
        "            img = enhancer.enhance(random.uniform(0.85, 1.15))\n",
        "        elif augmentation_idx % 5 == 2:\n",
        "            # Contrast adjustment\n",
        "            enhancer = ImageEnhance.Contrast(img)\n",
        "            img = enhancer.enhance(random.uniform(0.85, 1.15))\n",
        "        elif augmentation_idx % 5 == 3:\n",
        "            # Color adjustment\n",
        "            enhancer = ImageEnhance.Color(img)\n",
        "            img = enhancer.enhance(random.uniform(0.9, 1.1))\n",
        "        else:\n",
        "            # Sharpness adjustment\n",
        "            enhancer = ImageEnhance.Sharpness(img)\n",
        "            img = enhancer.enhance(random.uniform(0.9, 1.1))\n",
        "        \n",
        "        img.save(save_path, quality=95)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"      ‚ö†Ô∏è Augmentation failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def balance_dataset(dataset_path, target_images_per_class=None, strategy='hybrid'):\n",
        "    \"\"\"\n",
        "    Balance dataset to prevent model bias\n",
        "    \n",
        "    Args:\n",
        "        dataset_path: Path to unified dataset\n",
        "        target_images_per_class: Target number of images per class (None = use median)\n",
        "        strategy: 'undersample', 'oversample', or 'hybrid'\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with balancing statistics\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚öñÔ∏è BALANCING DATASET TO PREVENT BIAS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Count images per class\n",
        "    class_counts = {}\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "        \n",
        "        images = [f for f in os.listdir(class_path) \n",
        "                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        class_counts[class_name] = len(images)\n",
        "    \n",
        "    if not class_counts:\n",
        "        print(\"‚ùå No classes found!\")\n",
        "        return {}\n",
        "    \n",
        "    # Calculate statistics\n",
        "    min_count = min(class_counts.values())\n",
        "    max_count = max(class_counts.values())\n",
        "    median_count = sorted(class_counts.values())[len(class_counts) // 2]\n",
        "    mean_count = sum(class_counts.values()) // len(class_counts)\n",
        "    \n",
        "    print(f\"\\nüìä Current Distribution:\")\n",
        "    print(f\"   Min: {min_count} images\")\n",
        "    print(f\"   Max: {max_count} images\")\n",
        "    print(f\"   Median: {median_count} images\")\n",
        "    print(f\"   Mean: {mean_count} images\")\n",
        "    print(f\"   Imbalance ratio: {min_count/max_count:.2f}\")\n",
        "    \n",
        "    # Determine target\n",
        "    if target_images_per_class is None:\n",
        "        if strategy == 'undersample':\n",
        "            target = min_count\n",
        "        elif strategy == 'oversample':\n",
        "            target = max_count\n",
        "        else:  # hybrid\n",
        "            # Use median or mean, whichever is more balanced\n",
        "            target = median_count if median_count > mean_count * 0.8 else mean_count\n",
        "    else:\n",
        "        target = target_images_per_class\n",
        "    \n",
        "    print(f\"\\nüéØ Target: {target} images per class\")\n",
        "    print(f\"   Strategy: {strategy}\")\n",
        "    \n",
        "    # Balance each class\n",
        "    stats = {\n",
        "        'original': {},\n",
        "        'final': {},\n",
        "        'undersampled': [],\n",
        "        'oversampled': [],\n",
        "        'augmented_images': 0\n",
        "    }\n",
        "    \n",
        "    for class_name, current_count in class_counts.items():\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        images = [f for f in os.listdir(class_path) \n",
        "                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        \n",
        "        stats['original'][class_name] = current_count\n",
        "        \n",
        "        if current_count > target:\n",
        "            # Undersample: randomly remove excess images\n",
        "            excess = current_count - target\n",
        "            images_to_remove = random.sample(images, excess)\n",
        "            \n",
        "            for img_file in images_to_remove:\n",
        "                os.remove(os.path.join(class_path, img_file))\n",
        "            \n",
        "            stats['final'][class_name] = target\n",
        "            stats['undersampled'].append(class_name)\n",
        "            \n",
        "        elif current_count < target:\n",
        "            # Oversample: augment existing images\n",
        "            deficit = target - current_count\n",
        "            \n",
        "            # Calculate how many augmentations per image\n",
        "            augmentations_needed = deficit\n",
        "            augmentation_idx = 0\n",
        "            \n",
        "            while augmentations_needed > 0:\n",
        "                # Pick a random source image\n",
        "                source_img = random.choice(images)\n",
        "                source_path = os.path.join(class_path, source_img)\n",
        "                \n",
        "                # Create augmented filename\n",
        "                base_name = os.path.splitext(source_img)[0]\n",
        "                ext = os.path.splitext(source_img)[1]\n",
        "                aug_name = f\"{base_name}_aug{augmentation_idx}{ext}\"\n",
        "                aug_path = os.path.join(class_path, aug_name)\n",
        "                \n",
        "                # Skip if already exists\n",
        "                if os.path.exists(aug_path):\n",
        "                    augmentation_idx += 1\n",
        "                    continue\n",
        "                \n",
        "                # Augment and save\n",
        "                if augment_image_pil(source_path, aug_path, augmentation_idx):\n",
        "                    augmentations_needed -= 1\n",
        "                    stats['augmented_images'] += 1\n",
        "                \n",
        "                augmentation_idx += 1\n",
        "            \n",
        "            stats['final'][class_name] = target\n",
        "            stats['oversampled'].append(class_name)\n",
        "        else:\n",
        "            # Already balanced\n",
        "            stats['final'][class_name] = current_count\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\n‚úÖ Balancing Complete!\")\n",
        "    print(f\"   Undersampled: {len(stats['undersampled'])} classes\")\n",
        "    print(f\"   Oversampled: {len(stats['oversampled'])} classes\")\n",
        "    print(f\"   Augmented images created: {stats['augmented_images']}\")\n",
        "    \n",
        "    # Verify balance\n",
        "    final_counts = stats['final'].values()\n",
        "    if len(set(final_counts)) == 1:\n",
        "        print(f\"\\n‚úÖ Perfect balance achieved!\")\n",
        "        print(f\"   All classes now have {target} images\")\n",
        "    else:\n",
        "        print(f\"\\nüìä Final distribution:\")\n",
        "        print(f\"   Min: {min(final_counts)} images\")\n",
        "        print(f\"   Max: {max(final_counts)} images\")\n",
        "        print(f\"   Balance ratio: {min(final_counts)/max(final_counts):.2f}\")\n",
        "    \n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "    \n",
        "    return stats\n",
        "\n",
        "# Run balancing\n",
        "print(\"üîç Checking if dataset needs balancing...\")\n",
        "\n",
        "# Count current distribution\n",
        "current_counts = {}\n",
        "for class_name in sorted(class_names):\n",
        "    class_path = os.path.join(UNIFIED_DATASET_PATH, class_name)\n",
        "    num_images = len([f for f in os.listdir(class_path) \n",
        "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "    current_counts[class_name] = num_images\n",
        "\n",
        "min_imgs = min(current_counts.values())\n",
        "max_imgs = max(current_counts.values())\n",
        "imbalance_ratio = min_imgs / max_imgs\n",
        "\n",
        "if imbalance_ratio < 0.7:  # More than 30% imbalance\n",
        "    print(f\"‚ö†Ô∏è Imbalance detected! Ratio: {imbalance_ratio:.2f}\")\n",
        "    print(f\"   This can cause bias (e.g., towards maize)\")\n",
        "    print(f\"\\nüîß Applying automatic balancing...\\n\")\n",
        "    \n",
        "    # Use hybrid strategy: balance to median\n",
        "    balance_stats = balance_dataset(\n",
        "        UNIFIED_DATASET_PATH,\n",
        "        target_images_per_class=None,  # Auto-determine from median\n",
        "        strategy='hybrid'\n",
        "    )\n",
        "    \n",
        "    # Update class_names list (shouldn't change, but refresh counts)\n",
        "    print(\"üîÑ Refreshing dataset information...\")\n",
        "    \n",
        "else:\n",
        "    print(f\"‚úÖ Dataset is already balanced (ratio: {imbalance_ratio:.2f})\")\n",
        "    print(f\"   No balancing needed!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "756c35b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================\n",
        "# üìä DATASET VERIFICATION AND STATISTICS\n",
        "# =============================================================\n",
        "# Verify data quality and show class distribution\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üìä DATASET ANALYSIS\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Count images per class\n",
        "print(\"üìà Class Distribution:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "class_stats = {}\n",
        "total_images = 0\n",
        "\n",
        "for class_name in sorted(class_names):\n",
        "    class_path = os.path.join(UNIFIED_DATASET_PATH, class_name)\n",
        "    \n",
        "    # Count image files\n",
        "    num_images = len([f for f in os.listdir(class_path) \n",
        "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "    \n",
        "    class_stats[class_name] = num_images\n",
        "    total_images += num_images\n",
        "    \n",
        "    # Show crop and deficiency separately\n",
        "    crop, deficiency = class_name.split('_', 1)\n",
        "    print(f\"   {class_name:25s} {num_images:4d} images  ({crop:8s} | {deficiency})\")\n",
        "\n",
        "print(\"-\" * 70)\n",
        "print(f\"   TOTAL: {total_images:,} images across {num_classes} classes\")\n",
        "print()\n",
        "\n",
        "# Calculate statistics\n",
        "avg_per_class = total_images / num_classes\n",
        "min_images = min(class_stats.values())\n",
        "max_images = max(class_stats.values())\n",
        "min_class = min(class_stats, key=class_stats.get)\n",
        "max_class = max(class_stats, key=class_stats.get)\n",
        "\n",
        "print(\"üìä Statistics:\")\n",
        "print(f\"   Average per class: {avg_per_class:.0f} images\")\n",
        "print(f\"   Min: {min_images} images ({min_class})\")\n",
        "print(f\"   Max: {max_images} images ({max_class})\")\n",
        "print(f\"   Balance ratio: {min_images/max_images:.2f} (1.0 = perfectly balanced)\")\n",
        "\n",
        "# Check for severe imbalance\n",
        "if min_images / max_images < 0.3:\n",
        "    print(f\"\\n‚ö†Ô∏è  WARNING: Severe class imbalance detected!\")\n",
        "    print(f\"   Consider using class weights during training\")\n",
        "elif min_images / max_images < 0.5:\n",
        "    print(f\"\\n‚ö†Ô∏è  Moderate class imbalance - may affect performance\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Dataset is reasonably balanced\")\n",
        "\n",
        "# Show breakdown by crop\n",
        "print(f\"\\nüìä Breakdown by Crop:\")\n",
        "print(\"-\" * 70)\n",
        "for crop in CROP_DATASETS.keys():\n",
        "    crop_classes = [c for c in class_names if c.startswith(f\"{crop}_\")]\n",
        "    crop_images = sum(class_stats[c] for c in crop_classes)\n",
        "    print(f\"   {crop.upper():10s} {len(crop_classes):2d} classes  {crop_images:5d} images  \"\n",
        "          f\"{', '.join([c.replace(f'{crop}_', '') for c in crop_classes])}\")\n",
        "\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Verify data integrity (sample check)\n",
        "print(\"üîç Data Integrity Check (sampling 5 random images)...\")\n",
        "import random\n",
        "sample_classes = random.sample(class_names, min(3, len(class_names)))\n",
        "\n",
        "integrity_ok = True\n",
        "for class_name in sample_classes:\n",
        "    class_path = os.path.join(UNIFIED_DATASET_PATH, class_name)\n",
        "    image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    \n",
        "    # Check random images\n",
        "    for img_file in random.sample(image_files, min(2, len(image_files))):\n",
        "        img_path = os.path.join(class_path, img_file)\n",
        "        try:\n",
        "            from PIL import Image\n",
        "            img = Image.open(img_path)\n",
        "            img.verify()  # Verify it's a valid image\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Corrupted: {class_name}/{img_file}: {e}\")\n",
        "            integrity_ok = False\n",
        "\n",
        "if integrity_ok:\n",
        "    print(\"   ‚úÖ All sampled images are valid\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è Some images may be corrupted - consider data cleaning\")\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset verification complete!\")\n",
        "print(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edf0ec80",
      "metadata": {
        "id": "edf0ec80"
      },
      "source": [
        "## üîß Stage 3: Adapt Model for Unified Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb2a9be",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "beb2a9be"
      },
      "outputs": [],
      "source": [
        "# =============================================================\n",
        "# üîÑ STAGE 3: Adapt Model for Unified Classes (with Resume)\n",
        "# =============================================================\n",
        "if 'num_unified_classes' not in locals() or num_unified_classes == 0:\n",
        "    raise RuntimeError(\"‚ö†Ô∏è Run 'Build UNIFIED Dataset' cell first!\")\n",
        "\n",
        "print(f\"üîß Setting up Stage 3 for {num_unified_classes} unified classes...\")\n",
        "\n",
        "# Check for existing Stage 3 checkpoint\n",
        "STAGE3_CHECKPOINT = os.path.join(DRIVE_CHECKPOINT_DIR, 'unified_nutrient_best.keras')\n",
        "STAGE3_LOCAL = os.path.join(OUTPUT_DIR, 'unified_nutrient_best.keras')\n",
        "\n",
        "model_stage3 = None\n",
        "resume_stage3 = False\n",
        "initial_epoch = 0\n",
        "\n",
        "# Try to load existing checkpoint\n",
        "for checkpoint_path in [STAGE3_CHECKPOINT, STAGE3_LOCAL]:\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            print(f\"üîÑ Found existing Stage 3 checkpoint!\")\n",
        "            print(f\"   Loading from: {checkpoint_path}\")\n",
        "            model_stage3 = tf.keras.models.load_model(checkpoint_path)\n",
        "\n",
        "            # Verify correct output shape\n",
        "            if model_stage3.output_shape[-1] == num_unified_classes:\n",
        "                resume_stage3 = True\n",
        "                print(f\"‚úÖ Checkpoint valid ({num_unified_classes} classes)\")\n",
        "\n",
        "                # Evaluate current performance\n",
        "                print(\"üìä Evaluating checkpoint...\")\n",
        "                results = model_stage3.evaluate(val_nutrient, verbose=0)\n",
        "                print(f\"   Current - Loss: {results[0]:.4f}, Accuracy: {results[1]:.4f}\")\n",
        "\n",
        "                # Check training history for initial_epoch\n",
        "                history_path = os.path.join(DRIVE_CHECKPOINT_DIR, 'stage3_history.json')\n",
        "                if os.path.exists(history_path):\n",
        "                    with open(history_path, 'r') as f:\n",
        "                        prev_history = json.load(f)\n",
        "                        initial_epoch = len(prev_history.get('accuracy', []))\n",
        "                        print(f\"   Resuming from epoch {initial_epoch}\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Class mismatch ({model_stage3.output_shape[-1]} vs {num_unified_classes})\")\n",
        "                print(\"   Creating new model (classes changed)\")\n",
        "                model_stage3 = None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not load: {e}\")\n",
        "            model_stage3 = None\n",
        "\n",
        "# Create new model if needed\n",
        "if model_stage3 is None:\n",
        "    print(f\"üèóÔ∏è Creating NEW unified model...\")\n",
        "\n",
        "    # Get base model from Stage 2\n",
        "    base_model_stage2 = model_stage2.layers[0]\n",
        "    base_model_stage2.trainable = False  # Keep frozen initially\n",
        "\n",
        "    # Balanced classification head (prevents overfitting)\n",
        "    model_stage3 = tf.keras.Sequential([\n",
        "        base_model_stage2,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dropout(DROPOUT_RATE),\n",
        "        tf.keras.layers.Dense(384, activation='relu',\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(DROPOUT_RATE * 0.8),\n",
        "        tf.keras.layers.Dense(num_unified_classes, activation='softmax', dtype='float32')\n",
        "    ], name='unified_nutrient_model')\n",
        "\n",
        "# Compile with JIT for speed\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_STAGE3)\n",
        "\n",
        "model_stage3.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top3_acc')],\n",
        "    jit_compile=True  # XLA compilation - 10-20% faster\n",
        ")\n",
        "\n",
        "trainable_params = sum([tf.keras.backend.count_params(w) for w in model_stage3.trainable_weights])\n",
        "print(f\"\\nüìä Trainable params: {trainable_params:,}\")\n",
        "print(f\"üéØ Output classes: {num_unified_classes}\")\n",
        "print(f\"‚ö° JIT/XLA compilation: Enabled\")\n",
        "print(f\"üîÑ Resume training: {'Yes (epoch ' + str(initial_epoch) + ')' if resume_stage3 else 'No (fresh start)'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9acb2661",
      "metadata": {
        "id": "9acb2661"
      },
      "source": [
        "## üéØ Stage 3: Train on UNIFIED Nutrient Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9d3576d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547,
          "referenced_widgets": [
            "87cfe8554e6d49c08d359e7bf103bf56",
            "c41e2c6305cc494081a3ec7adad0311a"
          ]
        },
        "id": "b9d3576d",
        "outputId": "d12f68d2-55b7-4fbb-e477-a26db5835083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Stage 3: UNIFIED Nutrient Detection\n",
            "üåæ Training ALL 12 crops | Epochs: 10 | LR: 0.0005\n",
            "============================================================\n",
            "\n",
            "üöÄ Stage 3: Unified Nutrients Started\n",
            "   Target: 10 epochs\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87cfe8554e6d49c08d359e7bf103bf56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "üìà Stage 3: Unified Nutrients:   0%|                              | 0/10 [00:00<?, ?epoch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c41e2c6305cc494081a3ec7adad0311a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  Epoch 1/10:   0%|                    | 0/1306 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3035104938.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mstage3_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m history_stage3 = model_stage3.fit(\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mtrain_nutrient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_nutrient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"üöÄ Starting Stage 3: UNIFIED Nutrient Detection\")\n",
        "print(f\"üåæ Training ALL {len(CROP_DATASETS)} crops | Epochs: {UNIFIED_EPOCHS} | LR: {LEARNING_RATE_STAGE3}\")\n",
        "if resume_stage3:\n",
        "    print(f\"üîÑ RESUMING from epoch {initial_epoch}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Callbacks with TQDM progress\n",
        "callbacks_stage3 = [\n",
        "    # TQDM Progress with ETA\n",
        "    TQDMProgressCallback(UNIFIED_EPOCHS, stage_name=\"Stage 3: Unified Nutrients\"),\n",
        "\n",
        "    # Early stopping - balanced patience\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        min_delta=0.001\n",
        "    ),\n",
        "\n",
        "    # Reduce LR on plateau\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Save best to local (fast)\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        os.path.join(OUTPUT_DIR, 'unified_nutrient_best.keras'),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=0\n",
        "    ),\n",
        "\n",
        "    # Save best to Drive (persistent)\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        os.path.join(DRIVE_CHECKPOINT_DIR, 'unified_nutrient_best.keras'),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=0\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train with progress tracking\n",
        "stage3_start = datetime.now()\n",
        "\n",
        "history_stage3 = model_stage3.fit(\n",
        "    train_nutrient,\n",
        "    validation_data=val_nutrient,\n",
        "    epochs=UNIFIED_EPOCHS,\n",
        "    initial_epoch=initial_epoch,\n",
        "    callbacks=callbacks_stage3,\n",
        "    verbose=0  # Disable default output, use TQDM instead\n",
        ")\n",
        "\n",
        "# Save training history for resume\n",
        "history_dict = {k: [float(v) for v in vals] for k, vals in history_stage3.history.items()}\n",
        "with open(os.path.join(DRIVE_CHECKPOINT_DIR, 'stage3_history.json'), 'w') as f:\n",
        "    json.dump(history_dict, f)\n",
        "\n",
        "# Calculate final stats\n",
        "best_val_acc = max(history_stage3.history['val_accuracy'])\n",
        "best_val_loss = min(history_stage3.history['val_loss'])\n",
        "best_top3_acc = max(history_stage3.history['val_top3_acc'])\n",
        "final_train_acc = history_stage3.history['accuracy'][-1]\n",
        "stage3_time = datetime.now() - stage3_start\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(f\"‚úÖ Stage 3 completed in {stage3_time}\")\n",
        "print(f\"üìà Best val accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"üéØ Best top-3 accuracy: {best_top3_acc:.4f}\")\n",
        "print(f\"üìâ Best val loss: {best_val_loss:.4f}\")\n",
        "\n",
        "# Total training time\n",
        "if TRAINING_START_TIME:\n",
        "    total_training_time = datetime.now() - TRAINING_START_TIME\n",
        "    print(f\"\\n‚è±Ô∏è TOTAL TRAINING TIME: {total_training_time}\")\n",
        "\n",
        "# Check for overfitting/underfitting\n",
        "gap = final_train_acc - best_val_acc\n",
        "if gap > 0.20:\n",
        "    print(f\"\\n‚ö†Ô∏è Overfitting detected (gap: {gap:.2%})\")\n",
        "elif best_val_acc < 0.5:\n",
        "    print(f\"\\n‚ö†Ô∏è Possible underfitting (val_acc: {best_val_acc:.2%})\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Good generalization (gap: {gap:.2%})\")\n",
        "\n",
        "print(f\"\\nüíæ Model & history saved to Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "135fcb26",
      "metadata": {
        "id": "135fcb26"
      },
      "source": [
        "## üìà Stage 3 Results Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ce165f8",
      "metadata": {
        "id": "3ce165f8"
      },
      "outputs": [],
      "source": [
        "# Quick training visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(history_stage3.history['accuracy'], 'b-', label='Train')\n",
        "axes[0].plot(history_stage3.history['val_accuracy'], 'r-', label='Val')\n",
        "axes[0].set_title('Stage 3: Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(history_stage3.history['loss'], 'b-', label='Train')\n",
        "axes[1].plot(history_stage3.history['val_loss'], 'r-', label='Val')\n",
        "axes[1].set_title('Stage 3: Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'stage3_training_history.png'), dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10140073",
      "metadata": {
        "id": "10140073"
      },
      "source": [
        "## üîç Model Evaluation & Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cca3c2c1",
      "metadata": {
        "id": "cca3c2c1"
      },
      "outputs": [],
      "source": [
        "# Quick evaluation (skip heavy confusion matrix for speed)\n",
        "print(\"üîç Evaluating UNIFIED model...\")\n",
        "results = model_stage3.evaluate(val_nutrient, verbose=0)\n",
        "\n",
        "print(f\"\\nüìä Validation Metrics:\")\n",
        "print(f\"   Loss: {results[0]:.4f}\")\n",
        "print(f\"   Accuracy: {results[1]:.4f}\")\n",
        "print(f\"   Top-3 Accuracy: {results[2]:.4f}\")\n",
        "\n",
        "# Quick per-crop accuracy (sample-based for speed)\n",
        "print(f\"\\nüåæ Per-Crop Performance (quick check):\")\n",
        "y_true, y_pred = [], []\n",
        "for images, labels in val_nutrient.take(20):  # Sample only\n",
        "    predictions = model_stage3.predict(images, verbose=0)\n",
        "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
        "    y_pred.extend(np.argmax(predictions, axis=1))\n",
        "\n",
        "for crop in list(CROP_DATASETS.keys())[:6]:  # First 6 crops\n",
        "    crop_classes = [cls for cls in class_names if cls.startswith(f\"{crop}_\")]\n",
        "    if not crop_classes:\n",
        "        continue\n",
        "    crop_indices = [class_names.index(cls) for cls in crop_classes]\n",
        "    crop_mask = np.isin(y_true, crop_indices)\n",
        "    if crop_mask.sum() > 0:\n",
        "        crop_acc = (np.array(y_true)[crop_mask] == np.array(y_pred)[crop_mask]).mean()\n",
        "        print(f\"   {crop.upper():12s}: {crop_acc:.1%}\")\n",
        "\n",
        "# Save classification report\n",
        "report = classification_report(y_true, y_pred, target_names=[class_names[i] for i in sorted(set(y_true))], output_dict=True, zero_division=0)\n",
        "with open(os.path.join(OUTPUT_DIR, 'unified_classification_report.json'), 'w') as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Evaluation complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "719b395d",
      "metadata": {
        "id": "719b395d"
      },
      "source": [
        "## üíæ Export to TensorFlow Lite for Mobile Deployment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bf78c8d",
      "metadata": {
        "id": "1bf78c8d"
      },
      "outputs": [],
      "source": [
        "print(\"üì¶ Converting to TensorFlow Lite...\")\n",
        "print(f\"‚è±Ô∏è Session time: {get_session_time()}\")\n",
        "check_time_limit()  # Warn if approaching 3-hour limit\n",
        "\n",
        "# CRITICAL: Disable mixed precision completely\n",
        "print(\"üîÑ Disabling mixed precision for FP32 conversion...\")\n",
        "tf.keras.mixed_precision.set_global_policy('float32')\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Load best model (will still have FP16 signatures from training)\n",
        "best_model_path = os.path.join(OUTPUT_DIR, 'unified_nutrient_best.keras')\n",
        "if not os.path.exists(best_model_path):\n",
        "    best_model_path = os.path.join(DRIVE_CHECKPOINT_DIR, 'unified_nutrient_best.keras')\n",
        "\n",
        "print(\"üì• Loading original model...\")\n",
        "original_model = tf.keras.models.load_model(best_model_path)\n",
        "\n",
        "# Build the model by calling it with dummy input\n",
        "print(\"üîß Building model with dummy input...\")\n",
        "dummy_input = tf.zeros((1, 224, 224, 3), dtype=tf.float32)\n",
        "_ = original_model(dummy_input, training=False)\n",
        "print(f\"   ‚úÖ Model built successfully\")\n",
        "\n",
        "# Use concrete function with explicit FP32 signature\n",
        "print(\"\\n‚öôÔ∏è Creating TFLite converter with explicit FP32 signature...\")\n",
        "\n",
        "@tf.function(input_signature=[tf.TensorSpec(shape=[1, 224, 224, 3], dtype=tf.float32)])\n",
        "def serving_fn(input_image):\n",
        "    x = tf.cast(input_image, tf.float32)\n",
        "    output = original_model(x, training=False)\n",
        "    return tf.cast(output, tf.float32)\n",
        "\n",
        "# Get concrete function\n",
        "concrete_func = serving_fn.get_concrete_function()\n",
        "\n",
        "# Convert using concrete function (bypasses model signature issues)\n",
        "print(\"üí° Converting using concrete function (explicit FP32)...\")\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # 8-bit weight quantization\n",
        "\n",
        "# Try standard ops first\n",
        "try:\n",
        "    print(\"   Attempting standard TFLite ops...\")\n",
        "    tflite_model = converter.convert()\n",
        "    print(\"   ‚úÖ Standard ops conversion successful!\")\n",
        "    uses_flex = False\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Standard ops failed: {str(e)[:100]}...\")\n",
        "    print(\"   üîÑ Falling back to TF Select ops (flex delegates)...\")\n",
        "    \n",
        "    # Enable TF Select ops as fallback\n",
        "    converter.target_spec.supported_ops = [\n",
        "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "        tf.lite.OpsSet.SELECT_TF_OPS\n",
        "    ]\n",
        "    converter._experimental_lower_tensor_list_ops = False\n",
        "    tflite_model = converter.convert()\n",
        "    print(\"   ‚úÖ TF Select ops conversion successful!\")\n",
        "    uses_flex = True\n",
        "\n",
        "# Save to both local and Drive\n",
        "tflite_path = os.path.join(OUTPUT_DIR, 'fasalvaidya_unified.tflite')\n",
        "tflite_drive_path = os.path.join(DRIVE_CHECKPOINT_DIR, 'fasalvaidya_unified.tflite')\n",
        "\n",
        "with open(tflite_path, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "with open(tflite_drive_path, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "keras_size = os.path.getsize(best_model_path) / (1024 * 1024)\n",
        "tflite_size = os.path.getsize(tflite_path) / (1024 * 1024)\n",
        "\n",
        "print(f\"\\n‚úÖ Conversion complete!\")\n",
        "print(f\"üìä Keras: {keras_size:.1f}MB ‚Üí TFLite: {tflite_size:.1f}MB ({(1-tflite_size/keras_size)*100:.0f}% smaller)\")\n",
        "print(f\"üöÄ Single model for {len(CROP_DATASETS)} crops!\")\n",
        "print(f\"‚ö° Optimized with 8-bit weight quantization\")\n",
        "if uses_flex:\n",
        "    print(f\"üì± Uses TF Select ops (requires TFLite with flex delegate)\")\n",
        "    print(f\"   Note: App needs tensorflow-lite-select-tf-ops dependency\")\n",
        "else:\n",
        "    print(f\"‚úÖ Uses standard TFLite runtime (no flex ops needed)\")\n",
        "print(f\"üîÑ FP32 input/output (mobile-friendly)\")\n",
        "print(f\"\\nüíæ Saved to:\")\n",
        "print(f\"   Local: {tflite_path}\")\n",
        "print(f\"   Drive: {tflite_drive_path} (persistent)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6488f02a",
      "metadata": {
        "id": "6488f02a"
      },
      "source": [
        "## üß™ Test TFLite Model Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35b10789",
      "metadata": {
        "id": "35b10789"
      },
      "outputs": [],
      "source": [
        "# Quick TFLite verification\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"üîç TFLite Model:\")\n",
        "print(f\"   Input: {input_details[0]['shape']} ({input_details[0]['dtype']})\")\n",
        "print(f\"   Output: {output_details[0]['shape']} ({num_unified_classes} classes)\")\n",
        "\n",
        "# Quick test\n",
        "for images, labels in val_nutrient.take(1):\n",
        "    test_image = images[0].numpy()\n",
        "    input_data = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    pred_idx = np.argmax(output[0])\n",
        "    true_idx = np.argmax(labels[0].numpy())\n",
        "\n",
        "    print(f\"\\nüß™ Quick test:\")\n",
        "    print(f\"   True: {class_names[true_idx]}\")\n",
        "    print(f\"   Pred: {class_names[pred_idx]} ({output[0][pred_idx]:.1%})\")\n",
        "    print(f\"   {'‚úÖ CORRECT' if pred_idx == true_idx else '‚ùå INCORRECT'}\")\n",
        "    break\n",
        "\n",
        "print(\"\\n‚úÖ TFLite model verified!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "476ffab2",
      "metadata": {
        "id": "476ffab2"
      },
      "source": [
        "## üì§ Save Model Metadata & Class Labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f144a7",
      "metadata": {
        "id": "39f144a7"
      },
      "outputs": [],
      "source": [
        "# Save metadata and labels (to both local and Drive)\n",
        "print(\"üìù Saving metadata...\")\n",
        "\n",
        "crop_class_mapping = {crop: [c for c in class_names if c.startswith(f\"{crop}_\")]\n",
        "                      for crop in CROP_DATASETS.keys()}\n",
        "\n",
        "metadata = {\n",
        "    'model_type': 'unified_multi_crop',\n",
        "    'model_version': '2.0',\n",
        "    'training_date': datetime.now().isoformat(),\n",
        "    'architecture': 'MobileNetV2',\n",
        "    'supported_crops': list(CROP_DATASETS.keys()),\n",
        "    'num_crops': len(CROP_DATASETS),\n",
        "    'input_shape': [IMG_SIZE, IMG_SIZE, 3],\n",
        "    'num_classes': num_unified_classes,\n",
        "    'class_names': class_names,\n",
        "    'crop_class_mapping': crop_class_mapping,\n",
        "    'metrics': {'accuracy': float(results[1]), 'top3_accuracy': float(results[2])},\n",
        "    'preprocessing': {'method': 'MobileNetV2', 'normalization': '[-1, 1]'},\n",
        "    'training_config': {\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'plantvillage_epochs': PLANTVILLAGE_EPOCHS,\n",
        "        'unified_epochs': UNIFIED_EPOCHS,\n",
        "        'dropout_rate': DROPOUT_RATE,\n",
        "        'optimizations': ['mixed_precision_fp16', 'jit_compile', 'autotune_prefetch']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to both locations\n",
        "for save_dir in [OUTPUT_DIR, DRIVE_CHECKPOINT_DIR]:\n",
        "    with open(os.path.join(save_dir, 'unified_model_metadata.json'), 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    with open(os.path.join(save_dir, 'labels.txt'), 'w') as f:\n",
        "        f.write('\\n'.join(class_names))\n",
        "\n",
        "print(f\"‚úÖ Saved: metadata.json, labels.txt\")\n",
        "print(f\"üìä {len(CROP_DATASETS)} crops, {num_unified_classes} classes\")\n",
        "print(f\"üíæ Saved to both local and Drive (persistent)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e3d8a9c",
      "metadata": {
        "id": "5e3d8a9c"
      },
      "source": [
        "## üì¶ Download Models to Local Machine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9ddb7f6",
      "metadata": {
        "id": "d9ddb7f6"
      },
      "outputs": [],
      "source": [
        "# Create and download zip\n",
        "import shutil\n",
        "\n",
        "# Session summary\n",
        "print(\"=\" * 60)\n",
        "print(\"üéâ TRAINING SESSION COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"‚è±Ô∏è Total session time: {get_session_time()}\")\n",
        "print(f\"üìä Final validation accuracy: {results[1]:.4f}\")\n",
        "print(f\"üéØ Final top-3 accuracy: {results[2]:.4f}\")\n",
        "\n",
        "zip_filename = 'fasalvaidya_unified_model'\n",
        "shutil.make_archive(f'/content/{zip_filename}', 'zip', OUTPUT_DIR)\n",
        "\n",
        "print(f\"\\nüì¶ Created: {zip_filename}.zip\")\n",
        "print(f\"\\nüìÇ Contents:\")\n",
        "print(f\"   üì± fasalvaidya_unified.tflite ({tflite_size:.1f}MB)\")\n",
        "print(f\"   üíæ unified_nutrient_best.keras\")\n",
        "print(f\"   üìÑ unified_model_metadata.json\")\n",
        "print(f\"   üè∑Ô∏è labels.txt ({num_unified_classes} classes)\")\n",
        "print(f\"\\nüåæ Supports: {', '.join(list(CROP_DATASETS.keys())[:6])}...\")\n",
        "\n",
        "print(f\"\\nüíæ ALSO SAVED TO DRIVE (persistent):\")\n",
        "print(f\"   {DRIVE_CHECKPOINT_DIR}\")\n",
        "print(f\"   ‚úÖ Can resume training if disconnected!\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(f'/content/{zip_filename}.zip')\n",
        "print(f\"\\n‚¨áÔ∏è Download started!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17a73ed9",
      "metadata": {
        "id": "17a73ed9"
      },
      "source": [
        "## üéâ Training Complete!\n",
        "\n",
        "### üöÄ Performance Optimizations Applied\n",
        "\n",
        "This notebook is **fully optimized** for fastest training with consistent data architecture:\n",
        "\n",
        "| Optimization | Benefit |\n",
        "|--------------|---------|\n",
        "| **Float32 Precision** | No mixed precision bugs, stable training |\n",
        "| **Local SSD Data** | 10-50x faster I/O vs Google Drive |\n",
        "| **XLA/JIT Compilation** | 10-20% faster training |\n",
        "| **AUTOTUNE Prefetch** | Maximizes GPU utilization |\n",
        "| **Parallel Data Loading** | Uses all CPU cores |\n",
        "| **Smart Caching** | Validation set cached in memory |\n",
        "| **Optimized Augmentation** | Fast TF ops with XLA compilation |\n",
        "| **Batch Size 32** | Better GPU memory utilization |\n",
        "\n",
        "### ‚è±Ô∏è Expected Training Time (4 Crops)\n",
        "\n",
        "| Stage | Time | Description |\n",
        "|-------|------|-------------|\n",
        "| Data Copy to SSD | 3-5 min | One-time setup per session |\n",
        "| Stage 2: PlantVillage | 15-20 min | 8 epochs, transfer learning base |\n",
        "| Stage 3: Unified Nutrient | 30-40 min | 15 epochs, 4 crops combined |\n",
        "| Export & Download | 2-3 min | Model conversion and download |\n",
        "| **TOTAL** | **~50-70 min** | Complete end-to-end |\n",
        "\n",
        "### üìä Training Configuration\n",
        "\n",
        "```python\n",
        "# Dataset\n",
        "CROPS = ['rice', 'wheat', 'tomato', 'maize']\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32  # Optimized for GPU\n",
        "\n",
        "# Training\n",
        "PLANTVILLAGE_EPOCHS = 8\n",
        "UNIFIED_EPOCHS = 15\n",
        "LEARNING_RATE_STAGE2 = 1e-3\n",
        "LEARNING_RATE_STAGE3 = 5e-4\n",
        "DROPOUT_RATE = 0.3\n",
        "\n",
        "# Performance\n",
        "Precision: float32 (full compatibility)\n",
        "XLA/JIT: Enabled\n",
        "AUTOTUNE: Enabled\n",
        "Workers: All CPU cores\n",
        "Data Location: Local SSD\n",
        "```\n",
        "\n",
        "### üì¶ Output Files\n",
        "\n",
        "| File | Size | Description |\n",
        "|------|------|-------------|\n",
        "| `unified_savedmodel/` | ~20MB | SavedModel format (for backend) |\n",
        "| `fasalvaidya_unified.tflite` | ~4MB | Mobile TFLite model |\n",
        "| `unified_nutrient_best.keras` | ~15MB | Full Keras checkpoint |\n",
        "| `unified_model_metadata.json` | <1KB | Model info & class mappings |\n",
        "| `labels.txt` | <1KB | All {num_classes} class labels |\n",
        "\n",
        "### üîÑ Checkpoints Saved To\n",
        "\n",
        "- **Local:** `/content/fasalvaidya_unified_model/`\n",
        "- **Drive (Persistent):** `/content/drive/MyDrive/FasalVaidya_Checkpoints/`\n",
        "\n",
        "If training is interrupted, simply re-run from the checkpoint cells - it will automatically resume!\n",
        "\n",
        "---\n",
        "\n",
        "### üì± Next Steps\n",
        "\n",
        "1. **Download** the `unified_savedmodel` folder\n",
        "2. **Copy** to `backend/ml/models/unified_savedmodel/`\n",
        "3. **Test** with: `python backend/test_unified.py`\n",
        "4. **Verify** predictions show 70-95% confidence (not 0.2%)\n",
        "5. **Run** the Flask backend and Expo frontend\n",
        "\n",
        "Expected output:\n",
        "```\n",
        "N score: 78.5% (moderate deficiency)\n",
        "P score: 12.3% (healthy)\n",
        "K score: 5.2% (healthy)\n",
        "Detected: rice_Nitrogen(N)\n",
        "Confidence: 78.5%\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
