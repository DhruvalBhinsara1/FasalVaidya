{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e188b6",
   "metadata": {},
   "source": [
    "## üì¶ Setup & Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b15cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow>=2.15.0 kaggle opendatasets scikit-learn matplotlib seaborn\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ddc489",
   "metadata": {},
   "source": [
    "## üîë Configuration\n",
    "\n",
    "### Set your crop type and paths here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== UNIFIED MODEL - ALL CROPS ==========\n",
    "# Root path to your \"Leaf Nutrient Data Sets\" folder on Google Drive\n",
    "NUTRIENT_DATASETS_ROOT = '/content/drive/MyDrive/Leaf Nutrient Data Sets'\n",
    "\n",
    "# ALL 12 CROPS - Combined into ONE model automatically!\n",
    "CROP_DATASETS = {\n",
    "    'rice': 'Rice Nutrients',\n",
    "    'wheat': 'Wheat Nitrogen',\n",
    "    'tomato': 'Tomato Nutrients',\n",
    "    'maize': 'Maize Nutrients',\n",
    "    'banana': 'Banana leaves Nutrient',\n",
    "    'coffee': 'Coffee Nutrients',\n",
    "    'cucumber': 'Cucumber Nutrients',\n",
    "    'eggplant': 'EggPlant Nutrients',\n",
    "    'ashgourd': 'Ashgourd Nutrients',\n",
    "    'bittergourd': 'Bittergourd Nutrients',\n",
    "    'ridgegourd': 'Ridgegourd',\n",
    "    'snakegourd': 'Snakegourd Nutrients'\n",
    "}\n",
    "\n",
    "# =============================================================\n",
    "# üöÄ ULTRA MEMORY-SAFE SETTINGS FOR COLAB FREE TIER\n",
    "# =============================================================\n",
    "# ‚ö†Ô∏è Colab free tier has ~12GB RAM - we CANNOT cache or shuffle large datasets!\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32   # Small batches = less RAM\n",
    "PLANTVILLAGE_EPOCHS = 8   # More epochs since no shuffle buffer\n",
    "UNIFIED_EPOCHS = 15\n",
    "LEARNING_RATE_STAGE2 = 3e-4  # Slightly lower for stability\n",
    "LEARNING_RATE_STAGE3 = 1e-4\n",
    "\n",
    "# Enable mixed precision training\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"üöÄ Mixed precision enabled (FP16)\")\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = '/content/fasalvaidya_unified_model'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"üåæ UNIFIED MULTI-CROP MODEL TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training ONE model for ALL {len(CROP_DATASETS)} crops\")\n",
    "print(f\"\\nüî• ULTRA MEMORY-SAFE MODE (prevents RAM crash):\")\n",
    "print(f\"   - Batch size: {BATCH_SIZE} (small = safe)\")\n",
    "print(f\"   - NO caching (prevents RAM explosion)\")\n",
    "print(f\"   - NO shuffle buffer (prevents RAM explosion)\")\n",
    "print(f\"   - Stage 2: {PLANTVILLAGE_EPOCHS} epochs\")\n",
    "print(f\"   - Stage 3: {UNIFIED_EPOCHS} epochs\")\n",
    "print(f\"\\n‚ö° Training will be slower but WON'T CRASH!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ab98b",
   "metadata": {},
   "source": [
    "## üíæ Mount Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2242a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify ALL crop datasets exist\n",
    "print(\"üîç Verifying crop datasets...\")\n",
    "missing_crops = []\n",
    "for crop, folder_name in CROP_DATASETS.items():\n",
    "    crop_path = os.path.join(NUTRIENT_DATASETS_ROOT, folder_name)\n",
    "    if os.path.exists(crop_path):\n",
    "        num_classes = len([d for d in os.listdir(crop_path) if os.path.isdir(os.path.join(crop_path, d))])\n",
    "        print(f\"‚úÖ {crop.upper()}: {num_classes} classes at {crop_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {crop.upper()}: NOT FOUND at {crop_path}\")\n",
    "        missing_crops.append(crop)\n",
    "\n",
    "if missing_crops:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: {len(missing_crops)} crop(s) not found: {', '.join(missing_crops)}\")\n",
    "    print(\"Please verify paths in NUTRIENT_DATASETS_ROOT and CROP_DATASETS\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All {len(CROP_DATASETS)} crop datasets verified!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061a425",
   "metadata": {},
   "source": [
    "## üå± Stage 1: Download PlantVillage Dataset from Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c0d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Kaggle credentials\n",
    "# IMPORTANT: You need to manually download kaggle.json FIRST!\n",
    "# \n",
    "# üìù HOW TO GET kaggle.json:\n",
    "# 1. Go to https://www.kaggle.com/settings\n",
    "# 2. Scroll down to \"API\" section\n",
    "# 3. Click \"Create New Token\" button\n",
    "# 4. This will DOWNLOAD a file called \"kaggle.json\" to your computer\n",
    "# 5. Find the downloaded file (usually in your Downloads folder)\n",
    "# 6. Then come back here and upload it when prompted below\n",
    "#\n",
    "# ‚ö†Ô∏è NOTE: If you only see the API key on screen but no download happened,\n",
    "#    click \"Create New Token\" again - it should download the file\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üì§ UPLOAD YOUR kaggle.json FILE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüìù If you haven't downloaded it yet:\")\n",
    "print(\"   1. Go to: https://www.kaggle.com/settings\")\n",
    "print(\"   2. Scroll to 'API' section\")\n",
    "print(\"   3. Click 'Create New Token' (downloads kaggle.json)\")\n",
    "print(\"   4. Find the file in your Downloads folder\")\n",
    "print(\"   5. Click 'Choose Files' below and select it\")\n",
    "print(\"\\n‚è≥ Waiting for your kaggle.json file...\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Verify the file was uploaded\n",
    "if 'kaggle.json' not in uploaded:\n",
    "    print(\"\\n‚ùå ERROR: kaggle.json was not uploaded!\")\n",
    "    print(\"   Please make sure you selected the correct file.\")\n",
    "    raise FileNotFoundError(\"kaggle.json not found in uploaded files\")\n",
    "\n",
    "# Move kaggle.json to the correct location\n",
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"\\n‚úÖ Kaggle credentials configured successfully!\")\n",
    "print(\"üìÅ File saved to: ~/.kaggle/kaggle.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PlantVillage dataset from Kaggle (SKIP IF ALREADY EXISTS)\n",
    "import opendatasets as od\n",
    "\n",
    "PLANTVILLAGE_URL = 'https://www.kaggle.com/datasets/emmarex/plantdisease'\n",
    "PLANTVILLAGE_PATH = '/content/plantvillage'\n",
    "\n",
    "# Known possible paths after download\n",
    "POSSIBLE_PATHS = [\n",
    "    os.path.join(PLANTVILLAGE_PATH, 'plantdisease', 'PlantVillage'),\n",
    "    os.path.join(PLANTVILLAGE_PATH, 'PlantVillage'),\n",
    "    os.path.join(PLANTVILLAGE_PATH, 'plantdisease', 'plantvillage', 'PlantVillage'),\n",
    "]\n",
    "\n",
    "def find_plantvillage_dataset():\n",
    "    \"\"\"Find PlantVillage dataset if it exists\"\"\"\n",
    "    for path in POSSIBLE_PATHS:\n",
    "        if os.path.exists(path) and os.path.isdir(path):\n",
    "            subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "            if len(subdirs) >= 15:\n",
    "                sample_dir = os.path.join(path, subdirs[0])\n",
    "                sample_files = [f for f in os.listdir(sample_dir)\n",
    "                              if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                if len(sample_files) > 0:\n",
    "                    return path\n",
    "    return None\n",
    "\n",
    "# Check if dataset already exists\n",
    "existing_path = find_plantvillage_dataset()\n",
    "\n",
    "if existing_path:\n",
    "    print(\"‚úÖ PlantVillage dataset ALREADY EXISTS! Skipping download...\")\n",
    "    print(f\"üìÅ Using cached dataset at: {existing_path}\")\n",
    "    PLANTVILLAGE_PATH = existing_path\n",
    "else:\n",
    "    print(\"üì• Downloading PlantVillage dataset (54,305 images)...\")\n",
    "    print(\"‚è≥ This will take 3-5 minutes (first time only)...\")\n",
    "    \n",
    "    od.download(PLANTVILLAGE_URL, data_dir=PLANTVILLAGE_PATH)\n",
    "    \n",
    "    print(\"\\nüîç Locating dataset structure...\")\n",
    "    \n",
    "    # Find the dataset path\n",
    "    dataset_root = find_plantvillage_dataset()\n",
    "    \n",
    "    if not dataset_root:\n",
    "        # Search recursively as fallback\n",
    "        for root, dirs, files in os.walk(PLANTVILLAGE_PATH):\n",
    "            if len(dirs) >= 15:\n",
    "                has_images = False\n",
    "                for d in dirs[:3]:\n",
    "                    dir_path = os.path.join(root, d)\n",
    "                    if os.path.isdir(dir_path):\n",
    "                        dir_files = os.listdir(dir_path)\n",
    "                        if any(f.lower().endswith(('.jpg', '.jpeg', '.png')) for f in dir_files):\n",
    "                            has_images = True\n",
    "                            break\n",
    "                if has_images:\n",
    "                    dataset_root = root\n",
    "                    break\n",
    "    \n",
    "    if dataset_root:\n",
    "        PLANTVILLAGE_PATH = dataset_root\n",
    "    else:\n",
    "        raise FileNotFoundError(\"‚ùå PlantVillage dataset not found after download\")\n",
    "\n",
    "# Verify dataset\n",
    "class_dirs = [d for d in os.listdir(PLANTVILLAGE_PATH) \n",
    "              if os.path.isdir(os.path.join(PLANTVILLAGE_PATH, d))]\n",
    "num_classes = len(class_dirs)\n",
    "\n",
    "print(f\"\\n‚úÖ PlantVillage dataset ready!\")\n",
    "print(f\"üìÅ Path: {PLANTVILLAGE_PATH}\")\n",
    "print(f\"üåø Classes: {num_classes}\")\n",
    "\n",
    "# Quick image count\n",
    "total_images = sum(len([f for f in os.listdir(os.path.join(PLANTVILLAGE_PATH, cls))\n",
    "                        if f.lower().endswith(('.jpg', '.jpeg', '.png'))]) \n",
    "                   for cls in class_dirs[:5])\n",
    "print(f\"üìä Sample: First 5 classes have {total_images:,} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac424fc",
   "metadata": {},
   "source": [
    "## üìä Data Exploration & Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze PlantVillage dataset\n",
    "plantvillage_classes = sorted(os.listdir(PLANTVILLAGE_PATH))\n",
    "print(f\"üå± PlantVillage Dataset:\")\n",
    "print(f\"Total classes: {len(plantvillage_classes)}\")\n",
    "print(f\"\\nSample classes:\")\n",
    "for cls in plantvillage_classes[:5]:\n",
    "    class_path = os.path.join(PLANTVILLAGE_PATH, cls)\n",
    "    if os.path.isdir(class_path):\n",
    "        num_images = len(os.listdir(class_path))\n",
    "        print(f\"  - {cls}: {num_images} images\")\n",
    "\n",
    "# Build unified dataset info\n",
    "print(f\"\\nüåæ UNIFIED Nutrient Dataset (ALL {len(CROP_DATASETS)} crops):\")\n",
    "total_classes = 0\n",
    "total_images = 0\n",
    "\n",
    "for crop, folder_name in CROP_DATASETS.items():\n",
    "    crop_path = os.path.join(NUTRIENT_DATASETS_ROOT, folder_name)\n",
    "    if os.path.exists(crop_path):\n",
    "        crop_classes = [d for d in os.listdir(crop_path) if os.path.isdir(os.path.join(crop_path, d))]\n",
    "        crop_images = sum([len([f for f in os.listdir(os.path.join(crop_path, cls)) \n",
    "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))]) \n",
    "                          for cls in crop_classes])\n",
    "        total_classes += len(crop_classes)\n",
    "        total_images += crop_images\n",
    "        print(f\"  {crop.upper()}: {len(crop_classes)} classes, {crop_images} images\")\n",
    "\n",
    "print(f\"\\nüìä UNIFIED TOTALS:\")\n",
    "print(f\"  Total classes: {total_classes}\")\n",
    "print(f\"  Total images: {total_images}\")\n",
    "print(f\"  Class format: {{crop}}_{{deficiency}} (e.g., rice_N, wheat_healthy)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db7afbe",
   "metadata": {},
   "source": [
    "## üî® Create Data Pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# üöÄ ULTRA MEMORY-SAFE DATA PIPELINE (NO CACHING, NO SHUFFLE BUFFER)\n",
    "# =============================================================\n",
    "# ‚ö†Ô∏è Colab free tier has ~12GB RAM - caching causes crashes!\n",
    "# Solution: NO caching, NO shuffle buffer - slower but STABLE!\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def create_dataset(data_dir, img_size, batch_size, validation_split=0.2, subset=None):\n",
    "    \"\"\"Create dataset with built-in shuffling only (memory-safe)\"\"\"\n",
    "    return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=validation_split,\n",
    "        subset=subset,\n",
    "        seed=42,\n",
    "        image_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        label_mode='categorical',\n",
    "        shuffle=True  # Built-in shuffle is memory-safe (shuffles filenames only)\n",
    "    )\n",
    "\n",
    "@tf.function\n",
    "def augment_light(image, label):\n",
    "    \"\"\"Lightweight augmentation\"\"\"\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, 0.1)\n",
    "    return image, label\n",
    "\n",
    "@tf.function  \n",
    "def normalize_mobilenet(image, label):\n",
    "    \"\"\"Normalize for MobileNetV2\"\"\"\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "def build_pipeline(dataset, is_training=True):\n",
    "    \"\"\"Memory-safe pipeline - NO caching, NO shuffle buffer\"\"\"\n",
    "    if is_training:\n",
    "        dataset = dataset.map(augment_light, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.map(normalize_mobilenet, num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.prefetch(1)  # Prefetch just 1 batch to save RAM\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Create PlantVillage datasets\n",
    "print(\"üì¶ Creating PlantVillage datasets (MEMORY-SAFE mode)...\")\n",
    "print(\"‚ö†Ô∏è NO caching = stable but slower (prevents RAM crash)\")\n",
    "\n",
    "train_plantvillage_raw = create_dataset(\n",
    "    PLANTVILLAGE_PATH, IMG_SIZE, BATCH_SIZE, \n",
    "    validation_split=0.2, subset='training'\n",
    ")\n",
    "val_plantvillage_raw = create_dataset(\n",
    "    PLANTVILLAGE_PATH, IMG_SIZE, BATCH_SIZE,\n",
    "    validation_split=0.2, subset='validation'\n",
    ")\n",
    "\n",
    "# Apply memory-safe pipeline (NO caching!)\n",
    "train_plantvillage = build_pipeline(train_plantvillage_raw, is_training=True)\n",
    "val_plantvillage = build_pipeline(val_plantvillage_raw, is_training=False)\n",
    "\n",
    "train_batches = tf.data.experimental.cardinality(train_plantvillage_raw).numpy()\n",
    "val_batches = tf.data.experimental.cardinality(val_plantvillage_raw).numpy()\n",
    "\n",
    "print(f\"\\n‚úÖ PlantVillage datasets ready (MEMORY-SAFE)\")\n",
    "print(f\"   Training: {train_batches} batches √ó {BATCH_SIZE} = ~{train_batches * BATCH_SIZE:,} images\")\n",
    "print(f\"   Validation: {val_batches} batches\")\n",
    "print(f\"   ‚úÖ No caching = No RAM crash!\")\n",
    "print(f\"   ‚è±Ô∏è Each epoch reads from disk (slower but stable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c8d714",
   "metadata": {},
   "source": [
    "## ‚úÖ Pre-Training Validation\n",
    "\n",
    "Run this cell to verify everything is set up correctly before training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ PRE-TRAINING VALIDATION (Memory-Safe)\n",
    "print(\"=\" * 60)\n",
    "print(\"üîç PRE-TRAINING VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "errors = []\n",
    "\n",
    "# 1. GPU Check\n",
    "print(\"\\n1Ô∏è‚É£ GPU Check...\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"   ‚úÖ GPU: {gpus[0].name}\")\n",
    "else:\n",
    "    errors.append(\"No GPU detected!\")\n",
    "    print(\"   ‚ö†Ô∏è No GPU - training will be slow\")\n",
    "\n",
    "# 2. Quick data test (just 1 batch)\n",
    "print(\"\\n2Ô∏è‚É£ Data Pipeline Check...\")\n",
    "try:\n",
    "    for batch_images, batch_labels in train_plantvillage.take(1):\n",
    "        print(f\"   ‚úÖ Batch shape: {batch_images.shape}\")\n",
    "        print(f\"   ‚úÖ Labels shape: {batch_labels.shape}\")\n",
    "        print(f\"   ‚úÖ Image dtype: {batch_images.dtype}\")\n",
    "        break\n",
    "except Exception as e:\n",
    "    errors.append(f\"Data pipeline error: {e}\")\n",
    "    print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "# 3. MobileNetV2 base check (quick)\n",
    "print(\"\\n3Ô∏è‚É£ MobileNetV2 Base Check...\")\n",
    "try:\n",
    "    test_base = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    print(f\"   ‚úÖ MobileNetV2 loaded ({len(test_base.layers)} layers)\")\n",
    "    del test_base  # Free memory\n",
    "except Exception as e:\n",
    "    errors.append(f\"MobileNetV2 error: {e}\")\n",
    "    print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "# 4. Memory check\n",
    "print(\"\\n4Ô∏è‚É£ Memory Status...\")\n",
    "try:\n",
    "    import psutil\n",
    "    ram_gb = psutil.virtual_memory().available / (1024**3)\n",
    "    print(f\"   ‚úÖ Available RAM: {ram_gb:.1f} GB\")\n",
    "    if ram_gb < 2:\n",
    "        print(\"   ‚ö†Ô∏è Low RAM - may crash during training\")\n",
    "except:\n",
    "    print(\"   ‚ÑπÔ∏è Could not check RAM\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if errors:\n",
    "    print(\"‚ùå ISSUES FOUND:\")\n",
    "    for e in errors:\n",
    "        print(f\"   ‚Ä¢ {e}\")\n",
    "else:\n",
    "    print(\"‚úÖ ALL CHECKS PASSED!\")\n",
    "    print(f\"\\nüöÄ Ready to train with:\")\n",
    "    print(f\"   ‚Ä¢ Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"   ‚Ä¢ Memory-safe mode (no caching)\")\n",
    "    print(f\"   ‚Ä¢ Mixed precision: FP16\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a853f",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Stage 2: Build Model with MobileNetV2 Base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed2908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes, input_shape=(224, 224, 3), freeze_base=True):\n",
    "    \"\"\"Create MobileNetV2-based model optimized for T4 GPU\"\"\"\n",
    "    \n",
    "    # Load MobileNetV2 with ImageNet weights\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    base_model.trainable = not freeze_base\n",
    "    \n",
    "    # Streamlined classification head (faster training)\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        # Float32 output for numerical stability with mixed precision\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', dtype='float32')\n",
    "    ])\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Get number of PlantVillage classes\n",
    "num_plantvillage_classes = len(plantvillage_classes)\n",
    "\n",
    "print(f\"üèóÔ∏è Creating model for PlantVillage ({num_plantvillage_classes} classes)...\")\n",
    "\n",
    "model_stage2, base_model = create_model(num_plantvillage_classes, freeze_base=True)\n",
    "\n",
    "# Compile with mixed precision optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_STAGE2)\n",
    "\n",
    "model_stage2.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']  # Simplified metrics for faster training\n",
    ")\n",
    "\n",
    "# Count trainable params\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in model_stage2.trainable_weights])\n",
    "print(f\"\\nüîí Base model frozen: {not base_model.trainable}\")\n",
    "print(f\"üìä Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"üíæ Mixed precision: FP16 enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa4eee",
   "metadata": {},
   "source": [
    "## üéØ Stage 2: Train on PlantVillage Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0dc4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting Stage 2: PlantVillage Fine-tuning\")\n",
    "print(f\"‚è±Ô∏è Epochs: {PLANTVILLAGE_EPOCHS} | LR: {LEARNING_RATE_STAGE2}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Minimal callbacks for speed\n",
    "callbacks_stage2 = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=2,  # Aggressive early stopping\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(OUTPUT_DIR, 'stage2_plantvillage_best.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=0  # Silent for speed\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train with verbose=2 for cleaner output\n",
    "history_stage2 = model_stage2.fit(\n",
    "    train_plantvillage,\n",
    "    validation_data=val_plantvillage,\n",
    "    epochs=PLANTVILLAGE_EPOCHS,\n",
    "    callbacks=callbacks_stage2,\n",
    "    verbose=2  # One line per epoch (faster)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Stage 2 completed!\")\n",
    "print(f\"üìà Best val accuracy: {max(history_stage2.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0802c5",
   "metadata": {},
   "source": [
    "## üìà Stage 2 Results Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history_stage2.history['accuracy'], 'b-', label='Train')\n",
    "axes[0].plot(history_stage2.history['val_accuracy'], 'r-', label='Val')\n",
    "axes[0].set_title('Stage 2: Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_stage2.history['loss'], 'b-', label='Train')\n",
    "axes[1].plot(history_stage2.history['val_loss'], 'r-', label='Val')\n",
    "axes[1].set_title('Stage 2: Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'stage2_training_history.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Clear memory before Stage 3\n",
    "import gc\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "print(\"üßπ Memory cleared for Stage 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651b0c7",
   "metadata": {},
   "source": [
    "## üîÑ Stage 3: Build UNIFIED Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3160d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build unified dataset by combining all crops\n",
    "print(\"üèóÔ∏è Building UNIFIED dataset...\")\n",
    "\n",
    "UNIFIED_DATASET_PATH = '/content/unified_nutrient_dataset'\n",
    "\n",
    "# Check if unified dataset already exists\n",
    "if os.path.exists(UNIFIED_DATASET_PATH):\n",
    "    existing_classes = [d for d in os.listdir(UNIFIED_DATASET_PATH) \n",
    "                        if os.path.isdir(os.path.join(UNIFIED_DATASET_PATH, d))]\n",
    "    if len(existing_classes) > 10:\n",
    "        print(f\"‚úÖ Already exists with {len(existing_classes)} classes!\")\n",
    "        unified_classes = existing_classes\n",
    "    else:\n",
    "        import shutil\n",
    "        shutil.rmtree(UNIFIED_DATASET_PATH)\n",
    "        os.makedirs(UNIFIED_DATASET_PATH)\n",
    "        unified_classes = []\n",
    "else:\n",
    "    os.makedirs(UNIFIED_DATASET_PATH)\n",
    "    unified_classes = []\n",
    "\n",
    "if len(unified_classes) == 0:\n",
    "    skipped_crops = []\n",
    "    \n",
    "    for crop, folder_name in CROP_DATASETS.items():\n",
    "        crop_path = os.path.join(NUTRIENT_DATASETS_ROOT, folder_name)\n",
    "        \n",
    "        if not os.path.exists(crop_path):\n",
    "            skipped_crops.append(crop)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            crop_classes = [d for d in os.listdir(crop_path) \n",
    "                            if os.path.isdir(os.path.join(crop_path, d))]\n",
    "        except:\n",
    "            skipped_crops.append(crop)\n",
    "            continue\n",
    "        \n",
    "        for cls in crop_classes:\n",
    "            try:\n",
    "                clean_cls = cls.replace(f\"{crop}_\", \"\").replace(f\"{crop}__\", \"\")\n",
    "                unified_class_name = f\"{crop}_{clean_cls}\"\n",
    "                \n",
    "                src_dir = os.path.join(crop_path, cls)\n",
    "                dst_dir = os.path.join(UNIFIED_DATASET_PATH, unified_class_name)\n",
    "                \n",
    "                if not os.path.exists(dst_dir):\n",
    "                    os.symlink(src_dir, dst_dir)\n",
    "                    unified_classes.append(unified_class_name)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(f\"  ‚úÖ {crop.upper()}: {len([c for c in unified_classes if c.startswith(crop)])} classes\")\n",
    "    \n",
    "    if skipped_crops:\n",
    "        print(f\"‚ö†Ô∏è Skipped: {', '.join(skipped_crops)}\")\n",
    "\n",
    "if len(unified_classes) == 0:\n",
    "    raise RuntimeError(\"‚ùå No classes! Check Google Drive paths.\")\n",
    "\n",
    "class_names = sorted(unified_classes)\n",
    "num_unified_classes = len(class_names)\n",
    "\n",
    "print(f\"\\n‚úÖ Unified dataset: {num_unified_classes} classes\")\n",
    "\n",
    "# Create MEMORY-SAFE datasets\n",
    "print(\"üì¶ Creating datasets (MEMORY-SAFE)...\")\n",
    "\n",
    "train_nutrient_raw = create_dataset(\n",
    "    UNIFIED_DATASET_PATH, IMG_SIZE, BATCH_SIZE,\n",
    "    validation_split=0.2, subset='training'\n",
    ")\n",
    "val_nutrient_raw = create_dataset(\n",
    "    UNIFIED_DATASET_PATH, IMG_SIZE, BATCH_SIZE,\n",
    "    validation_split=0.2, subset='validation'\n",
    ")\n",
    "\n",
    "train_nutrient = build_pipeline(train_nutrient_raw, is_training=True)\n",
    "val_nutrient = build_pipeline(val_nutrient_raw, is_training=False)\n",
    "\n",
    "print(f\"‚úÖ Datasets ready (MEMORY-SAFE)\")\n",
    "print(f\"   Training: {tf.data.experimental.cardinality(train_nutrient_raw).numpy()} batches\")\n",
    "print(f\"   Validation: {tf.data.experimental.cardinality(val_nutrient_raw).numpy()} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf0ec80",
   "metadata": {},
   "source": [
    "## üîß Stage 3: Adapt Model for Unified Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb2a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt model head for unified multi-crop detection\n",
    "if 'num_unified_classes' not in locals() or num_unified_classes == 0:\n",
    "    raise RuntimeError(\"‚ö†Ô∏è Run 'Build UNIFIED Dataset' cell first!\")\n",
    "\n",
    "print(f\"üîß Adapting model for {num_unified_classes} unified classes...\")\n",
    "\n",
    "# Get the base model from Stage 2\n",
    "base_model_stage2 = model_stage2.layers[0]\n",
    "base_model_stage2.trainable = False\n",
    "\n",
    "# Streamlined classification head for speed\n",
    "model_stage3 = tf.keras.Sequential([\n",
    "    base_model_stage2,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(384, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(num_unified_classes, activation='softmax', dtype='float32')\n",
    "], name='unified_nutrient_model')\n",
    "\n",
    "# Compile\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_STAGE3)\n",
    "\n",
    "model_stage3.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top3_acc')]\n",
    ")\n",
    "\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in model_stage3.trainable_weights])\n",
    "print(f\"üìä Trainable params: {trainable_params:,}\")\n",
    "print(f\"üéØ Output classes: {num_unified_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb2661",
   "metadata": {},
   "source": [
    "## üéØ Stage 3: Train on UNIFIED Nutrient Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting Stage 3: UNIFIED Nutrient Detection\")\n",
    "print(f\"üåæ Training ALL {len(CROP_DATASETS)} crops | Epochs: {UNIFIED_EPOCHS} | LR: {LEARNING_RATE_STAGE3}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Optimized callbacks\n",
    "callbacks_stage3 = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=4,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(OUTPUT_DIR, 'unified_nutrient_best.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train\n",
    "history_stage3 = model_stage3.fit(\n",
    "    train_nutrient,\n",
    "    validation_data=val_nutrient,\n",
    "    epochs=UNIFIED_EPOCHS,\n",
    "    callbacks=callbacks_stage3,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Stage 3 completed!\")\n",
    "print(f\"üìà Best val accuracy: {max(history_stage3.history['val_accuracy']):.4f}\")\n",
    "print(f\"üéØ Top-3 accuracy: {max(history_stage3.history['val_top3_acc']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fcb26",
   "metadata": {},
   "source": [
    "## üìà Stage 3 Results Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce165f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history_stage3.history['accuracy'], 'b-', label='Train')\n",
    "axes[0].plot(history_stage3.history['val_accuracy'], 'r-', label='Val')\n",
    "axes[0].set_title('Stage 3: Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_stage3.history['loss'], 'b-', label='Train')\n",
    "axes[1].plot(history_stage3.history['val_loss'], 'r-', label='Val')\n",
    "axes[1].set_title('Stage 3: Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'stage3_training_history.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10140073",
   "metadata": {},
   "source": [
    "## üîç Model Evaluation & Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation (skip heavy confusion matrix for speed)\n",
    "print(\"üîç Evaluating UNIFIED model...\")\n",
    "results = model_stage3.evaluate(val_nutrient, verbose=0)\n",
    "\n",
    "print(f\"\\nüìä Validation Metrics:\")\n",
    "print(f\"   Loss: {results[0]:.4f}\")\n",
    "print(f\"   Accuracy: {results[1]:.4f}\")\n",
    "print(f\"   Top-3 Accuracy: {results[2]:.4f}\")\n",
    "\n",
    "# Quick per-crop accuracy (sample-based for speed)\n",
    "print(f\"\\nüåæ Per-Crop Performance (quick check):\")\n",
    "y_true, y_pred = [], []\n",
    "for images, labels in val_nutrient.take(20):  # Sample only\n",
    "    predictions = model_stage3.predict(images, verbose=0)\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "for crop in list(CROP_DATASETS.keys())[:6]:  # First 6 crops\n",
    "    crop_classes = [cls for cls in class_names if cls.startswith(f\"{crop}_\")]\n",
    "    if not crop_classes:\n",
    "        continue\n",
    "    crop_indices = [class_names.index(cls) for cls in crop_classes]\n",
    "    crop_mask = np.isin(y_true, crop_indices)\n",
    "    if crop_mask.sum() > 0:\n",
    "        crop_acc = (np.array(y_true)[crop_mask] == np.array(y_pred)[crop_mask]).mean()\n",
    "        print(f\"   {crop.upper():12s}: {crop_acc:.1%}\")\n",
    "\n",
    "# Save classification report\n",
    "report = classification_report(y_true, y_pred, target_names=[class_names[i] for i in sorted(set(y_true))], output_dict=True, zero_division=0)\n",
    "with open(os.path.join(OUTPUT_DIR, 'unified_classification_report.json'), 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b395d",
   "metadata": {},
   "source": [
    "## üíæ Export to TensorFlow Lite for Mobile Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf78c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Converting to TensorFlow Lite...\")\n",
    "\n",
    "# Load best model\n",
    "best_model_path = os.path.join(OUTPUT_DIR, 'unified_nutrient_best.keras')\n",
    "best_model = tf.keras.models.load_model(best_model_path)\n",
    "\n",
    "# Convert to TFLite with FP16 quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "print(\"‚öôÔ∏è Converting with FP16 quantization...\")\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save\n",
    "tflite_path = os.path.join(OUTPUT_DIR, 'fasalvaidya_unified.tflite')\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "keras_size = os.path.getsize(best_model_path) / (1024 * 1024)\n",
    "tflite_size = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n‚úÖ Conversion complete!\")\n",
    "print(f\"üìä Keras: {keras_size:.1f}MB ‚Üí TFLite: {tflite_size:.1f}MB ({(1-tflite_size/keras_size)*100:.0f}% smaller)\")\n",
    "print(f\"üöÄ Single model for {len(CROP_DATASETS)} crops!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6488f02a",
   "metadata": {},
   "source": [
    "## üß™ Test TFLite Model Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b10789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick TFLite verification\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"üîç TFLite Model:\")\n",
    "print(f\"   Input: {input_details[0]['shape']} ({input_details[0]['dtype']})\")\n",
    "print(f\"   Output: {output_details[0]['shape']} ({num_unified_classes} classes)\")\n",
    "\n",
    "# Quick test\n",
    "for images, labels in val_nutrient.take(1):\n",
    "    test_image = images[0].numpy()\n",
    "    input_data = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    \n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    pred_idx = np.argmax(output[0])\n",
    "    true_idx = np.argmax(labels[0].numpy())\n",
    "    \n",
    "    print(f\"\\nüß™ Quick test:\")\n",
    "    print(f\"   True: {class_names[true_idx]}\")\n",
    "    print(f\"   Pred: {class_names[pred_idx]} ({output[0][pred_idx]:.1%})\")\n",
    "    print(f\"   {'‚úÖ CORRECT' if pred_idx == true_idx else '‚ùå INCORRECT'}\")\n",
    "    break\n",
    "\n",
    "print(\"\\n‚úÖ TFLite model verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ffab2",
   "metadata": {},
   "source": [
    "## üì§ Save Model Metadata & Class Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f144a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata and labels\n",
    "print(\"üìù Saving metadata...\")\n",
    "\n",
    "crop_class_mapping = {crop: [c for c in class_names if c.startswith(f\"{crop}_\")] \n",
    "                      for crop in CROP_DATASETS.keys()}\n",
    "\n",
    "metadata = {\n",
    "    'model_type': 'unified_multi_crop',\n",
    "    'model_version': '2.0',\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'architecture': 'MobileNetV2',\n",
    "    'supported_crops': list(CROP_DATASETS.keys()),\n",
    "    'num_crops': len(CROP_DATASETS),\n",
    "    'input_shape': [IMG_SIZE, IMG_SIZE, 3],\n",
    "    'num_classes': num_unified_classes,\n",
    "    'class_names': class_names,\n",
    "    'crop_class_mapping': crop_class_mapping,\n",
    "    'metrics': {'accuracy': float(results[1]), 'top3_accuracy': float(results[2])},\n",
    "    'preprocessing': {'method': 'MobileNetV2', 'normalization': '[-1, 1]'}\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'unified_model_metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'labels.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(class_names))\n",
    "\n",
    "print(f\"‚úÖ Saved: metadata.json, labels.txt\")\n",
    "print(f\"üìä {len(CROP_DATASETS)} crops, {num_unified_classes} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3d8a9c",
   "metadata": {},
   "source": [
    "## üì¶ Download Models to Local Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ddb7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and download zip\n",
    "import shutil\n",
    "\n",
    "zip_filename = 'fasalvaidya_unified_model'\n",
    "shutil.make_archive(f'/content/{zip_filename}', 'zip', OUTPUT_DIR)\n",
    "\n",
    "print(f\"üì¶ Created: {zip_filename}.zip\")\n",
    "print(f\"\\nüìÇ Contents:\")\n",
    "print(f\"   üì± fasalvaidya_unified.tflite ({tflite_size:.1f}MB)\")\n",
    "print(f\"   üíæ unified_nutrient_best.keras\")\n",
    "print(f\"   üìÑ unified_model_metadata.json\")\n",
    "print(f\"   üè∑Ô∏è labels.txt ({num_unified_classes} classes)\")\n",
    "print(f\"\\nüåæ Supports: {', '.join(list(CROP_DATASETS.keys())[:6])}...\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download(f'/content/{zip_filename}.zip')\n",
    "print(f\"\\n‚¨áÔ∏è Download started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a73ed9",
   "metadata": {},
   "source": [
    "## üéâ UNIFIED Model Training Complete!\n",
    "\n",
    "### üöÄ What You Got:\n",
    "\n",
    "**ONE powerful model for ALL crops** instead of 12 separate models:\n",
    "\n",
    "- üì± Single TFLite file: `fasalvaidya_unified.tflite` (~10-15MB)\n",
    "- üåæ Handles 12 crops automatically: rice, wheat, tomato, maize, banana, coffee, cucumber, eggplant, ashgourd, bittergourd, ridgegourd, snakegourd\n",
    "- üéØ 40-60+ deficiency classes (varies by available data)\n",
    "- üî• Class format: `{crop}_{deficiency}` (e.g., `rice_N`, `wheat_healthy`, `tomato_K`)\n",
    "\n",
    "### üìä Files Included:\n",
    "\n",
    "- `fasalvaidya_unified.tflite` - Main model\n",
    "- `unified_model_metadata.json` - Complete model info & crop mappings\n",
    "- `labels.txt` - All class labels\n",
    "- `unified_classification_report.json` - Performance metrics\n",
    "- `unified_confusion_matrix.png` - Visualization\n",
    "- Training history plots\n",
    "- TensorBoard logs\n",
    "\n",
    "### üéØ Model Architecture:\n",
    "\n",
    "- **Stage 1**: ImageNet pretrained weights (general vision)\n",
    "- **Stage 2**: PlantVillage fine-tuning (plant disease patterns)\n",
    "- **Stage 3**: Unified nutrient training (ALL crops combined)\n",
    "\n",
    "### üí° Benefits:\n",
    "\n",
    "- ‚úÖ **12√ó smaller**: ~15MB vs 120MB (12 separate models)\n",
    "- ‚úÖ **Simpler deployment**: One model to manage\n",
    "- ‚úÖ **Consistent performance**: Same base features for all crops\n",
    "- ‚úÖ **Easy updates**: Retrain once, update all crops\n",
    "- ‚úÖ **Mobile-optimized**: FP16 quantization\n",
    "- ‚úÖ **Grad-CAM ready**: Visualize deficiency regions\n",
    "\n",
    "### üì¶ Integration Steps:\n",
    "\n",
    "1. **Extract the ZIP file** you downloaded\n",
    "\n",
    "2. **Copy the unified TFLite model** to your app:\n",
    "\n",
    "   ```\n",
    "   frontend/assets/models/fasalvaidya_unified.tflite\n",
    "   ```\n",
    "\n",
    "3. **Copy the labels file**:\n",
    "\n",
    "   ```\n",
    "   frontend/assets/models/labels.txt\n",
    "   ```\n",
    "\n",
    "4. **Update your inference code** to:\n",
    "\n",
    "   - Load the single unified model\n",
    "   - Parse predictions: split class name on `_` to get crop and deficiency\n",
    "   - Example: `rice_N` ‚Üí crop=`rice`, deficiency=`N`\n",
    "\n",
    "5. **Grad-CAM visualization** (optional):\n",
    "   - Use `Conv_1` layer from MobileNetV2 base\n",
    "   - See metadata for implementation notes\n",
    "\n",
    "### üîÑ To Retrain:\n",
    "\n",
    "1. Update datasets in Google Drive\n",
    "2. Verify paths in configuration cell\n",
    "3. Run all cells again\n",
    "4. ONE training session updates ALL crops!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
