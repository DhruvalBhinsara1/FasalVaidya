{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9a5ad1",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Mount Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4de45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check GPU\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db360f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies if needed\n",
    "!pip install -q pillow tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e2a56",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration\n",
    "\n",
    "**IMPORTANT:** Update `DATASET_ROOT` to point to your dataset in Google Drive.\n",
    "\n",
    "Expected structure:\n",
    "\n",
    "```\n",
    "Leaf Nutrient Data Sets/\n",
    "‚îú‚îÄ‚îÄ Rice Nutrients/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Nitrogen(N)/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Phosphorus(P)/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Potassium(K)/\n",
    "‚îú‚îÄ‚îÄ Tomato Nutrients/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ train/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ Tomato - Healthy/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ Tomato - Nitrogen Deficiency/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ ... other crops\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB2, MobileNetV3Large\n",
    "\n",
    "# ========================================\n",
    "# üìÅ UPDATE THIS PATH TO YOUR DRIVE LOCATION\n",
    "# ========================================\n",
    "DATASET_ROOT = Path('/content/drive/MyDrive/Leaf Nutrient Data Sets')\n",
    "MODEL_OUTPUT = Path('/content/drive/MyDrive/FasalVaidya_Models')\n",
    "\n",
    "# Create output directory\n",
    "MODEL_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify dataset exists\n",
    "if DATASET_ROOT.exists():\n",
    "    print(f\"‚úÖ Dataset found at: {DATASET_ROOT}\")\n",
    "    print(f\"üìÇ Contents: {[f.name for f in DATASET_ROOT.iterdir()]}\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset NOT found at: {DATASET_ROOT}\")\n",
    "    print(\"Please update DATASET_ROOT to your Google Drive path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda51de",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Crop Configurations\n",
    "\n",
    "Each crop has specific folder-to-label mappings for N, P, K, Mg deficiencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7523b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Image settings\n",
    "IMG_SIZE = 224\n",
    "MAX_SAMPLES_PER_CLASS = 2000\n",
    "\n",
    "# Crop configurations with folder‚Üílabel mappings\n",
    "CROP_CONFIGS = {\n",
    "    'rice': {\n",
    "        'name': 'Rice',\n",
    "        'dataset_path': DATASET_ROOT / 'Rice Nutrients',\n",
    "        'class_mapping': {\n",
    "            'Nitrogen(N)': {'N': 1, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'Phosphorus(P)': {'N': 0, 'P': 1, 'K': 0, 'Mg': 0},\n",
    "            'Potassium(K)': {'N': 0, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': False,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "    'tomato': {\n",
    "        'name': 'Tomato',\n",
    "        'dataset_path': DATASET_ROOT / 'Tomato Nutrients',\n",
    "        'use_train_folder': True,\n",
    "        'class_mapping': {\n",
    "            'Tomato - Healthy': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'Tomato - Nitrogen Deficiency': {'N': 1, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'Tomato - Potassium Deficiency': {'N': 0, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "            'Tomato - Nitrogen and Potassium Deficiency': {'N': 1, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "            'Tomato - Jassid and Mite': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'Tomato - Leaf Miner': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'Tomato - Mite': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': True,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "    'wheat': {\n",
    "        'name': 'Wheat',\n",
    "        'dataset_path': DATASET_ROOT / 'Wheat Nitrogen',\n",
    "        'use_train_folder': True,\n",
    "        'class_mapping': {\n",
    "            'control': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'deficiency': {'N': 1, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': True,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "    'maize': {\n",
    "        'name': 'Maize',\n",
    "        'dataset_path': DATASET_ROOT / 'Maize Nutrients',\n",
    "        'use_train_folder': True,\n",
    "        'class_mapping': {\n",
    "            'ALL Present': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'NAB': {'N': 1, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'PAB': {'N': 0, 'P': 1, 'K': 0, 'Mg': 0},\n",
    "            'KAB': {'N': 0, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "            'ALLAB': {'N': 1, 'P': 1, 'K': 1, 'Mg': 0},\n",
    "            'ZNAB': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': True,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "    'banana': {\n",
    "        'name': 'Banana',\n",
    "        'dataset_path': DATASET_ROOT / 'Banana leaves Nutrient',\n",
    "        'class_mapping': {\n",
    "            'healthy': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'potassium': {'N': 0, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "            'magnesium': {'N': 0, 'P': 0, 'K': 0, 'Mg': 1},\n",
    "            'boron': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'calcium': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'iron': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'manganese': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'sulphur': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'zinc': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': True,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "    'coffee': {\n",
    "        'name': 'Coffee',\n",
    "        'dataset_path': DATASET_ROOT / 'Coffee Nutrients',\n",
    "        'class_mapping': {\n",
    "            'healthy': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'nitrogen-N': {'N': 1, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'phosphorus-P': {'N': 0, 'P': 1, 'K': 0, 'Mg': 0},\n",
    "            'potasium-K': {'N': 0, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': True,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "    'cucumber': {\n",
    "        'name': 'Cucumber',\n",
    "        'dataset_path': DATASET_ROOT / 'Cucumber Nutrients',\n",
    "        'class_mapping': {\n",
    "            'healthy': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'N': {'N': 1, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'K': {'N': 0, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "            'N_K': {'N': 1, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': True,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "    'eggplant': {\n",
    "        'name': 'Eggplant',\n",
    "        'dataset_path': DATASET_ROOT / 'EggPlant Nutrients',\n",
    "        'class_mapping': {\n",
    "            'healthy': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'N': {'N': 1, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'K': {'N': 0, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "            'N_K': {'N': 1, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': True,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "    'ashgourd': {\n",
    "        'name': 'Ash Gourd',\n",
    "        'dataset_path': DATASET_ROOT / 'Ashgourd Nutrients',\n",
    "        'class_mapping': {\n",
    "            'ash_gourd__healthy': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'N': {'N': 1, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'K': {'N': 0, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "            'N_K': {'N': 1, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "            'K_Mg': {'N': 0, 'P': 0, 'K': 1, 'Mg': 1},\n",
    "            'N_Mg': {'N': 1, 'P': 0, 'K': 0, 'Mg': 1},\n",
    "            'PM': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': True,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "    'bittergourd': {\n",
    "        'name': 'Bitter Gourd',\n",
    "        'dataset_path': DATASET_ROOT / 'Bittergourd Nutrients',\n",
    "        'class_mapping': {\n",
    "            'healthy': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'N': {'N': 1, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'K': {'N': 0, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "            'N_K': {'N': 1, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "            'K_Mg': {'N': 0, 'P': 0, 'K': 1, 'Mg': 1},\n",
    "            'N_Mg': {'N': 1, 'P': 0, 'K': 0, 'Mg': 1},\n",
    "            'DM': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'JAS': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'LS': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': True,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "    'ridgegourd': {\n",
    "        'name': 'Ridge Gourd',\n",
    "        'dataset_path': DATASET_ROOT / 'Ridgegourd',\n",
    "        'class_mapping': {\n",
    "            'healthy': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'N': {'N': 1, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'N_Mg': {'N': 1, 'P': 0, 'K': 0, 'Mg': 1},\n",
    "            'PC': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': True,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "    'snakegourd': {\n",
    "        'name': 'Snake Gourd',\n",
    "        'dataset_path': DATASET_ROOT / 'Snakegourd Nutrients',\n",
    "        'class_mapping': {\n",
    "            'healthy': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'N': {'N': 1, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "            'K': {'N': 0, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "            'N_K': {'N': 1, 'P': 0, 'K': 1, 'Mg': 0},\n",
    "            'LS': {'N': 0, 'P': 0, 'K': 0, 'Mg': 0},\n",
    "        },\n",
    "        'has_healthy': True,\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"üìã Available crops: {list(CROP_CONFIGS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30d6dc",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Data Loading & Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def load_and_preprocess_image(img_path, target_size=(IMG_SIZE, IMG_SIZE)):\n",
    "    \"\"\"Load and preprocess a single image.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = img.resize(target_size, Image.LANCZOS)\n",
    "        return np.array(img, dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_image_paths_and_labels(crop_id):\n",
    "    \"\"\"Get list of image paths and their labels (doesn't load images into memory).\"\"\"\n",
    "    config = CROP_CONFIGS[crop_id]\n",
    "    dataset_path = config['dataset_path']\n",
    "    class_mapping = config['class_mapping']\n",
    "    use_train_folder = config.get('use_train_folder', False)\n",
    "    \n",
    "    if use_train_folder:\n",
    "        dataset_path = dataset_path / 'train'\n",
    "    \n",
    "    if not dataset_path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found: {dataset_path}\")\n",
    "    \n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_counts = {}\n",
    "    \n",
    "    print(f\"\\nüìÇ Scanning {config['name']} from: {dataset_path}\")\n",
    "    \n",
    "    for folder_name, label_dict in class_mapping.items():\n",
    "        folder_path = dataset_path / folder_name\n",
    "        if not folder_path.exists():\n",
    "            print(f\"  ‚ö†Ô∏è Folder not found: {folder_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Get image files\n",
    "        img_files = list(folder_path.glob('*.jpg')) + list(folder_path.glob('*.jpeg')) + \\\n",
    "                    list(folder_path.glob('*.png')) + list(folder_path.glob('*.JPG')) + \\\n",
    "                    list(folder_path.glob('*.JPEG')) + list(folder_path.glob('*.PNG'))\n",
    "        \n",
    "        # Limit samples per class to prevent memory issues\n",
    "        if len(img_files) > MAX_SAMPLES_PER_CLASS:\n",
    "            img_files = random.sample(img_files, MAX_SAMPLES_PER_CLASS)\n",
    "        \n",
    "        class_counts[folder_name] = len(img_files)\n",
    "        \n",
    "        for img_path in img_files:\n",
    "            image_paths.append(str(img_path))\n",
    "            label = [label_dict.get('N', 0), label_dict.get('P', 0), \n",
    "                     label_dict.get('K', 0), label_dict.get('Mg', 0)]\n",
    "            labels.append(label)\n",
    "    \n",
    "    print(f\"  üìä Class distribution: {class_counts}\")\n",
    "    print(f\"  ‚úÖ Found {len(image_paths)} images\")\n",
    "    \n",
    "    return image_paths, np.array(labels, dtype=np.float32)\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Memory-efficient data generator that loads images on-the-fly.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, batch_size=32, img_size=IMG_SIZE, shuffle=True, augment=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.indices = np.arange(len(self.image_paths))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        for i in batch_indices:\n",
    "            img = load_and_preprocess_image(self.image_paths[i], (self.img_size, self.img_size))\n",
    "            if img is not None:\n",
    "                batch_images.append(img)\n",
    "                batch_labels.append(self.labels[i])\n",
    "        \n",
    "        return np.array(batch_images), np.array(batch_labels)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and RAM memory.\"\"\"\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    print(\"üßπ Memory cleared\")\n",
    "\n",
    "\n",
    "# Data augmentation layer (applied in model, not generator)\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.15),\n",
    "    layers.RandomContrast(0.15),\n",
    "], name='data_augmentation')\n",
    "\n",
    "print(\"‚úÖ Memory-efficient data loading ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87897768",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ec3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(backbone='efficientnetb0', num_outputs=4):\n",
    "    \"\"\"Create the nutrient deficiency detection model.\"\"\"\n",
    "    \n",
    "    # Select backbone\n",
    "    if backbone == 'efficientnetb0':\n",
    "        base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    elif backbone == 'efficientnetb2':\n",
    "        base = EfficientNetB2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    elif backbone == 'mobilenetv3large':\n",
    "        base = MobileNetV3Large(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backbone: {backbone}\")\n",
    "    \n",
    "    # Freeze base initially\n",
    "    base.trainable = False\n",
    "    \n",
    "    # Build model\n",
    "    inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    \n",
    "    # Preprocessing (scale to [0,1] then ImageNet normalization)\n",
    "    x = layers.Rescaling(1./255)(inputs)\n",
    "    x = layers.Normalization(mean=[0.485, 0.456, 0.406], variance=[0.229**2, 0.224**2, 0.225**2])(x)\n",
    "    \n",
    "    # Augmentation (only during training)\n",
    "    x = data_augmentation(x)\n",
    "    \n",
    "    # Backbone\n",
    "    x = base(x, training=False)\n",
    "    \n",
    "    # Classification head\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Output: 4 sigmoid outputs for N, P, K, Mg\n",
    "    outputs = layers.Dense(num_outputs, activation='sigmoid', name='npkmg_output')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs, name=f'fasalvaidya_{backbone}')\n",
    "    return model, base\n",
    "\n",
    "\n",
    "# Test model creation\n",
    "test_model, _ = create_model()\n",
    "print(f\"‚úÖ Model created: {test_model.count_params():,} parameters\")\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cbcd87",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caacca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crop_model(crop_id, epochs=50, batch_size=16, backbone='efficientnetb0', fine_tune=True):\n",
    "    \"\"\"\n",
    "    Train or improve a model for a specific crop (MEMORY-EFFICIENT VERSION).\n",
    "    \n",
    "    - Uses data generators to load images on-the-fly (prevents OOM crashes)\n",
    "    - If a model already exists: loads it and continues training (fine-tuning)\n",
    "    - If no model exists: trains from scratch\n",
    "    \n",
    "    Args:\n",
    "        crop_id: Crop identifier (e.g., 'rice', 'tomato')\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size (default 16 to prevent OOM)\n",
    "        backbone: Model backbone (only used for new models)\n",
    "        fine_tune: Whether to fine-tune backbone layers\n",
    "    \"\"\"\n",
    "    # Clear memory before starting\n",
    "    clear_memory()\n",
    "    \n",
    "    config = CROP_CONFIGS[crop_id]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üå± Training model for: {config['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Check for existing model\n",
    "    output_dir = MODEL_OUTPUT / crop_id\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    existing_model_path = output_dir / f'{crop_id}_best.keras'\n",
    "    \n",
    "    is_continuing = existing_model_path.exists()\n",
    "    \n",
    "    # Get image paths and labels (doesn't load images into memory!)\n",
    "    image_paths, labels = get_image_paths_and_labels(crop_id)\n",
    "    \n",
    "    if len(image_paths) == 0:\n",
    "        print(f\"‚ùå No images found for {crop_id}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Split into train/val\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        image_paths, labels, test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    print(f\"\\nüìä Train: {len(train_paths)}, Validation: {len(val_paths)}\")\n",
    "    \n",
    "    # Create data generators (memory-efficient!)\n",
    "    train_gen = DataGenerator(train_paths, train_labels, batch_size=batch_size, shuffle=True, augment=True)\n",
    "    val_gen = DataGenerator(val_paths, val_labels, batch_size=batch_size, shuffle=False, augment=False)\n",
    "    \n",
    "    # Either load existing model or create new one\n",
    "    if is_continuing:\n",
    "        print(f\"\\nüîÑ Found existing model - CONTINUING training to improve accuracy\")\n",
    "        print(f\"   Loading: {existing_model_path}\")\n",
    "        model = keras.models.load_model(str(existing_model_path))\n",
    "        base_model = None\n",
    "        \n",
    "        # Check previous accuracy on a small sample\n",
    "        sample_x, sample_y = val_gen[0]\n",
    "        old_results = model.evaluate(sample_x, sample_y, verbose=0)\n",
    "        print(f\"   Previous (sample) - Accuracy: {old_results[1]:.4f}, AUC: {old_results[2]:.4f}\")\n",
    "        \n",
    "        initial_lr = 1e-5\n",
    "    else:\n",
    "        print(f\"\\nüÜï No existing model - training from SCRATCH\")\n",
    "        model, base_model = create_model(backbone=backbone)\n",
    "        initial_lr = 1e-3\n",
    "        old_results = None\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=initial_lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            str(output_dir / f'{crop_id}_best.keras'),\n",
    "            monitor='val_auc',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_auc',\n",
    "            mode='max',\n",
    "            patience=8,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    if is_continuing:\n",
    "        # For existing models: single phase of continued training\n",
    "        print(f\"\\nüü¢ Continuing training for {epochs} epochs...\")\n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "    else:\n",
    "        # For new models: two-phase training\n",
    "        phase1_epochs = max(epochs // 2, 5)\n",
    "        phase2_epochs = epochs - phase1_epochs\n",
    "        \n",
    "        # Phase 1: Train classifier head\n",
    "        print(f\"\\nüîµ Phase 1: Training classifier head ({phase1_epochs} epochs)...\")\n",
    "        history1 = model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=phase1_epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Phase 2: Fine-tune backbone\n",
    "        if fine_tune and base_model is not None:\n",
    "            print(f\"\\nüü¢ Phase 2: Fine-tuning backbone ({phase2_epochs} epochs)...\")\n",
    "            base_model.trainable = True\n",
    "            \n",
    "            # Freeze early layers, train later ones\n",
    "            for layer in base_model.layers[:-20]:\n",
    "                layer.trainable = False\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    "            )\n",
    "            \n",
    "            history2 = model.fit(\n",
    "                train_gen,\n",
    "                validation_data=val_gen,\n",
    "                epochs=phase2_epochs,\n",
    "                callbacks=callbacks,\n",
    "                verbose=1\n",
    "            )\n",
    "    \n",
    "    # Evaluate on full validation set\n",
    "    print(f\"\\nüìà Final Evaluation:\")\n",
    "    results = model.evaluate(val_gen, verbose=0)\n",
    "    print(f\"  Loss: {results[0]:.4f}\")\n",
    "    print(f\"  Accuracy: {results[1]:.4f}\")\n",
    "    print(f\"  AUC: {results[2]:.4f}\")\n",
    "    \n",
    "    if is_continuing and old_results:\n",
    "        acc_change = results[1] - old_results[1]\n",
    "        auc_change = results[2] - old_results[2]\n",
    "        print(f\"\\nüìä Improvement (vs sample):\")\n",
    "        print(f\"  Accuracy: {'+' if acc_change >= 0 else ''}{acc_change:.4f}\")\n",
    "        print(f\"  AUC: {'+' if auc_change >= 0 else ''}{auc_change:.4f}\")\n",
    "    \n",
    "    # Save final model\n",
    "    model.save(str(output_dir / f'{crop_id}_final.keras'))\n",
    "    print(f\"\\nüíæ Model saved to: {output_dir}\")\n",
    "    \n",
    "    # Save/update metadata\n",
    "    metadata = {\n",
    "        'crop_id': crop_id,\n",
    "        'crop_name': config['name'],\n",
    "        'backbone': backbone if not is_continuing else 'continued',\n",
    "        'outputs': ['N', 'P', 'K', 'Mg'],\n",
    "        'val_accuracy': float(results[1]),\n",
    "        'val_auc': float(results[2]),\n",
    "        'trained_at': datetime.now().isoformat(),\n",
    "        'train_samples': len(train_paths),\n",
    "        'val_samples': len(val_paths),\n",
    "        'training_mode': 'continued' if is_continuing else 'from_scratch',\n",
    "    }\n",
    "    with open(output_dir / 'metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # Clear memory after training\n",
    "    clear_memory()\n",
    "    \n",
    "    return model, results\n",
    "\n",
    "print(\"‚úÖ Training function ready (memory-efficient with continue-training support)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca7b10",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Train a Single Crop (Demo)\n",
    "\n",
    "Run this cell to train a model for a single crop. Change `CROP_TO_TRAIN` to train different crops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3022e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üéØ SELECT CROP TO TRAIN\n",
    "# ========================================\n",
    "CROP_TO_TRAIN = 'rice'  # Change this: rice, tomato, wheat, maize, banana, coffee, etc.\n",
    "EPOCHS = 30             # Increase for better results (50-100 recommended)\n",
    "BATCH_SIZE = 16         # Keep at 16 to prevent OOM crashes on T4\n",
    "BACKBONE = 'efficientnetb0'  # or 'efficientnetb2', 'mobilenetv3large'\n",
    "\n",
    "# Verify crop exists\n",
    "if CROP_TO_TRAIN in CROP_CONFIGS:\n",
    "    print(f\"üöÄ Starting training for: {CROP_TO_TRAIN}\")\n",
    "    print(f\"üí° Tip: If you run this again, it will IMPROVE the existing model!\")\n",
    "    model, results = train_crop_model(\n",
    "        CROP_TO_TRAIN, \n",
    "        epochs=EPOCHS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        backbone=BACKBONE\n",
    "    )\n",
    "else:\n",
    "    print(f\"‚ùå Unknown crop: {CROP_TO_TRAIN}\")\n",
    "    print(f\"Available: {list(CROP_CONFIGS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67808eb4",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Train All Crops (Full Run)\n",
    "\n",
    "‚ö†Ô∏è This will take a while! Only run if you want to train models for all crops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d247a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üöÄ TRAIN ALL CROPS\n",
    "# ========================================\n",
    "# This will train/improve models for ALL crops sequentially\n",
    "# Memory is cleared between each crop to prevent crashes\n",
    "\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 16  # Keep at 16 to prevent OOM crashes\n",
    "\n",
    "results_summary = {}\n",
    "\n",
    "for crop_id in CROP_CONFIGS.keys():\n",
    "    try:\n",
    "        print(f\"\\n\\n{'#'*70}\")\n",
    "        print(f\"# Training crop {list(CROP_CONFIGS.keys()).index(crop_id)+1}/{len(CROP_CONFIGS)}: {crop_id}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        model, results = train_crop_model(crop_id, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        if results:\n",
    "            results_summary[crop_id] = {\n",
    "                'accuracy': float(results[1]),\n",
    "                'auc': float(results[2]),\n",
    "                'status': 'success'\n",
    "            }\n",
    "        else:\n",
    "            results_summary[crop_id] = {'status': 'skipped', 'error': 'No images found'}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to train {crop_id}: {e}\")\n",
    "        results_summary[crop_id] = {'status': 'failed', 'error': str(e)}\n",
    "        clear_memory()  # Clear memory on failure to recover\n",
    "\n",
    "# Save summary\n",
    "with open(MODEL_OUTPUT / 'training_summary.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for crop, res in results_summary.items():\n",
    "    if res['status'] == 'success':\n",
    "        print(f\"  ‚úÖ {crop}: Accuracy={res['accuracy']:.4f}, AUC={res['auc']:.4f}\")\n",
    "    elif res['status'] == 'skipped':\n",
    "        print(f\"  ‚è≠Ô∏è {crop}: SKIPPED - {res.get('error', 'Unknown')}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {crop}: FAILED - {res.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5b258f",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Download Trained Models\n",
    "\n",
    "After training, your models are saved in Google Drive at:\n",
    "`/content/drive/MyDrive/FasalVaidya_Models/<crop_id>/`\n",
    "\n",
    "Each folder contains:\n",
    "\n",
    "- `<crop>_best.keras` - Best model by validation AUC\n",
    "- `<crop>_final.keras` - Final model after all epochs\n",
    "- `metadata.json` - Training info and metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9198e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List trained models\n",
    "print(\"üì¶ Trained models:\")\n",
    "if MODEL_OUTPUT.exists():\n",
    "    for crop_dir in MODEL_OUTPUT.iterdir():\n",
    "        if crop_dir.is_dir():\n",
    "            files = list(crop_dir.glob('*.keras'))\n",
    "            if files:\n",
    "                print(f\"  {crop_dir.name}/\")\n",
    "                for f in files:\n",
    "                    size_mb = f.stat().st_size / (1024*1024)\n",
    "                    print(f\"    - {f.name} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"  No models found yet. Run training first!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
