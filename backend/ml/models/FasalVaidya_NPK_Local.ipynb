{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d87a57cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä TensorFlow: 2.15.0\n",
      "üßµ CPU cores: 16 (all will be used)\n",
      "‚ö†Ô∏è AMD GPU not supported by TF on Windows - using optimized CPU mode\n",
      "üì¶ Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Step 1: Setup - Optimized for 16-core CPU\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# ‚ö° CPU Optimization: Enable all cores for TensorFlow (must be set BEFORE importing TF)\n",
    "NUM_WORKERS = multiprocessing.cpu_count()\n",
    "os.environ['TF_NUM_INTEROP_THREADS'] = str(NUM_WORKERS)\n",
    "os.environ['TF_NUM_INTRAOP_THREADS'] = str(NUM_WORKERS)\n",
    "\n",
    "# Install required packages\n",
    "def install_if_missing(package, import_name=None):\n",
    "    try:\n",
    "        __import__(import_name or package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "\n",
    "install_if_missing(\"tqdm\")\n",
    "install_if_missing(\"scikit-learn\", \"sklearn\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(f\"üìä TensorFlow: {tf.__version__}\")\n",
    "print(f\"üßµ CPU cores: {NUM_WORKERS} (all will be used)\")\n",
    "\n",
    "# Check GPU (won't find any for AMD on Windows)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"‚úÖ GPU: {gpus}\")\n",
    "    BATCH_SIZE = 32\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è AMD GPU not supported by TF on Windows - using optimized CPU mode\")\n",
    "    BATCH_SIZE = 32  # Larger batch OK with 16 cores\n",
    "\n",
    "print(f\"üì¶ Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8dcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Base: E:\\FasalVaidya\\backend\\ml\\models\n",
      "üîç Checking datasets...\n",
      "   ‚úÖ CoLeaf DATASET\n",
      "   ‚úÖ Contrast_Stretching\n",
      "   ‚úÖ Histogram_Equalization\n",
      "   ‚úÖ Log_Transformation\n",
      "   ‚úÖ Nitrogen deficiency\n",
      "   ‚úÖ ThorCam_semiFiltered\n",
      "   ‚úÖ POTASSIUM DEFICIENCY\n"
     ]
    }
   ],
   "source": [
    "# üìÅ Step 2: Paths & Configuration\n",
    "BASE_DIR = Path(r\"E:\\FasalVaidya\\backend\\ml\\models\")\n",
    "LOCAL_CACHE = BASE_DIR / \"combined_balanced_dataset\"\n",
    "OUTPUT_DIR = BASE_DIR\n",
    "\n",
    "DATA_SOURCES = [\n",
    "    BASE_DIR / \"Bigger CoLeaf DATASET\" / \"CoLeaf DATASET\",\n",
    "    BASE_DIR / \"Propossed_Data\" / \"Contrast_Stretching\",\n",
    "    BASE_DIR / \"Propossed_Data\" / \"Histogram_Equalization\",\n",
    "    BASE_DIR / \"Propossed_Data\" / \"Log_Transformation\",\n",
    "    BASE_DIR / \"Nitrogen deficiency\",\n",
    "    BASE_DIR / \"ThorCam_semiFiltered\",\n",
    "    BASE_DIR / \"POTASSIUM DEFICIENCY\",\n",
    "]\n",
    "\n",
    "CLASS_MAPPING = {\n",
    "    \"healthy\": \"healthy\", \"control\": \"healthy\", \"-C\": \"healthy\",\n",
    "    \"nitrogen-N\": \"nitrogen-N\", \"deficiency\": \"nitrogen-N\", \"N\": \"nitrogen-N\",\n",
    "    \"phosphorus-P\": \"phosphorus-P\", \"-P\": \"phosphorus-P\", \"-P50\": \"phosphorus-P\",\n",
    "    \"potasium-K\": \"potasium-K\", \"K\": \"potasium-K\",\n",
    "    \"boron-B\": \"boron-B\", \"calcium-Ca\": \"calcium-Ca\", \"iron-Fe\": \"iron-Fe\",\n",
    "    \"magnesium-Mg\": \"magnesium-Mg\", \"manganese-Mn\": \"manganese-Mn\",\n",
    "}\n",
    "\n",
    "ALL_CLASSES = [\"healthy\", \"nitrogen-N\", \"phosphorus-P\", \"potasium-K\",\n",
    "               \"boron-B\", \"calcium-Ca\", \"iron-Fe\", \"magnesium-Mg\", \"manganese-Mn\"]\n",
    "\n",
    "# Hyperparameters - BATCH_SIZE set in Step 1 based on GPU availability\n",
    "IMG_SIZE = 224\n",
    "# BATCH_SIZE already set above (32 for GPU, 16 for CPU)\n",
    "EPOCHS_PHASE1 = 25\n",
    "EPOCHS_PHASE2 = 20\n",
    "EPOCHS_PHASE3 = 15\n",
    "TARGET_SAMPLES_PER_CLASS = 400\n",
    "\n",
    "print(f\"üìÅ Base: {BASE_DIR}\")\n",
    "print(f\"‚öôÔ∏è Batch size: {BATCH_SIZE}\")\n",
    "print(f\"üîç Checking datasets...\")\n",
    "for src in DATA_SOURCES:\n",
    "    if src.exists():\n",
    "        print(f\"   ‚úÖ {src.name}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {src.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "858cd904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Scanning datasets...\n",
      "   üìÇ CoLeaf DATASET\n",
      "   üìÇ Contrast_Stretching\n",
      "   üìÇ Histogram_Equalization\n",
      "   üìÇ Log_Transformation\n",
      "   üìÇ Nitrogen deficiency\n",
      "   üìÇ ThorCam_semiFiltered\n",
      "   üìÇ POTASSIUM DEFICIENCY\n",
      "\n",
      "üìä Dataset stats:\n",
      "   healthy        : 1395\n",
      "   nitrogen-N     : 523\n",
      "   phosphorus-P   : 3508\n",
      "   potasium-K     : 1815\n",
      "   boron-B        : 401\n",
      "   calcium-Ca     : 351\n",
      "   iron-Fe        : 140\n",
      "   magnesium-Mg   : 316\n",
      "   manganese-Mn   : 266\n",
      "\n",
      "‚öñÔ∏è Balancing to 400/class...\n",
      "\n",
      "üìÅ Copying 3724 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3724/3724 [00:37<00:00, 100.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# üìÇ Step 3: Combine & Balance Datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def collect_images():\n",
    "    class_images = {cls: [] for cls in ALL_CLASSES}\n",
    "    \n",
    "    for dataset_path in DATA_SOURCES:\n",
    "        if not dataset_path.exists():\n",
    "            continue\n",
    "        name = dataset_path.name\n",
    "        print(f\"   üìÇ {name}\")\n",
    "        \n",
    "        if \"Nitrogen\" in name or \"nitrogen\" in name:\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                split_dir = dataset_path / split\n",
    "                if split_dir.exists():\n",
    "                    for class_dir in split_dir.iterdir():\n",
    "                        if class_dir.is_dir():\n",
    "                            std = CLASS_MAPPING.get(class_dir.name)\n",
    "                            if std in class_images:\n",
    "                                for img in class_dir.glob(\"*\"):\n",
    "                                    if img.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "                                        class_images[std].append(img)\n",
    "        elif \"ThorCam\" in name:\n",
    "            for class_dir in dataset_path.iterdir():\n",
    "                if class_dir.is_dir():\n",
    "                    std = CLASS_MAPPING.get(class_dir.name)\n",
    "                    if std in class_images:\n",
    "                        for img in class_dir.glob(\"*\"):\n",
    "                            if img.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"]:\n",
    "                                class_images[std].append(img)\n",
    "        elif \"POTASSIUM\" in name.upper():\n",
    "            for img in dataset_path.glob(\"*\"):\n",
    "                if img.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "                    class_images[\"potasium-K\"].append(img)\n",
    "        else:\n",
    "            for class_dir in dataset_path.iterdir():\n",
    "                if class_dir.is_dir():\n",
    "                    std = CLASS_MAPPING.get(class_dir.name, class_dir.name)\n",
    "                    if std in class_images:\n",
    "                        for img in class_dir.glob(\"*\"):\n",
    "                            if img.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "                                class_images[std].append(img)\n",
    "    return class_images\n",
    "\n",
    "print(\"üìÅ Scanning datasets...\")\n",
    "class_images = collect_images()\n",
    "\n",
    "print(\"\\nüìä Dataset stats:\")\n",
    "for cls in ALL_CLASSES:\n",
    "    print(f\"   {cls:15s}: {len(class_images[cls])}\")\n",
    "\n",
    "# Balance\n",
    "print(f\"\\n‚öñÔ∏è Balancing to {TARGET_SAMPLES_PER_CLASS}/class...\")\n",
    "for cls in ALL_CLASSES:\n",
    "    n = len(class_images[cls])\n",
    "    if n == 0:\n",
    "        print(f\"   ‚ö†Ô∏è {cls}: No samples!\")\n",
    "    elif n < TARGET_SAMPLES_PER_CLASS:\n",
    "        class_images[cls].extend(random.choices(class_images[cls], k=TARGET_SAMPLES_PER_CLASS-n))\n",
    "    elif n > TARGET_SAMPLES_PER_CLASS * 2:\n",
    "        class_images[cls] = random.sample(class_images[cls], TARGET_SAMPLES_PER_CLASS)\n",
    "\n",
    "# Copy to cache\n",
    "if LOCAL_CACHE.exists():\n",
    "    shutil.rmtree(LOCAL_CACHE)\n",
    "\n",
    "def copy_file(args):\n",
    "    src, dst = args\n",
    "    try:\n",
    "        shutil.copy2(src, dst)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "tasks = []\n",
    "for cls in ALL_CLASSES:\n",
    "    (LOCAL_CACHE / cls).mkdir(parents=True, exist_ok=True)\n",
    "    for i, src in enumerate(class_images[cls]):\n",
    "        dst = LOCAL_CACHE / cls / f\"{cls}_{i:05d}{src.suffix.lower()}\"\n",
    "        tasks.append((src, dst))\n",
    "\n",
    "print(f\"\\nüìÅ Copying {len(tasks)} files...\")\n",
    "with ThreadPoolExecutor(max_workers=NUM_WORKERS) as ex:\n",
    "    results = list(tqdm(ex.map(copy_file, tasks), total=len(tasks)))\n",
    "print(f\"‚úÖ Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894acdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train: 2848 | Val: 503 | Test: 373\n"
     ]
    }
   ],
   "source": [
    "# üîÄ Step 4: Create Splits\n",
    "all_images, all_labels = [], []\n",
    "class_to_idx = {c: i for i, c in enumerate(ALL_CLASSES)}\n",
    "\n",
    "for cls in ALL_CLASSES:\n",
    "    for img in (LOCAL_CACHE / cls).glob(\"*\"):\n",
    "        all_images.append(str(img))\n",
    "        all_labels.append(class_to_idx[cls])\n",
    "\n",
    "all_images, all_labels = np.array(all_images), np.array(all_labels)\n",
    "\n",
    "train_imgs, test_imgs, train_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.1, stratify=all_labels, random_state=42)\n",
    "train_imgs, val_imgs, train_labels, val_labels = train_test_split(\n",
    "    train_imgs, train_labels, test_size=0.15, stratify=train_labels, random_state=42)\n",
    "\n",
    "NUM_CLASSES = len(ALL_CLASSES)\n",
    "print(f\"‚úÖ Train: {len(train_imgs)} | Val: {len(val_imgs)} | Test: {len(test_imgs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7840d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data pipeline ready\n"
     ]
    }
   ],
   "source": [
    "# ‚ö° Step 5: Data Pipeline\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def parse_image(path, label, training=True):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    if training:\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_brightness(img, 0.2)\n",
    "        img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "    return img, tf.one_hot(label, NUM_CLASSES)\n",
    "\n",
    "def make_ds(imgs, labels, training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((imgs, labels))\n",
    "    if training:\n",
    "        ds = ds.shuffle(len(imgs))\n",
    "    ds = ds.map(lambda x, y: parse_image(x, y, training), num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_ds(train_imgs, train_labels, True)\n",
    "val_ds = make_ds(val_imgs, val_labels, False)\n",
    "test_ds = make_ds(test_imgs, test_labels, False)\n",
    "print(f\"‚úÖ Data pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b9a7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\FasalVaidya\\backend\\.venv311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\FasalVaidya\\backend\\.venv311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "‚úÖ Model built: 21,125,993 params\n"
     ]
    }
   ],
   "source": [
    "# üèóÔ∏è Step 6: Build Model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "base = tf.keras.applications.EfficientNetV2S(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')\n",
    "base.trainable = False\n",
    "\n",
    "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = base(inputs, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "print(f\"‚úÖ Model built: {model.count_params():,} params\")\n",
    "\n",
    "# Class weights\n",
    "weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights = dict(enumerate(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc96e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Callbacks ready\n"
     ]
    }
   ],
   "source": [
    "# üéØ Step 7: Callbacks\n",
    "CHECKPOINT_DIR = OUTPUT_DIR / \"checkpoints\"\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-7),\n",
    "    tf.keras.callbacks.ModelCheckpoint(str(CHECKPOINT_DIR / \"best.keras\"), monitor='val_accuracy', save_best_only=True),\n",
    "]\n",
    "print(\"‚úÖ Callbacks ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ced3d5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üöÄ PHASE 1: Training Head (backbone frozen)\n",
      "==================================================\n",
      "Epoch 1/25\n",
      "178/178 [==============================] - 134s 700ms/step - loss: 1.8985 - accuracy: 0.3427 - val_loss: 1.8875 - val_accuracy: 0.3797 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "178/178 [==============================] - 117s 657ms/step - loss: 1.8389 - accuracy: 0.3722 - val_loss: 1.7494 - val_accuracy: 0.4493 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "178/178 [==============================] - 119s 666ms/step - loss: 1.7667 - accuracy: 0.3947 - val_loss: 1.7086 - val_accuracy: 0.4553 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "178/178 [==============================] - 115s 642ms/step - loss: 1.7189 - accuracy: 0.4199 - val_loss: 1.6389 - val_accuracy: 0.4632 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "178/178 [==============================] - 111s 619ms/step - loss: 1.7125 - accuracy: 0.4315 - val_loss: 1.6564 - val_accuracy: 0.4453 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "178/178 [==============================] - 110s 616ms/step - loss: 1.6848 - accuracy: 0.4477 - val_loss: 1.6338 - val_accuracy: 0.4612 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "178/178 [==============================] - 112s 627ms/step - loss: 1.6762 - accuracy: 0.4487 - val_loss: 1.5895 - val_accuracy: 0.5050 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "178/178 [==============================] - 110s 612ms/step - loss: 1.6613 - accuracy: 0.4505 - val_loss: 1.5963 - val_accuracy: 0.4751 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "178/178 [==============================] - 108s 605ms/step - loss: 1.6552 - accuracy: 0.4600 - val_loss: 1.5915 - val_accuracy: 0.4811 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "178/178 [==============================] - 111s 623ms/step - loss: 1.6260 - accuracy: 0.4796 - val_loss: 1.5597 - val_accuracy: 0.5109 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "178/178 [==============================] - 110s 616ms/step - loss: 1.6288 - accuracy: 0.4607 - val_loss: 1.5722 - val_accuracy: 0.4871 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "178/178 [==============================] - 110s 615ms/step - loss: 1.6152 - accuracy: 0.4846 - val_loss: 1.5738 - val_accuracy: 0.5070 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "178/178 [==============================] - 110s 618ms/step - loss: 1.6025 - accuracy: 0.4863 - val_loss: 1.6186 - val_accuracy: 0.4612 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "178/178 [==============================] - 111s 618ms/step - loss: 1.6005 - accuracy: 0.4838 - val_loss: 1.6098 - val_accuracy: 0.4334 - lr: 0.0010\n",
      "Epoch 15/25\n",
      "178/178 [==============================] - 112s 626ms/step - loss: 1.5713 - accuracy: 0.4877 - val_loss: 1.5212 - val_accuracy: 0.5507 - lr: 5.0000e-04\n",
      "Epoch 16/25\n",
      "178/178 [==============================] - 110s 613ms/step - loss: 1.5468 - accuracy: 0.5200 - val_loss: 1.5239 - val_accuracy: 0.5328 - lr: 5.0000e-04\n",
      "Epoch 17/25\n",
      "178/178 [==============================] - 109s 611ms/step - loss: 1.5517 - accuracy: 0.5028 - val_loss: 1.5149 - val_accuracy: 0.5209 - lr: 5.0000e-04\n",
      "Epoch 18/25\n",
      "178/178 [==============================] - 109s 610ms/step - loss: 1.5380 - accuracy: 0.5140 - val_loss: 1.5097 - val_accuracy: 0.4970 - lr: 5.0000e-04\n",
      "Epoch 19/25\n",
      "178/178 [==============================] - 113s 635ms/step - loss: 1.5293 - accuracy: 0.5137 - val_loss: 1.5051 - val_accuracy: 0.5308 - lr: 5.0000e-04\n",
      "Epoch 20/25\n",
      "178/178 [==============================] - 110s 617ms/step - loss: 1.5434 - accuracy: 0.5190 - val_loss: 1.4772 - val_accuracy: 0.5408 - lr: 5.0000e-04\n",
      "Epoch 21/25\n",
      "178/178 [==============================] - 110s 615ms/step - loss: 1.5258 - accuracy: 0.5126 - val_loss: 1.4896 - val_accuracy: 0.5308 - lr: 5.0000e-04\n",
      "Epoch 22/25\n",
      "178/178 [==============================] - 110s 617ms/step - loss: 1.5225 - accuracy: 0.5193 - val_loss: 1.5035 - val_accuracy: 0.5268 - lr: 5.0000e-04\n",
      "Epoch 23/25\n",
      "178/178 [==============================] - 111s 619ms/step - loss: 1.5094 - accuracy: 0.5235 - val_loss: 1.5157 - val_accuracy: 0.5129 - lr: 5.0000e-04\n",
      "‚úÖ Phase 1 done! Val acc: 55.07%\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Step 8: Phase 1 - Train Head\n",
    "print(\"=\"*50)\n",
    "print(\"üöÄ PHASE 1: Training Head (backbone frozen)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "h1 = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_PHASE1,\n",
    "               callbacks=callbacks, class_weight=class_weights)\n",
    "print(f\"‚úÖ Phase 1 done! Val acc: {max(h1.history['val_accuracy']):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b491fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üî• PHASE 2: Fine-tuning top 30%\n",
      "==================================================\n",
      "Epoch 1/20\n",
      "178/178 [==============================] - 185s 946ms/step - loss: 1.5815 - accuracy: 0.4993 - val_loss: 1.6095 - val_accuracy: 0.4751 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 167s 935ms/step - loss: 1.5162 - accuracy: 0.5235 - val_loss: 1.5096 - val_accuracy: 0.5189 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 167s 935ms/step - loss: 1.5079 - accuracy: 0.5242 - val_loss: 1.5026 - val_accuracy: 0.5149 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 166s 930ms/step - loss: 1.4850 - accuracy: 0.5320 - val_loss: 1.5107 - val_accuracy: 0.5010 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 167s 933ms/step - loss: 1.4458 - accuracy: 0.5488 - val_loss: 1.4535 - val_accuracy: 0.5408 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 167s 934ms/step - loss: 1.4520 - accuracy: 0.5499 - val_loss: 1.4734 - val_accuracy: 0.5268 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 167s 936ms/step - loss: 1.4316 - accuracy: 0.5558 - val_loss: 1.5035 - val_accuracy: 0.5348 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 169s 943ms/step - loss: 1.4145 - accuracy: 0.5681 - val_loss: 1.4522 - val_accuracy: 0.5507 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 173s 968ms/step - loss: 1.4120 - accuracy: 0.5744 - val_loss: 1.4385 - val_accuracy: 0.5586 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 172s 965ms/step - loss: 1.3903 - accuracy: 0.5808 - val_loss: 1.5098 - val_accuracy: 0.5348 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 168s 941ms/step - loss: 1.3793 - accuracy: 0.5899 - val_loss: 1.5485 - val_accuracy: 0.5229 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 173s 968ms/step - loss: 1.3535 - accuracy: 0.5987 - val_loss: 1.3987 - val_accuracy: 0.5646 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 175s 979ms/step - loss: 1.3307 - accuracy: 0.6071 - val_loss: 1.3956 - val_accuracy: 0.5726 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 170s 951ms/step - loss: 1.3209 - accuracy: 0.6117 - val_loss: 1.4407 - val_accuracy: 0.5666 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 174s 977ms/step - loss: 1.3055 - accuracy: 0.6261 - val_loss: 1.4005 - val_accuracy: 0.5984 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 174s 973ms/step - loss: 1.2913 - accuracy: 0.6348 - val_loss: 1.3184 - val_accuracy: 0.6262 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "178/178 [==============================] - 169s 944ms/step - loss: 1.2607 - accuracy: 0.6387 - val_loss: 1.3363 - val_accuracy: 0.5984 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 173s 969ms/step - loss: 1.2711 - accuracy: 0.6440 - val_loss: 1.3620 - val_accuracy: 0.6143 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 175s 979ms/step - loss: 1.2696 - accuracy: 0.6471 - val_loss: 1.3586 - val_accuracy: 0.6322 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "178/178 [==============================] - 170s 952ms/step - loss: 1.2446 - accuracy: 0.6471 - val_loss: 1.3566 - val_accuracy: 0.5924 - lr: 1.0000e-04\n",
      "‚úÖ Phase 2 done! Val acc: 63.22%\n"
     ]
    }
   ],
   "source": [
    "# üî• Step 9: Phase 2 - Unfreeze Top 30%\n",
    "print(\"=\"*50)\n",
    "print(\"üî• PHASE 2: Fine-tuning top 30%\")\n",
    "print(\"=\"*50)\n",
    "      \n",
    "base.trainable = True\n",
    "for layer in base.layers[:int(len(base.layers) * 0.7)]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "h2 = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_PHASE2,\n",
    "               callbacks=callbacks, class_weight=class_weights)\n",
    "print(f\"‚úÖ Phase 2 done! Val acc: {max(h2.history['val_accuracy']):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7178ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üéì PHASE 3: Full fine-tuning\n",
      "==================================================\n",
      "Epoch 1/15\n",
      "178/178 [==============================] - 511s 3s/step - loss: 1.2022 - accuracy: 0.6703 - val_loss: 1.1788 - val_accuracy: 0.7078 - lr: 2.0000e-05\n",
      "Epoch 2/15\n",
      "178/178 [==============================] - 512s 3s/step - loss: 1.1498 - accuracy: 0.7012 - val_loss: 1.1638 - val_accuracy: 0.6998 - lr: 2.0000e-05\n",
      "Epoch 3/15\n",
      "178/178 [==============================] - 440s 2s/step - loss: 1.1148 - accuracy: 0.7307 - val_loss: 1.1374 - val_accuracy: 0.7336 - lr: 2.0000e-05\n",
      "Epoch 4/15\n",
      "178/178 [==============================] - 433s 2s/step - loss: 1.1089 - accuracy: 0.7335 - val_loss: 1.1124 - val_accuracy: 0.7097 - lr: 2.0000e-05\n",
      "Epoch 5/15\n",
      "178/178 [==============================] - 435s 2s/step - loss: 1.0804 - accuracy: 0.7472 - val_loss: 1.1506 - val_accuracy: 0.7097 - lr: 2.0000e-05\n",
      "Epoch 6/15\n",
      "178/178 [==============================] - 434s 2s/step - loss: 1.0588 - accuracy: 0.7609 - val_loss: 1.1447 - val_accuracy: 0.7117 - lr: 2.0000e-05\n",
      "Epoch 7/15\n",
      "178/178 [==============================] - 441s 2s/step - loss: 1.0274 - accuracy: 0.7781 - val_loss: 1.0928 - val_accuracy: 0.7396 - lr: 2.0000e-05\n",
      "Epoch 8/15\n",
      "178/178 [==============================] - 445s 2s/step - loss: 1.0124 - accuracy: 0.7841 - val_loss: 1.0128 - val_accuracy: 0.7793 - lr: 2.0000e-05\n",
      "Epoch 9/15\n",
      "178/178 [==============================] - 437s 2s/step - loss: 1.0045 - accuracy: 0.7949 - val_loss: 1.0894 - val_accuracy: 0.7435 - lr: 2.0000e-05\n",
      "Epoch 10/15\n",
      "178/178 [==============================] - 437s 2s/step - loss: 0.9892 - accuracy: 0.8016 - val_loss: 1.0086 - val_accuracy: 0.7773 - lr: 2.0000e-05\n",
      "Epoch 11/15\n",
      "178/178 [==============================] - 438s 2s/step - loss: 0.9586 - accuracy: 0.8150 - val_loss: 1.0165 - val_accuracy: 0.7793 - lr: 2.0000e-05\n",
      "Epoch 12/15\n",
      "178/178 [==============================] - 438s 2s/step - loss: 0.9414 - accuracy: 0.8213 - val_loss: 1.0015 - val_accuracy: 0.7734 - lr: 2.0000e-05\n",
      "Epoch 13/15\n",
      "178/178 [==============================] - 438s 2s/step - loss: 0.9074 - accuracy: 0.8374 - val_loss: 0.9915 - val_accuracy: 0.7753 - lr: 2.0000e-05\n",
      "Epoch 14/15\n",
      "178/178 [==============================] - 444s 2s/step - loss: 0.9007 - accuracy: 0.8409 - val_loss: 0.9784 - val_accuracy: 0.7992 - lr: 2.0000e-05\n",
      "Epoch 15/15\n",
      "178/178 [==============================] - 441s 2s/step - loss: 0.8845 - accuracy: 0.8511 - val_loss: 1.0225 - val_accuracy: 0.7793 - lr: 2.0000e-05\n",
      "‚úÖ Phase 3 done! Val acc: 79.92%\n"
     ]
    }
   ],
   "source": [
    "# üéì Step 10: Phase 3 - Full Fine-tuning\n",
    "print(\"=\"*50)\n",
    "print(\"üéì PHASE 3: Full fine-tuning\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for layer in base.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(2e-5),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "    \n",
    "h3 = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_PHASE3,\n",
    "               callbacks=callbacks, class_weight=class_weights)\n",
    "print(f\"‚úÖ Phase 3 done! Val acc: {max(h3.history['val_accuracy']):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b43f3d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üß™ EVALUATION\n",
      "==================================================\n",
      "24/24 [==============================] - 13s 519ms/step - loss: 0.9285 - accuracy: 0.8338\n",
      "\n",
      "üìä Test Accuracy: 83.38%\n",
      "\n",
      "üìã Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     healthy      0.811     0.750     0.779        40\n",
      "  nitrogen-N      0.833     0.755     0.792        53\n",
      "phosphorus-P      0.857     0.750     0.800        40\n",
      "  potasium-K      0.949     0.925     0.937        40\n",
      "     boron-B      0.921     0.875     0.897        40\n",
      "  calcium-Ca      0.818     0.900     0.857        40\n",
      "     iron-Fe      0.921     0.875     0.897        40\n",
      "magnesium-Mg      0.714     0.875     0.787        40\n",
      "manganese-Mn      0.733     0.825     0.776        40\n",
      "\n",
      "    accuracy                          0.834       373\n",
      "   macro avg      0.840     0.837     0.836       373\n",
      "weighted avg      0.840     0.834     0.834       373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üß™ Step 11: Evaluate\n",
    "print(\"=\"*50)\n",
    "print(\"üß™ EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "loss, acc = model.evaluate(test_ds)\n",
    "print(f\"\\nüìä Test Accuracy: {acc:.2%}\")\n",
    "\n",
    "# Quick classification report\n",
    "from sklearn.metrics import classification_report\n",
    "y_true, y_pred = [], []\n",
    "for imgs, labels in test_ds:\n",
    "    preds = model.predict(imgs, verbose=0)\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=ALL_CLASSES, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edcd7ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved: E:\\FasalVaidya\\backend\\ml\\models\\plantvillage-npk-v3.h5\n",
      "\n",
      "üéâ TRAINING COMPLETE!\n",
      "   Model: E:\\FasalVaidya\\backend\\ml\\models\\plantvillage-npk-v3.h5\n",
      "   Accuracy: 83.38%\n",
      "\n",
      "üìã Next: Update MODEL_PATH in tasks.json and restart backend\n"
     ]
    }
   ],
   "source": [
    "# üíæ Step 12: Save Model\n",
    "import json\n",
    "\n",
    "model_path = OUTPUT_DIR / \"plantvillage-npk-v3.h5\"\n",
    "model.save(str(model_path))\n",
    "print(f\"‚úÖ Model saved: {model_path}\")\n",
    "\n",
    "with open(OUTPUT_DIR / \"class_names.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(ALL_CLASSES))\n",
    "\n",
    "config = {'model': 'EfficientNetV2-S', 'img_size': IMG_SIZE,\n",
    "          'classes': ALL_CLASSES, 'test_accuracy': float(acc)}\n",
    "with open(OUTPUT_DIR / \"model_config.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\nüéâ TRAINING COMPLETE!\")\n",
    "print(f\"   Model: {model_path}\")\n",
    "print(f\"   Accuracy: {acc:.2%}\")\n",
    "print(f\"\\nüìã Next: Update MODEL_PATH in tasks.json and restart backend\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FasalVaidya (.venv311)",
   "language": "python",
   "name": "fasalvaidya-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
